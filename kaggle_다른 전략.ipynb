{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12288614,
          "sourceType": "datasetVersion",
          "datasetId": 7744692
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook5a12e8f9ad",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/dacon_new_drug/blob/main/kaggle_%EB%8B%A4%EB%A5%B8%20%EC%A0%84%EB%9E%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Zm-M_J1jAVhp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "biniroun_datafile1_path = kagglehub.dataset_download('biniroun/datafile1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "oQ1MDhM9AVhq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret[full] rdkit-pypi --quiet\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:48:11.396108Z",
          "iopub.execute_input": "2025-06-27T08:48:11.396335Z",
          "iopub.status.idle": "2025-06-27T08:48:17.794818Z",
          "execution_failed": "2025-06-27T09:52:08.238Z"
        },
        "id": "t8xqSGGzAVhr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ✅ 데이터 경로\n",
        "data_path = \"/kaggle/input/datafile1\"\n",
        "\n",
        "# ✅ CSV 파일 로드\n",
        "train = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "test = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "submission = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
        "\n",
        "# ✅ 데이터 확인\n",
        "print(\"✅ train shape:\", train.shape)\n",
        "print(\"✅ test shape:\", test.shape)\n",
        "print(\"✅ submission shape:\", submission.shape)\n",
        "train.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:48:24.05694Z",
          "iopub.execute_input": "2025-06-27T08:48:24.057346Z",
          "iopub.status.idle": "2025-06-27T08:48:24.383827Z",
          "shell.execute_reply.started": "2025-06-27T08:48:24.057299Z",
          "shell.execute_reply": "2025-06-27T08:48:24.383232Z"
        },
        "id": "kNMUi9hmAVhs",
        "outputId": "543cc13f-df36-483b-961e-6b38a37b44a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ train shape: (1681, 3)\n✅ test shape: (100, 2)\n✅ submission shape: (100, 2)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "           ID                                   Canonical_Smiles  Inhibition\n0  TRAIN_0000                        Cl.OC1(Cc2cccc(Br)c2)CCNCC1       12.50\n1  TRAIN_0001                              Brc1ccc2OCCc3ccnc1c23        4.45\n2  TRAIN_0002         CC1(CO)CC(=NO1)c2cc(c(F)cc2Cl)[N+](=O)[O-]        4.92\n3  TRAIN_0003  Fc1ccc2nc(Nc3cccc(COc4cccc(c4)C(=O)N5CCOCC5)c3...       71.50\n4  TRAIN_0004       CC(C)CC(=O)C1=C(Nc2c(Cl)ccc(Cl)c2C1=O)S(=O)C       18.30",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Canonical_Smiles</th>\n      <th>Inhibition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_0000</td>\n      <td>Cl.OC1(Cc2cccc(Br)c2)CCNCC1</td>\n      <td>12.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_0001</td>\n      <td>Brc1ccc2OCCc3ccnc1c23</td>\n      <td>4.45</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_0002</td>\n      <td>CC1(CO)CC(=NO1)c2cc(c(F)cc2Cl)[N+](=O)[O-]</td>\n      <td>4.92</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_0003</td>\n      <td>Fc1ccc2nc(Nc3cccc(COc4cccc(c4)C(=O)N5CCOCC5)c3...</td>\n      <td>71.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_0004</td>\n      <td>CC(C)CC(=O)C1=C(Nc2c(Cl)ccc(Cl)c2C1=O)S(=O)C</td>\n      <td>18.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/kaggle/input/datafile1\"))"
      ],
      "metadata": {
        "trusted": true,
        "id": "nfLp6R4-AVht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(\"/kaggle/input/datafile1/train.csv\"))  # True면 OK"
      ],
      "metadata": {
        "trusted": true,
        "id": "VghvlcceAVht"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle 노트북 상단에 추가 (다른 pip install 명령과 함께)\n",
        "# 기존 pandas를 제거하고 특정 안정 버전으로 설치\n",
        "!pip uninstall -y pandas\n",
        "!pip install pandas==1.5.3\n",
        "\n",
        "# (선택 사항: 다른 라이브러리도 함께 설치해야 한다면)\n",
        "# !pip install --upgrade joblib loky scikit-learn==1.3.0 rdkit optuna catboost lightgbm xgboost"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:48:27.746763Z",
          "iopub.execute_input": "2025-06-27T08:48:27.74731Z",
          "iopub.status.idle": "2025-06-27T08:48:36.838718Z",
          "shell.execute_reply.started": "2025-06-27T08:48:27.747286Z",
          "shell.execute_reply": "2025-06-27T08:48:36.837607Z"
        },
        "id": "pRfosdZzAVhu",
        "outputId": "71822d62-ca8e-47d9-9eb3-02aed23a3599"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found existing installation: pandas 2.1.4\nUninstalling pandas-2.1.4:\n  Successfully uninstalled pandas-2.1.4\nCollecting pandas==1.5.3\n  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pandas==1.5.3) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pandas==1.5.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pandas==1.5.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->pandas==1.5.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->pandas==1.5.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->pandas==1.5.3) (2024.2.0)\nDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: pandas\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsalib 1.5.1 requires pandas>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\nmizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-1.5.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ✅ 데이터 경로 설정\n",
        "data_path = \"/kaggle/input/datafile1\"\n",
        "\n",
        "# ✅ 데이터 로드\n",
        "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "submission_df = pd.read_csv(f\"{data_path}/sample_submission.csv\").copy()  # <- 반드시 copy\n",
        "\n",
        "# ✅ SMILES → ECFP 변환\n",
        "def smiles_to_ecfp(smiles, radius=2, n_bits=2048):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(n_bits)\n",
        "    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits))\n",
        "\n",
        "# ✅ Fingerprint 생성\n",
        "X_train = np.array([smiles_to_ecfp(s) for s in train_df[\"Canonical_Smiles\"]])\n",
        "X_test = np.array([smiles_to_ecfp(s) for s in test_df[\"Canonical_Smiles\"]])\n",
        "y_train = np.log1p(train_df[\"Inhibition\"])  # log1p 변환\n",
        "\n",
        "# ✅ Feature Selection\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_train_sel = selector.fit_transform(X_train)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "\n",
        "# ✅ 모델 정의\n",
        "base_models = [\n",
        "    (\"lgb\", LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)),\n",
        "    (\"xgb\", XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)),\n",
        "    (\"cat\", CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=42)),\n",
        "]\n",
        "meta_model = Ridge()\n",
        "\n",
        "# ✅ 스태킹 앙상블 학습\n",
        "model = StackingRegressor(estimators=base_models, final_estimator=meta_model, n_jobs=-1)\n",
        "model.fit(X_train_sel, y_train)\n",
        "\n",
        "# ✅ 예측 및 역변환\n",
        "y_pred_log = model.predict(X_test_sel)\n",
        "y_pred = np.expm1(y_pred_log)  # log1p 역변환\n",
        "\n",
        "# ✅ 결과 저장 준비\n",
        "if not isinstance(submission_df, pd.DataFrame):\n",
        "    raise TypeError(\"submission_df가 DataFrame이 아닙니다!\")\n",
        "\n",
        "submission_df[\"Inhibition\"] = y_pred\n",
        "\n",
        "# ✅ 제출 파일 저장\n",
        "output_path = \"/kaggle/working/submission_stack_log1p.csv\"\n",
        "submission_df.to_csv(output_path, index=False)\n",
        "print(f\"✅ 제출 파일 저장 완료: {output_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:48:43.056882Z",
          "iopub.execute_input": "2025-06-27T08:48:43.057715Z",
          "iopub.status.idle": "2025-06-27T08:49:00.661573Z",
          "shell.execute_reply.started": "2025-06-27T08:48:43.057671Z",
          "shell.execute_reply": "2025-06-27T08:49:00.660729Z"
        },
        "id": "uXUaqJsKAVhv",
        "outputId": "17a7dd4f-11b2-485f-860e-9d69563301d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014064 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1542\n[LightGBM] [Info] Number of data points in the train set: 1681, number of used features: 771\n[LightGBM] [Info] Start training from score 2.996934\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022376 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1208\n[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 604\n[LightGBM] [Info] Start training from score 3.068565\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017009 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1158\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 579\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Start training from score 2.965889\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024779 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1240\n[LightGBM] [Info] Total Bins 1182\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 620\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 591\n[LightGBM] [Info] Start training from score 3.022786\n[LightGBM] [Info] Start training from score 2.892919\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051617 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1194\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 597\n[LightGBM] [Info] Start training from score 3.034563\n✅ 제출 파일 저장 완료: /kaggle/working/submission_stack_log1p.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ✅ 데이터 경로 설정 (Kaggle 경로에 맞게)\n",
        "data_path = \"/kaggle/input/datafile1\"\n",
        "output_path = \"/kaggle/working/baseline_submit.csv\"\n",
        "\n",
        "# ✅ 데이터 로드\n",
        "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "submission_df = pd.read_csv(f\"{data_path}/sample_submission.csv\").copy()\n",
        "\n",
        "# ✅ 분자 특성 추출 함수 정의\n",
        "def extract_rdkit_features(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            return [\n",
        "                Descriptors.MolWt(mol),             # 분자량\n",
        "                Descriptors.MolLogP(mol),           # LogP\n",
        "                Descriptors.NumHAcceptors(mol),     # 수소 수용자 수\n",
        "                Descriptors.NumHDonors(mol),        # 수소 기부자 수\n",
        "                Descriptors.TPSA(mol),              # 극성 표면적\n",
        "                Descriptors.NumRotatableBonds(mol)  # 회전 가능한 결합 수\n",
        "            ]\n",
        "    except:\n",
        "        pass\n",
        "    return [0.0] * 6  # 실패 시 0으로 채움\n",
        "\n",
        "# ✅ 훈련 데이터 특성 생성\n",
        "train_df[\"Features\"] = train_df[\"Canonical_Smiles\"].apply(extract_rdkit_features)\n",
        "train_x = np.stack(train_df[\"Features\"].values)\n",
        "train_y = train_df[\"Inhibition\"].astype(float).values\n",
        "\n",
        "# ✅ 모델 학습\n",
        "model = xgb.XGBRegressor(random_state=42, n_estimators=300, learning_rate=0.05)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# ✅ 테스트 데이터 특성 생성\n",
        "test_df[\"Features\"] = test_df[\"Canonical_Smiles\"].apply(extract_rdkit_features)\n",
        "test_x = np.stack(test_df[\"Features\"].values)\n",
        "\n",
        "# ✅ 예측 수행\n",
        "test_y_pred = model.predict(test_x)\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission_df[\"Inhibition\"] = test_y_pred\n",
        "submission_df.to_csv(output_path, index=False)\n",
        "print(f\"✅ 제출 파일 저장 완료: {output_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:52:11.459759Z",
          "iopub.execute_input": "2025-06-27T08:52:11.460607Z",
          "iopub.status.idle": "2025-06-27T08:52:13.049692Z",
          "shell.execute_reply.started": "2025-06-27T08:52:11.460575Z",
          "shell.execute_reply": "2025-06-27T08:52:13.048854Z"
        },
        "id": "9FziDhgAAVhv",
        "outputId": "4652700c-62b6-4a81-98df-78e8ae90455e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ 제출 파일 저장 완료: /kaggle/working/baseline_submit.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ✅ 데이터 경로 설정\n",
        "data_path = \"/kaggle/input/datafile1\"\n",
        "output_path = \"/kaggle/working/baseline_submit2.csv\"\n",
        "\n",
        "# ✅ 데이터 로드\n",
        "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "submission_df = pd.read_csv(f\"{data_path}/sample_submission.csv\").copy()  # <- 반드시 copy\n",
        "\n",
        "# ✅ 분자 특성 추출 함수 정의\n",
        "def extract_rdkit_features(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            return [\n",
        "                Descriptors.MolWt(mol),             # 분자량\n",
        "                Descriptors.MolLogP(mol),           # LogP\n",
        "                Descriptors.NumHAcceptors(mol),     # 수소 수용자 수\n",
        "                Descriptors.NumHDonors(mol),        # 수소 기부자 수\n",
        "                Descriptors.TPSA(mol),              # 극성 표면적\n",
        "                Descriptors.NumRotatableBonds(mol)  # 회전 가능한 결합 수\n",
        "            ]\n",
        "    except:\n",
        "        pass\n",
        "    return [0.0] * 6  # 실패 시 0으로 채움\n",
        "\n",
        "# ✅ 훈련 데이터 특성 생성\n",
        "train_df[\"Features\"] = train_df[\"Canonical_Smiles\"].apply(extract_rdkit_features)\n",
        "train_x = np.stack(train_df[\"Features\"].values)\n",
        "train_y = train_df[\"Inhibition\"].astype(float).values\n",
        "\n",
        "# ✅ 모델 학습\n",
        "model = xgb.XGBRegressor(random_state=42, n_estimators=300, learning_rate=0.05)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# ✅ 테스트 데이터 특성 생성\n",
        "test_df[\"Features\"] = test_df[\"Canonical_Smiles\"].apply(extract_rdkit_features)\n",
        "test_x = np.stack(test_df[\"Features\"].values)\n",
        "\n",
        "# ✅ 예측 수행\n",
        "test_y_pred = model.predict(test_x)\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission_df[\"Inhibition\"] = test_y_pred\n",
        "submission_df.to_csv(output_path, index=False)\n",
        "print(f\"✅ 제출 파일 저장 완료: {output_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:53:56.208651Z",
          "iopub.execute_input": "2025-06-27T08:53:56.20957Z",
          "iopub.status.idle": "2025-06-27T08:53:57.750629Z",
          "shell.execute_reply.started": "2025-06-27T08:53:56.209534Z",
          "shell.execute_reply": "2025-06-27T08:53:57.749936Z"
        },
        "id": "-fui-Qt8AVhw",
        "outputId": "fd108be5-f38f-40c1-8f88-a745790c6d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ 제출 파일 저장 완료: /kaggle/working/baseline_submit2.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- Kaggle 최적화 신약개발 예측 파이프라인 (PyCaret 기반) ---\n",
        "# ==============================================================================\n",
        "\n",
        "# ✅ 1. 라이브러리 임포트\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io # 예제 데이터 생성 시 필요 (현재는 사용하지 않음)\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# RDKit 및 PyCaret 라이브러리 임포트\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from pycaret.regression import setup, compare_models, finalize_model, predict_model, pull\n",
        "\n",
        "# 경고 메시지 무시 및 RDKit 에러 메시지 억제\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "\n",
        "print(f\"--- Kaggle 파이프라인 시작: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} ---\")\n",
        "\n",
        "# ✅ 2. 데이터 로드 (Kaggle 전용 경로)\n",
        "# Kaggle 데이터셋 경로를 사용합니다. 'datafile1'은 실제 데이터셋 이름에 따라 변경해야 할 수 있습니다.\n",
        "# 예를 들어, 대회 데이터셋 이름이 'dacon-cyp3a4-inhibition-prediction'이라면\n",
        "# data_path = \"/kaggle/input/dacon-cyp3a4-inhibition-prediction\" 으로 변경합니다.\n",
        "data_path = \"/kaggle/input/datafile1\" # 예시 경로\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(f'{data_path}/train.csv')\n",
        "    test_df = pd.read_csv(f'{data_path}/test.csv')\n",
        "    submission_df = pd.read_csv(f'{data_path}/sample_submission.csv').copy() # .copy() 추가\n",
        "    print(\"파일에서 데이터를 성공적으로 로드했습니다.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {data_path}\")\n",
        "    # Kaggle 환경에서는 FileNotFoundError 발생 시 실행이 중단되므로, 예외 처리는 주로 개발 환경에서 유용합니다.\n",
        "    # Kaggle에서는 데이터셋이 제대로 추가되었는지 확인해야 합니다.\n",
        "    raise # Kaggle에서는 이 시점에서 오류를 발생시켜 데이터셋 문제를 알립니다.\n",
        "\n",
        "\n",
        "# --- 1. 특성 추출 (Feature Engineering) - Morgan Fingerprints ---\n",
        "# 모든 RDKit Descriptors를 포함하는 것이 성능에 유리할 수 있습니다.\n",
        "# 이전 대화에서 제공했던 'featurize_all_rdkit' 함수를 사용하여 모든 Descriptors와 Fingerprint를 함께 사용하는 것을 고려하세요.\n",
        "# 현재 코드는 제공해주신 코드와 동일하게 Morgan Fingerprint만 사용합니다.\n",
        "def get_morgan_fingerprint(smiles_string, n_bits=2048, radius=2):\n",
        "    \"\"\"SMILES 문자열을 Morgan Fingerprint(numpy array)로 변환\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles_string)\n",
        "        if mol is not None:\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "            return np.array(fp, dtype=int) # PyCaret에 맞게 int 타입 유지\n",
        "        else:\n",
        "            return np.zeros(n_bits, dtype=int)\n",
        "    except:\n",
        "        return np.zeros(n_bits, dtype=int)\n",
        "\n",
        "print(\"\\n특성 추출을 시작합니다 (Morgan Fingerprints)...\")\n",
        "# 학습 데이터와 테스트 데이터에 함수 적용\n",
        "train_fps = train_df['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "test_fps = test_df['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "\n",
        "# --- PyCaret을 위한 데이터 준비 ---\n",
        "# Fingerprint를 DataFrame으로 변환\n",
        "fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "train_fp_df = pd.DataFrame(np.vstack(train_fps.values), columns=fp_columns)\n",
        "test_fp_df = pd.DataFrame(np.vstack(test_fps.values), columns=fp_columns)\n",
        "\n",
        "# PyCaret 학습을 위해 특성과 타겟 변수를 하나의 DataFrame으로 결합\n",
        "# Inhibition은 PyCaret 내부에서 자동으로 처리되므로 log1p 변환은 PyCaret 외부에서 수행하지 않습니다.\n",
        "# 만약 PyCaret 내부에서 log1p 변환을 원한다면, setup() 함수의 transformation=True, transformation_method='yeo-johnson' 등을 고려할 수 있습니다.\n",
        "pycaret_train_df = pd.concat([train_fp_df, train_df['Inhibition']], axis=1)\n",
        "\n",
        "print(f\"PyCaret에 사용할 학습 데이터 형태: {pycaret_train_df.shape}\")\n",
        "print(f\"PyCaret에 사용할 테스트 데이터 형태: {test_fp_df.shape}\")\n",
        "print(\"특성 추출 완료.\")\n",
        "\n",
        "# --- 2. PyCaret을 이용한 모델 학습 및 예측 ---\n",
        "print(\"\\nPyCaret 환경 설정을 시작합니다...\")\n",
        "# PyCaret 회귀 환경 설정\n",
        "# session_id는 재현성을 위해 고정합니다.\n",
        "# fold=5로 설정하여 5-Fold 교차 검증을 수행합니다.\n",
        "# normalize=True: MinMax 스케일링은 FP에 덜 중요할 수 있지만, Descriptors와 함께 사용 시 필요\n",
        "s = setup(data=pycaret_train_df,\n",
        "          target='Inhibition',\n",
        "          session_id=42,\n",
        "          fold=5, # KFold(n_splits=5)와 동일\n",
        "          normalize=True,\n",
        "          normalize_method='minmax', # MinMax 또는 zscore\n",
        "          n_jobs=-1, # Kaggle에서 모든 CPU 코어 사용\n",
        "          verbose=False) # 상세 정보 출력 생략 (필요시 True로 변경하여 디버깅)\n",
        "\n",
        "print(\"PyCaret 환경 설정 완료.\")\n",
        "\n",
        "print(\"\\n다양한 모델을 비교하여 최적의 모델을 찾습니다...\")\n",
        "# 여러 모델을 비교하고 RMSE 기준으로 가장 좋은 모델을 선택\n",
        "# include 파라미터로 특정 모델만 비교할 수 있습니다 (예: include=['lightgbm', 'xgboost', 'catboost'])\n",
        "best_model = compare_models(sort='RMSE', n_select=1, verbose=True) # 진행 과정 출력 확인\n",
        "\n",
        "print(\"\\n모델 비교 결과 (상위 5개):\")\n",
        "# pull() 함수를 사용하여 비교 결과를 DataFrame으로 가져와 출력\n",
        "results = pull()\n",
        "print(results.head())\n",
        "\n",
        "print(f\"\\n최적 모델로 '{results.index[0]}'가 선택되었습니다.\")\n",
        "\n",
        "# --- 3. 최종 모델 학습 및 예측 ---\n",
        "print(\"\\n최종 모델을 전체 데이터로 학습합니다...\")\n",
        "# 선택된 최적의 모델을 전체 학습 데이터로 다시 학습시킵니다.\n",
        "final_model = finalize_model(best_model)\n",
        "print(\"최종 모델 학습 완료.\")\n",
        "\n",
        "print(\"\\n테스트 데이터에 대한 예측을 수행합니다...\")\n",
        "# 최종 모델로 테스트 데이터 예측\n",
        "predictions = predict_model(final_model, data=test_fp_df)\n",
        "print(\"예측 완료.\")\n",
        "\n",
        "# --- 4. 제출 파일 생성 ---\n",
        "print(\"\\n제출 파일을 생성합니다...\")\n",
        "# 예측 결과가 0~100 범위를 벗어나지 않도록 클리핑\n",
        "# PyCaret 3.x 버전에서는 예측 결과 컬럼명이 'prediction_label' 입니다.\n",
        "final_predictions = np.clip(predictions['prediction_label'].values, 0, 100)\n",
        "\n",
        "submission_df['Inhibition'] = final_predictions\n",
        "\n",
        "# ✅ 5. 제출 파일 저장 (Kaggle 전용 경로)\n",
        "output_path = \"/kaggle/working/submission_pycaret.csv\"\n",
        "submission_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ 제출 파일 저장 완료: {output_path}\")\n",
        "print(\"\\n최종 제출 파일 미리보기:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "print(f\"\\n--- Kaggle 파이프라인 완료: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-27T08:56:48.693297Z",
          "iopub.execute_input": "2025-06-27T08:56:48.693663Z",
          "iopub.status.idle": "2025-06-27T09:02:52.142653Z",
          "shell.execute_reply.started": "2025-06-27T08:56:48.693639Z",
          "shell.execute_reply": "2025-06-27T09:02:52.141831Z"
        },
        "id": "b4A3A3oVAVhw",
        "outputId": "0b7c07dd-cbc2-4641-c62e-ac55ba816be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Kaggle 파이프라인 시작: 2025-06-27 08:56:53 ---\n파일에서 데이터를 성공적으로 로드했습니다.\n\n특성 추출을 시작합니다 (Morgan Fingerprints)...\nPyCaret에 사용할 학습 데이터 형태: (1681, 2049)\nPyCaret에 사용할 테스트 데이터 형태: (100, 2048)\n특성 추출 완료.\n\nPyCaret 환경 설정을 시작합니다...\nPyCaret 환경 설정 완료.\n\n다양한 모델을 비교하여 최적의 모델을 찾습니다...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<pandas.io.formats.style.Styler at 0x7d263ea3c0d0>",
            "text/html": "<style type=\"text/css\">\n#T_6f08b th {\n  text-align: left;\n}\n#T_6f08b_row0_col0, #T_6f08b_row0_col5, #T_6f08b_row0_col6, #T_6f08b_row1_col0, #T_6f08b_row1_col1, #T_6f08b_row1_col2, #T_6f08b_row1_col3, #T_6f08b_row1_col4, #T_6f08b_row1_col6, #T_6f08b_row2_col0, #T_6f08b_row2_col1, #T_6f08b_row2_col2, #T_6f08b_row2_col3, #T_6f08b_row2_col4, #T_6f08b_row2_col5, #T_6f08b_row2_col6, #T_6f08b_row3_col0, #T_6f08b_row3_col1, #T_6f08b_row3_col2, #T_6f08b_row3_col3, #T_6f08b_row3_col4, #T_6f08b_row3_col5, #T_6f08b_row3_col6, #T_6f08b_row4_col0, #T_6f08b_row4_col1, #T_6f08b_row4_col2, #T_6f08b_row4_col3, #T_6f08b_row4_col4, #T_6f08b_row4_col5, #T_6f08b_row4_col6, #T_6f08b_row5_col0, #T_6f08b_row5_col1, #T_6f08b_row5_col2, #T_6f08b_row5_col3, #T_6f08b_row5_col4, #T_6f08b_row5_col5, #T_6f08b_row5_col6, #T_6f08b_row6_col0, #T_6f08b_row6_col1, #T_6f08b_row6_col2, #T_6f08b_row6_col3, #T_6f08b_row6_col4, #T_6f08b_row6_col5, #T_6f08b_row6_col6, #T_6f08b_row7_col0, #T_6f08b_row7_col1, #T_6f08b_row7_col2, #T_6f08b_row7_col3, #T_6f08b_row7_col4, #T_6f08b_row7_col5, #T_6f08b_row7_col6, #T_6f08b_row8_col0, #T_6f08b_row8_col1, #T_6f08b_row8_col2, #T_6f08b_row8_col3, #T_6f08b_row8_col4, #T_6f08b_row8_col5, #T_6f08b_row8_col6, #T_6f08b_row9_col0, #T_6f08b_row9_col1, #T_6f08b_row9_col2, #T_6f08b_row9_col3, #T_6f08b_row9_col4, #T_6f08b_row9_col5, #T_6f08b_row9_col6, #T_6f08b_row10_col0, #T_6f08b_row10_col1, #T_6f08b_row10_col2, #T_6f08b_row10_col3, #T_6f08b_row10_col4, #T_6f08b_row10_col5, #T_6f08b_row10_col6, #T_6f08b_row11_col0, #T_6f08b_row11_col1, #T_6f08b_row11_col2, #T_6f08b_row11_col3, #T_6f08b_row11_col4, #T_6f08b_row11_col5, #T_6f08b_row12_col0, #T_6f08b_row12_col1, #T_6f08b_row12_col2, #T_6f08b_row12_col3, #T_6f08b_row12_col4, #T_6f08b_row12_col5, #T_6f08b_row12_col6, #T_6f08b_row13_col0, #T_6f08b_row13_col1, #T_6f08b_row13_col2, #T_6f08b_row13_col3, #T_6f08b_row13_col4, #T_6f08b_row13_col5, #T_6f08b_row13_col6, #T_6f08b_row14_col0, #T_6f08b_row14_col1, #T_6f08b_row14_col2, #T_6f08b_row14_col3, #T_6f08b_row14_col4, #T_6f08b_row14_col5, #T_6f08b_row14_col6, #T_6f08b_row15_col0, #T_6f08b_row15_col1, #T_6f08b_row15_col2, #T_6f08b_row15_col3, #T_6f08b_row15_col4, #T_6f08b_row15_col5, #T_6f08b_row15_col6, #T_6f08b_row16_col0, #T_6f08b_row16_col1, #T_6f08b_row16_col2, #T_6f08b_row16_col3, #T_6f08b_row16_col4, #T_6f08b_row16_col5, #T_6f08b_row16_col6, #T_6f08b_row17_col0, #T_6f08b_row17_col1, #T_6f08b_row17_col2, #T_6f08b_row17_col3, #T_6f08b_row17_col4, #T_6f08b_row17_col5, #T_6f08b_row17_col6, #T_6f08b_row18_col0, #T_6f08b_row18_col1, #T_6f08b_row18_col2, #T_6f08b_row18_col3, #T_6f08b_row18_col4, #T_6f08b_row18_col5, #T_6f08b_row18_col6, #T_6f08b_row19_col0, #T_6f08b_row19_col1, #T_6f08b_row19_col2, #T_6f08b_row19_col3, #T_6f08b_row19_col4, #T_6f08b_row19_col5, #T_6f08b_row19_col6 {\n  text-align: left;\n}\n#T_6f08b_row0_col1, #T_6f08b_row0_col2, #T_6f08b_row0_col3, #T_6f08b_row0_col4, #T_6f08b_row1_col5, #T_6f08b_row11_col6 {\n  text-align: left;\n  background-color: yellow;\n}\n#T_6f08b_row0_col7, #T_6f08b_row1_col7, #T_6f08b_row2_col7, #T_6f08b_row3_col7, #T_6f08b_row4_col7, #T_6f08b_row5_col7, #T_6f08b_row6_col7, #T_6f08b_row7_col7, #T_6f08b_row8_col7, #T_6f08b_row10_col7, #T_6f08b_row11_col7, #T_6f08b_row12_col7, #T_6f08b_row13_col7, #T_6f08b_row14_col7, #T_6f08b_row15_col7, #T_6f08b_row16_col7, #T_6f08b_row17_col7, #T_6f08b_row18_col7, #T_6f08b_row19_col7 {\n  text-align: left;\n  background-color: lightgrey;\n}\n#T_6f08b_row9_col7 {\n  text-align: left;\n  background-color: yellow;\n  background-color: lightgrey;\n}\n</style>\n<table id=\"T_6f08b\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_6f08b_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n      <th id=\"T_6f08b_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n      <th id=\"T_6f08b_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n      <th id=\"T_6f08b_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n      <th id=\"T_6f08b_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n      <th id=\"T_6f08b_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n      <th id=\"T_6f08b_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n      <th id=\"T_6f08b_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_6f08b_level0_row0\" class=\"row_heading level0 row0\" >br</th>\n      <td id=\"T_6f08b_row0_col0\" class=\"data row0 col0\" >Bayesian Ridge</td>\n      <td id=\"T_6f08b_row0_col1\" class=\"data row0 col1\" >21.0313</td>\n      <td id=\"T_6f08b_row0_col2\" class=\"data row0 col2\" >643.1184</td>\n      <td id=\"T_6f08b_row0_col3\" class=\"data row0 col3\" >25.3313</td>\n      <td id=\"T_6f08b_row0_col4\" class=\"data row0 col4\" >0.0841</td>\n      <td id=\"T_6f08b_row0_col5\" class=\"data row0 col5\" >1.3562</td>\n      <td id=\"T_6f08b_row0_col6\" class=\"data row0 col6\" >5.5619</td>\n      <td id=\"T_6f08b_row0_col7\" class=\"data row0 col7\" >0.9240</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row1\" class=\"row_heading level0 row1\" >rf</th>\n      <td id=\"T_6f08b_row1_col0\" class=\"data row1 col0\" >Random Forest Regressor</td>\n      <td id=\"T_6f08b_row1_col1\" class=\"data row1 col1\" >21.0550</td>\n      <td id=\"T_6f08b_row1_col2\" class=\"data row1 col2\" >661.6154</td>\n      <td id=\"T_6f08b_row1_col3\" class=\"data row1 col3\" >25.6895</td>\n      <td id=\"T_6f08b_row1_col4\" class=\"data row1 col4\" >0.0580</td>\n      <td id=\"T_6f08b_row1_col5\" class=\"data row1 col5\" >1.3258</td>\n      <td id=\"T_6f08b_row1_col6\" class=\"data row1 col6\" >5.1628</td>\n      <td id=\"T_6f08b_row1_col7\" class=\"data row1 col7\" >6.2240</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row2\" class=\"row_heading level0 row2\" >llar</th>\n      <td id=\"T_6f08b_row2_col0\" class=\"data row2 col0\" >Lasso Least Angle Regression</td>\n      <td id=\"T_6f08b_row2_col1\" class=\"data row2 col1\" >21.8003</td>\n      <td id=\"T_6f08b_row2_col2\" class=\"data row2 col2\" >678.5771</td>\n      <td id=\"T_6f08b_row2_col3\" class=\"data row2 col3\" >26.0170</td>\n      <td id=\"T_6f08b_row2_col4\" class=\"data row2 col4\" >0.0342</td>\n      <td id=\"T_6f08b_row2_col5\" class=\"data row2 col5\" >1.3839</td>\n      <td id=\"T_6f08b_row2_col6\" class=\"data row2 col6\" >5.8185</td>\n      <td id=\"T_6f08b_row2_col7\" class=\"data row2 col7\" >0.1440</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row3\" class=\"row_heading level0 row3\" >lasso</th>\n      <td id=\"T_6f08b_row3_col0\" class=\"data row3 col0\" >Lasso Regression</td>\n      <td id=\"T_6f08b_row3_col1\" class=\"data row3 col1\" >21.8003</td>\n      <td id=\"T_6f08b_row3_col2\" class=\"data row3 col2\" >678.5772</td>\n      <td id=\"T_6f08b_row3_col3\" class=\"data row3 col3\" >26.0170</td>\n      <td id=\"T_6f08b_row3_col4\" class=\"data row3 col4\" >0.0342</td>\n      <td id=\"T_6f08b_row3_col5\" class=\"data row3 col5\" >1.3839</td>\n      <td id=\"T_6f08b_row3_col6\" class=\"data row3 col6\" >5.8186</td>\n      <td id=\"T_6f08b_row3_col7\" class=\"data row3 col7\" >0.1700</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row4\" class=\"row_heading level0 row4\" >en</th>\n      <td id=\"T_6f08b_row4_col0\" class=\"data row4 col0\" >Elastic Net</td>\n      <td id=\"T_6f08b_row4_col1\" class=\"data row4 col1\" >21.8877</td>\n      <td id=\"T_6f08b_row4_col2\" class=\"data row4 col2\" >682.6070</td>\n      <td id=\"T_6f08b_row4_col3\" class=\"data row4 col3\" >26.0949</td>\n      <td id=\"T_6f08b_row4_col4\" class=\"data row4 col4\" >0.0284</td>\n      <td id=\"T_6f08b_row4_col5\" class=\"data row4 col5\" >1.3875</td>\n      <td id=\"T_6f08b_row4_col6\" class=\"data row4 col6\" >5.8932</td>\n      <td id=\"T_6f08b_row4_col7\" class=\"data row4 col7\" >0.1420</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row5\" class=\"row_heading level0 row5\" >gbr</th>\n      <td id=\"T_6f08b_row5_col0\" class=\"data row5 col0\" >Gradient Boosting Regressor</td>\n      <td id=\"T_6f08b_row5_col1\" class=\"data row5 col1\" >21.5632</td>\n      <td id=\"T_6f08b_row5_col2\" class=\"data row5 col2\" >684.5222</td>\n      <td id=\"T_6f08b_row5_col3\" class=\"data row5 col3\" >26.1268</td>\n      <td id=\"T_6f08b_row5_col4\" class=\"data row5 col4\" >0.0261</td>\n      <td id=\"T_6f08b_row5_col5\" class=\"data row5 col5\" >1.3760</td>\n      <td id=\"T_6f08b_row5_col6\" class=\"data row5 col6\" >5.7961</td>\n      <td id=\"T_6f08b_row5_col7\" class=\"data row5 col7\" >1.1220</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row6\" class=\"row_heading level0 row6\" >catboost</th>\n      <td id=\"T_6f08b_row6_col0\" class=\"data row6 col0\" >CatBoost Regressor</td>\n      <td id=\"T_6f08b_row6_col1\" class=\"data row6 col1\" >21.5711</td>\n      <td id=\"T_6f08b_row6_col2\" class=\"data row6 col2\" >694.4035</td>\n      <td id=\"T_6f08b_row6_col3\" class=\"data row6 col3\" >26.3328</td>\n      <td id=\"T_6f08b_row6_col4\" class=\"data row6 col4\" >0.0081</td>\n      <td id=\"T_6f08b_row6_col5\" class=\"data row6 col5\" >1.3714</td>\n      <td id=\"T_6f08b_row6_col6\" class=\"data row6 col6\" >5.6998</td>\n      <td id=\"T_6f08b_row6_col7\" class=\"data row6 col7\" >6.3020</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row7\" class=\"row_heading level0 row7\" >lightgbm</th>\n      <td id=\"T_6f08b_row7_col0\" class=\"data row7 col0\" >Light Gradient Boosting Machine</td>\n      <td id=\"T_6f08b_row7_col1\" class=\"data row7 col1\" >21.5410</td>\n      <td id=\"T_6f08b_row7_col2\" class=\"data row7 col2\" >695.4474</td>\n      <td id=\"T_6f08b_row7_col3\" class=\"data row7 col3\" >26.3534</td>\n      <td id=\"T_6f08b_row7_col4\" class=\"data row7 col4\" >0.0074</td>\n      <td id=\"T_6f08b_row7_col5\" class=\"data row7 col5\" >1.3694</td>\n      <td id=\"T_6f08b_row7_col6\" class=\"data row7 col6\" >5.2157</td>\n      <td id=\"T_6f08b_row7_col7\" class=\"data row7 col7\" >32.4780</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row8\" class=\"row_heading level0 row8\" >ada</th>\n      <td id=\"T_6f08b_row8_col0\" class=\"data row8 col0\" >AdaBoost Regressor</td>\n      <td id=\"T_6f08b_row8_col1\" class=\"data row8 col1\" >22.3830</td>\n      <td id=\"T_6f08b_row8_col2\" class=\"data row8 col2\" >695.5039</td>\n      <td id=\"T_6f08b_row8_col3\" class=\"data row8 col3\" >26.3583</td>\n      <td id=\"T_6f08b_row8_col4\" class=\"data row8 col4\" >0.0069</td>\n      <td id=\"T_6f08b_row8_col5\" class=\"data row8 col5\" >1.4323</td>\n      <td id=\"T_6f08b_row8_col6\" class=\"data row8 col6\" >6.5755</td>\n      <td id=\"T_6f08b_row8_col7\" class=\"data row8 col7\" >1.0860</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row9\" class=\"row_heading level0 row9\" >dummy</th>\n      <td id=\"T_6f08b_row9_col0\" class=\"data row9 col0\" >Dummy Regressor</td>\n      <td id=\"T_6f08b_row9_col1\" class=\"data row9 col1\" >22.2970</td>\n      <td id=\"T_6f08b_row9_col2\" class=\"data row9 col2\" >708.6996</td>\n      <td id=\"T_6f08b_row9_col3\" class=\"data row9 col3\" >26.5932</td>\n      <td id=\"T_6f08b_row9_col4\" class=\"data row9 col4\" >-0.0094</td>\n      <td id=\"T_6f08b_row9_col5\" class=\"data row9 col5\" >1.4002</td>\n      <td id=\"T_6f08b_row9_col6\" class=\"data row9 col6\" >6.0433</td>\n      <td id=\"T_6f08b_row9_col7\" class=\"data row9 col7\" >0.1220</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row10\" class=\"row_heading level0 row10\" >xgboost</th>\n      <td id=\"T_6f08b_row10_col0\" class=\"data row10 col0\" >Extreme Gradient Boosting</td>\n      <td id=\"T_6f08b_row10_col1\" class=\"data row10 col1\" >22.4551</td>\n      <td id=\"T_6f08b_row10_col2\" class=\"data row10 col2\" >758.8901</td>\n      <td id=\"T_6f08b_row10_col3\" class=\"data row10 col3\" >27.5265</td>\n      <td id=\"T_6f08b_row10_col4\" class=\"data row10 col4\" >-0.0839</td>\n      <td id=\"T_6f08b_row10_col5\" class=\"data row10 col5\" >1.3886</td>\n      <td id=\"T_6f08b_row10_col6\" class=\"data row10 col6\" >5.6420</td>\n      <td id=\"T_6f08b_row10_col7\" class=\"data row10 col7\" >0.7720</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row11\" class=\"row_heading level0 row11\" >knn</th>\n      <td id=\"T_6f08b_row11_col0\" class=\"data row11 col0\" >K Neighbors Regressor</td>\n      <td id=\"T_6f08b_row11_col1\" class=\"data row11 col1\" >23.1917</td>\n      <td id=\"T_6f08b_row11_col2\" class=\"data row11 col2\" >827.1634</td>\n      <td id=\"T_6f08b_row11_col3\" class=\"data row11 col3\" >28.7244</td>\n      <td id=\"T_6f08b_row11_col4\" class=\"data row11 col4\" >-0.1781</td>\n      <td id=\"T_6f08b_row11_col5\" class=\"data row11 col5\" >1.3847</td>\n      <td id=\"T_6f08b_row11_col6\" class=\"data row11 col6\" >5.0824</td>\n      <td id=\"T_6f08b_row11_col7\" class=\"data row11 col7\" >0.1460</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row12\" class=\"row_heading level0 row12\" >ridge</th>\n      <td id=\"T_6f08b_row12_col0\" class=\"data row12 col0\" >Ridge Regression</td>\n      <td id=\"T_6f08b_row12_col1\" class=\"data row12 col1\" >25.0705</td>\n      <td id=\"T_6f08b_row12_col2\" class=\"data row12 col2\" >967.8800</td>\n      <td id=\"T_6f08b_row12_col3\" class=\"data row12 col3\" >31.1009</td>\n      <td id=\"T_6f08b_row12_col4\" class=\"data row12 col4\" >-0.3876</td>\n      <td id=\"T_6f08b_row12_col5\" class=\"data row12 col5\" >1.4688</td>\n      <td id=\"T_6f08b_row12_col6\" class=\"data row12 col6\" >5.4593</td>\n      <td id=\"T_6f08b_row12_col7\" class=\"data row12 col7\" >0.2040</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row13\" class=\"row_heading level0 row13\" >huber</th>\n      <td id=\"T_6f08b_row13_col0\" class=\"data row13 col0\" >Huber Regressor</td>\n      <td id=\"T_6f08b_row13_col1\" class=\"data row13 col1\" >25.7914</td>\n      <td id=\"T_6f08b_row13_col2\" class=\"data row13 col2\" >1035.3330</td>\n      <td id=\"T_6f08b_row13_col3\" class=\"data row13 col3\" >32.1667</td>\n      <td id=\"T_6f08b_row13_col4\" class=\"data row13 col4\" >-0.4847</td>\n      <td id=\"T_6f08b_row13_col5\" class=\"data row13 col5\" >1.4677</td>\n      <td id=\"T_6f08b_row13_col6\" class=\"data row13 col6\" >5.5262</td>\n      <td id=\"T_6f08b_row13_col7\" class=\"data row13 col7\" >0.6940</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row14\" class=\"row_heading level0 row14\" >omp</th>\n      <td id=\"T_6f08b_row14_col0\" class=\"data row14 col0\" >Orthogonal Matching Pursuit</td>\n      <td id=\"T_6f08b_row14_col1\" class=\"data row14 col1\" >25.6402</td>\n      <td id=\"T_6f08b_row14_col2\" class=\"data row14 col2\" >1036.1041</td>\n      <td id=\"T_6f08b_row14_col3\" class=\"data row14 col3\" >32.1707</td>\n      <td id=\"T_6f08b_row14_col4\" class=\"data row14 col4\" >-0.4803</td>\n      <td id=\"T_6f08b_row14_col5\" class=\"data row14 col5\" >1.5010</td>\n      <td id=\"T_6f08b_row14_col6\" class=\"data row14 col6\" >5.4497</td>\n      <td id=\"T_6f08b_row14_col7\" class=\"data row14 col7\" >0.2240</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row15\" class=\"row_heading level0 row15\" >lar</th>\n      <td id=\"T_6f08b_row15_col0\" class=\"data row15 col0\" >Least Angle Regression</td>\n      <td id=\"T_6f08b_row15_col1\" class=\"data row15 col1\" >26.3204</td>\n      <td id=\"T_6f08b_row15_col2\" class=\"data row15 col2\" >1074.6430</td>\n      <td id=\"T_6f08b_row15_col3\" class=\"data row15 col3\" >32.7024</td>\n      <td id=\"T_6f08b_row15_col4\" class=\"data row15 col4\" >-0.5489</td>\n      <td id=\"T_6f08b_row15_col5\" class=\"data row15 col5\" >1.5017</td>\n      <td id=\"T_6f08b_row15_col6\" class=\"data row15 col6\" >5.7950</td>\n      <td id=\"T_6f08b_row15_col7\" class=\"data row15 col7\" >0.5160</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row16\" class=\"row_heading level0 row16\" >lr</th>\n      <td id=\"T_6f08b_row16_col0\" class=\"data row16 col0\" >Linear Regression</td>\n      <td id=\"T_6f08b_row16_col1\" class=\"data row16 col1\" >26.8370</td>\n      <td id=\"T_6f08b_row16_col2\" class=\"data row16 col2\" >1120.3078</td>\n      <td id=\"T_6f08b_row16_col3\" class=\"data row16 col3\" >33.4555</td>\n      <td id=\"T_6f08b_row16_col4\" class=\"data row16 col4\" >-0.6090</td>\n      <td id=\"T_6f08b_row16_col5\" class=\"data row16 col5\" >1.4788</td>\n      <td id=\"T_6f08b_row16_col6\" class=\"data row16 col6\" >5.5461</td>\n      <td id=\"T_6f08b_row16_col7\" class=\"data row16 col7\" >1.3540</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row17\" class=\"row_heading level0 row17\" >par</th>\n      <td id=\"T_6f08b_row17_col0\" class=\"data row17 col0\" >Passive Aggressive Regressor</td>\n      <td id=\"T_6f08b_row17_col1\" class=\"data row17 col1\" >26.8351</td>\n      <td id=\"T_6f08b_row17_col2\" class=\"data row17 col2\" >1120.6516</td>\n      <td id=\"T_6f08b_row17_col3\" class=\"data row17 col3\" >33.4614</td>\n      <td id=\"T_6f08b_row17_col4\" class=\"data row17 col4\" >-0.6091</td>\n      <td id=\"T_6f08b_row17_col5\" class=\"data row17 col5\" >1.4749</td>\n      <td id=\"T_6f08b_row17_col6\" class=\"data row17 col6\" >5.6060</td>\n      <td id=\"T_6f08b_row17_col7\" class=\"data row17 col7\" >0.4940</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row18\" class=\"row_heading level0 row18\" >et</th>\n      <td id=\"T_6f08b_row18_col0\" class=\"data row18 col0\" >Extra Trees Regressor</td>\n      <td id=\"T_6f08b_row18_col1\" class=\"data row18 col1\" >27.6817</td>\n      <td id=\"T_6f08b_row18_col2\" class=\"data row18 col2\" >1245.2112</td>\n      <td id=\"T_6f08b_row18_col3\" class=\"data row18 col3\" >35.2349</td>\n      <td id=\"T_6f08b_row18_col4\" class=\"data row18 col4\" >-0.7851</td>\n      <td id=\"T_6f08b_row18_col5\" class=\"data row18 col5\" >1.7605</td>\n      <td id=\"T_6f08b_row18_col6\" class=\"data row18 col6\" >5.3733</td>\n      <td id=\"T_6f08b_row18_col7\" class=\"data row18 col7\" >10.6280</td>\n    </tr>\n    <tr>\n      <th id=\"T_6f08b_level0_row19\" class=\"row_heading level0 row19\" >dt</th>\n      <td id=\"T_6f08b_row19_col0\" class=\"data row19 col0\" >Decision Tree Regressor</td>\n      <td id=\"T_6f08b_row19_col1\" class=\"data row19 col1\" >28.5123</td>\n      <td id=\"T_6f08b_row19_col2\" class=\"data row19 col2\" >1321.7099</td>\n      <td id=\"T_6f08b_row19_col3\" class=\"data row19 col3\" >36.2942</td>\n      <td id=\"T_6f08b_row19_col4\" class=\"data row19 col4\" >-0.8977</td>\n      <td id=\"T_6f08b_row19_col5\" class=\"data row19 col5\" >1.8726</td>\n      <td id=\"T_6f08b_row19_col6\" class=\"data row19 col6\" >5.2563</td>\n      <td id=\"T_6f08b_row19_col7\" class=\"data row19 col7\" >0.2640</td>\n    </tr>\n  </tbody>\n</table>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n모델 비교 결과 (상위 5개):\n                              Model      MAE       MSE     RMSE      R2  \\\nbr                   Bayesian Ridge  21.0313  643.1184  25.3313  0.0841   \nrf          Random Forest Regressor  21.0550  661.6154  25.6895  0.0580   \nllar   Lasso Least Angle Regression  21.8003  678.5771  26.0170  0.0342   \nlasso              Lasso Regression  21.8003  678.5772  26.0170  0.0342   \nen                      Elastic Net  21.8877  682.6070  26.0949  0.0284   \n\n        RMSLE    MAPE  TT (Sec)  \nbr     1.3562  5.5619     0.924  \nrf     1.3258  5.1628     6.224  \nllar   1.3839  5.8185     0.144  \nlasso  1.3839  5.8186     0.170  \nen     1.3875  5.8932     0.142  \n\n최적 모델로 'br'가 선택되었습니다.\n\n최종 모델을 전체 데이터로 학습합니다...\n최종 모델 학습 완료.\n\n테스트 데이터에 대한 예측을 수행합니다...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "예측 완료.\n\n제출 파일을 생성합니다...\n✅ 제출 파일 저장 완료: /kaggle/working/submission_pycaret.csv\n\n최종 제출 파일 미리보기:\n         ID  Inhibition\n0  TEST_000   34.716853\n1  TEST_001   29.709531\n2  TEST_002   25.155910\n3  TEST_003   38.779637\n4  TEST_004   26.630452\n\n--- Kaggle 파이프라인 완료: 2025-06-27 09:02:52 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 1. 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import AllChem\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # ✅ 2. 데이터 로드\n",
        "# data_path = \"/kaggle/input/datafile1\"\n",
        "# train = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "# test = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "# submission = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
        "\n",
        "# # ✅ 3. Fingerprint 변환 함수\n",
        "# def smiles_to_morgan(smiles_list, radius=2, n_bits=2048):\n",
        "#     fps = []\n",
        "#     for smi in tqdm(smiles_list):\n",
        "#         mol = Chem.MolFromSmiles(smi)\n",
        "#         if mol is not None:\n",
        "#             fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "#             arr = np.zeros((1,), dtype=int)\n",
        "#             AllChem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "#             fps.append(arr)\n",
        "#         else:\n",
        "#             fps.append(np.zeros((n_bits,), dtype=int))  # 실패한 경우 0벡터\n",
        "#     return np.array(fps)\n",
        "\n",
        "# # ✅ 4. Feature Engineering\n",
        "# X = smiles_to_morgan(train[\"Canonical_Smiles\"])\n",
        "# X_test = smiles_to_morgan(test[\"Canonical_Smiles\"])\n",
        "# y = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 5. 모델 훈련 (LightGBM + 5-Fold CV)\n",
        "# preds = np.zeros(X_test.shape[0])\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# mae_scores = []\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X):\n",
        "#     X_tr, X_val = X[train_idx], X[val_idx]\n",
        "#     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "#     model = LGBMRegressor(random_state=42)\n",
        "#     model.fit(X_tr, y_tr)\n",
        "\n",
        "#     val_pred = model.predict(X_val)\n",
        "#     mae = mean_absolute_error(y_val, val_pred)\n",
        "#     mae_scores.append(mae)\n",
        "\n",
        "#     preds += model.predict(X_test) / kf.n_splits\n",
        "\n",
        "# print(\"✅ 평균 MAE:\", np.mean(mae_scores))\n",
        "\n",
        "# # ✅ 6. 제출 파일 생성\n",
        "# submission[\"Inhibition\"] = preds\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n",
        "# print(\"🎉 submission.csv 저장 완료!\")\n",
        "# #"
      ],
      "metadata": {
        "trusted": true,
        "id": "OAt8K5s2AVhx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import warnings\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from pycaret.regression import *\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ✅ 실제 Kaggle 경로 사용\n",
        "train_path = \"/kaggle/input/datafile1/train.csv\"\n",
        "test_path = \"/kaggle/input/datafile1/test.csv\"\n",
        "submission_path = \"/kaggle/input/datafile1/sample_submission.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "submission_df = pd.read_csv(submission_path)\n",
        "\n",
        "print(\"✅ train_df:\", train_df.shape)\n",
        "print(\"✅ test_df:\", test_df.shape)\n",
        "print(\"✅ submission_df:\", submission_df.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aU2-A68PAVhy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_morgan_fingerprint(smiles_string, n_bits=2048, radius=2):\n",
        "    \"\"\"SMILES → Morgan Fingerprint (numpy array)\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles_string)\n",
        "        if mol is not None:\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "            return np.array(fp)\n",
        "        else:\n",
        "            return np.zeros(n_bits, dtype=int)\n",
        "    except:\n",
        "        return np.zeros(n_bits, dtype=int)\n",
        "\n",
        "# Fingerprint 추출\n",
        "train_fps = train_df['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "test_fps = test_df['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "\n",
        "# DataFrame 변환\n",
        "fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "train_fp_df = pd.DataFrame(np.vstack(train_fps.values), columns=fp_columns)\n",
        "test_fp_df = pd.DataFrame(np.vstack(test_fps.values), columns=fp_columns)\n",
        "\n",
        "# PyCaret용 최종 학습 데이터\n",
        "pycaret_train_df = pd.concat([train_fp_df, train_df['Inhibition']], axis=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8do4dugQAVhy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ PyCaret 환경 설정\n",
        "s = setup(data=pycaret_train_df,\n",
        "          target='Inhibition',\n",
        "          session_id=42,\n",
        "          fold=5,\n",
        "          normalize=True,\n",
        "          normalize_method='minmax',\n",
        "          verbose=False)\n",
        "\n",
        "# ✅ 여러 모델 비교 및 최적 모델 선택\n",
        "best_model = compare_models(sort='RMSE', n_select=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ey26UtZ-AVhy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 최종 모델 학습\n",
        "final_model = finalize_model(best_model)\n",
        "\n",
        "# ✅ 테스트 데이터 예측\n",
        "predictions = predict_model(final_model, data=test_fp_df)\n",
        "\n",
        "# ✅ 예측값 0~100 범위로 클리핑\n",
        "final_predictions = np.clip(predictions['prediction_label'].values, 0, 100)\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission_df['Inhibition'] = final_predictions\n",
        "submission_df.to_csv(\"/kaggle/working/submission_pycaret.csv\", index=False)\n",
        "\n",
        "print(\"✅ 제출 파일 생성 완료 → submission_pycaret.csv\")\n",
        "submission_df.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nRvLSZ_dAVhy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from pycaret.regression import setup, compare_models, finalize_model, predict_model\n",
        "import warnings\n",
        "from rdkit import rdBase\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.error') # RDKit 에러 메시지 억제\n",
        "\n",
        "# ✅ 실제 Kaggle 경로 사용\n",
        "train_path = \"/kaggle/input/datafile1/train.csv\"\n",
        "test_path = \"/kaggle/input/datafile1/test.csv\"\n",
        "submission_path = \"/kaggle/input/datafile1/sample_submission.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "submission_df = pd.read_csv(submission_path)\n",
        "\n",
        "# --- 개선된 피처 추출 함수 (RDKit Descriptors + Morgan Fingerprints) ---\n",
        "def get_all_features(smiles_string, n_bits=2048, radius=2):\n",
        "    mol = Chem.MolFromSmiles(str(smiles_string))\n",
        "    if mol is None:\n",
        "        num_descriptors = len(Descriptors._descList)\n",
        "        return np.zeros(num_descriptors + n_bits, dtype=float)\n",
        "\n",
        "    descriptor_values = []\n",
        "    for desc_name, desc_func in Descriptors._descList:\n",
        "        try:\n",
        "            val = desc_func(mol)\n",
        "            if np.isnan(val) or np.isinf(val):\n",
        "                descriptor_values.append(0.0) # NaN 또는 Inf는 0으로 처리\n",
        "            else:\n",
        "                descriptor_values.append(val)\n",
        "        except:\n",
        "            descriptor_values.append(0.0) # 계산 오류 시 0으로 처리\n",
        "\n",
        "    try:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "        morgan_fp_values = np.array(fp, dtype=float)\n",
        "    except:\n",
        "        morgan_fp_values = np.zeros(n_bits, dtype=float) # 지문 계산 오류 시 0으로 처리\n",
        "\n",
        "    return np.concatenate([descriptor_values, morgan_fp_values])\n",
        "\n",
        "print(\"피처 추출 시작 (RDKit Descriptors + Morgan Fingerprints)...\")\n",
        "\n",
        "# 모든 SMILES에 대해 피처 추출\n",
        "train_features_raw = np.vstack(train_df['Canonical_Smiles'].apply(get_all_features).values)\n",
        "test_features_raw = np.vstack(test_df['Canonical_Smiles'].apply(get_all_features).values)\n",
        "\n",
        "# 컬럼 이름 생성 (Descriptors + Fingerprints)\n",
        "descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
        "fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "all_feature_columns = descriptor_names + fp_columns\n",
        "\n",
        "train_features_df = pd.DataFrame(train_features_raw, columns=all_feature_columns)\n",
        "test_features_df = pd.DataFrame(test_features_raw, columns=all_feature_columns)\n",
        "\n",
        "print(f\"훈련 데이터 피처 형태: {train_features_df.shape}\")\n",
        "print(f\"테스트 데이터 피처 형태: {test_features_df.shape}\")\n",
        "\n",
        "# PyCaret용 최종 학습 데이터\n",
        "pycaret_train_df = pd.concat([train_features_df, train_df['Inhibition']], axis=1)\n",
        "\n",
        "# ✅ PyCaret 환경 설정 (normalize=True 유지, minmax 또는 zscore 고려)\n",
        "s = setup(data=pycaret_train_df,\n",
        "          target='Inhibition',\n",
        "          session_id=42,\n",
        "          fold=5,\n",
        "          normalize=True,\n",
        "          normalize_method='zscore', # 'minmax' 또는 'zscore' 시도\n",
        "          transformation=False, # 데이터 분포가 심하게 왜곡되지 않았다면 우선 False\n",
        "          n_jobs=-1, # CPU 코어 전체 사용\n",
        "          verbose=False)\n",
        "\n",
        "# ✅ 여러 모델 비교 및 최적 모델 선택\n",
        "# 앙상블 모델(LGBM, XGBoost, CatBoost)이 좋은 성능을 보일 수 있습니다.\n",
        "best_model = compare_models(sort='RMSE', n_select=1, include=['lightgbm', 'xgboost', 'catboost', 'et'])\n",
        "# 'et' (Extra Trees Regressor)는 무작위성을 높여 과적합을 줄이는 데 도움이 될 수 있습니다.\n",
        "\n",
        "# ✅ 최종 모델 학습\n",
        "final_model = finalize_model(best_model)\n",
        "\n",
        "# ✅ 테스트 데이터 예측\n",
        "predictions = predict_model(final_model, data=test_features_df)\n",
        "\n",
        "# ✅ 예측값 0~100 범위로 클리핑\n",
        "final_predictions = np.clip(predictions['prediction_label'].values, 0, 100)\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission_df['Inhibition'] = final_predictions\n",
        "submission_df.to_csv(\"/kaggle/working/submission_pycaret_enhanced_features.csv\", index=False)\n",
        "\n",
        "print(\"✅ 제출 파일 생성 완료 → submission_pycaret_enhanced_features.csv\")\n",
        "submission_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "HArIkctPAVhy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. 필수 라이브러리\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ✅ 2. 경로 설정\n",
        "train_path = \"/kaggle/input/datafile1/train.csv\"\n",
        "test_path = \"/kaggle/input/datafile1/test.csv\"\n",
        "submission_path = \"/kaggle/input/datafile1/sample_submission.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "submission_df = pd.read_csv(submission_path)\n",
        "\n",
        "# ✅ 3. 피처 추출 함수 (Descriptors + Morgan Fingerprint)\n",
        "def get_all_features(smiles_string, n_bits=2048, radius=2):\n",
        "    mol = Chem.MolFromSmiles(str(smiles_string))\n",
        "    if mol is None:\n",
        "        num_descriptors = len(Descriptors._descList)\n",
        "        return np.zeros(num_descriptors + n_bits, dtype=float)\n",
        "\n",
        "    descriptor_values = []\n",
        "    for desc_name, desc_func in Descriptors._descList:\n",
        "        try:\n",
        "            val = desc_func(mol)\n",
        "            if np.isnan(val) or np.isinf(val):\n",
        "                descriptor_values.append(0.0)\n",
        "            else:\n",
        "                descriptor_values.append(val)\n",
        "        except:\n",
        "            descriptor_values.append(0.0)\n",
        "\n",
        "    try:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "        morgan_fp_values = np.array(fp, dtype=float)\n",
        "    except:\n",
        "        morgan_fp_values = np.zeros(n_bits, dtype=float)\n",
        "\n",
        "    return np.concatenate([descriptor_values, morgan_fp_values])\n",
        "\n",
        "# ✅ 4. 피처 생성\n",
        "X_train = np.vstack(train_df['Canonical_Smiles'].apply(get_all_features).values)\n",
        "X_test = np.vstack(test_df['Canonical_Smiles'].apply(get_all_features).values)\n",
        "y_train = train_df['Inhibition'].values\n",
        "\n",
        "# ✅ 5. Optuna 튜닝 함수 정의\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'n_estimators': 10000,\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 128),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10),\n",
        "    }\n",
        "\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_list = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X_train):\n",
        "        X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
        "        y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            callbacks=[early_stopping(50, verbose=False), log_evaluation(0)]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse_list.append(rmse)\n",
        "\n",
        "    return np.mean(rmse_list)\n",
        "\n",
        "# ✅ 6. Optuna 최적화 실행\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "# ✅ 7. 최적 파라미터로 최종 학습\n",
        "best_params = study.best_params\n",
        "best_params.update({\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'verbosity': -1,\n",
        "    'n_estimators': 10000\n",
        "})\n",
        "\n",
        "final_model = lgb.LGBMRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# ✅ 8. 테스트 예측 및 제출\n",
        "preds_test = final_model.predict(X_test)\n",
        "submission_df['Inhibition'] = np.clip(preds_test, 0, 100)\n",
        "submission_df.to_csv(\"/kaggle/working/submission_lgb_optuna.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "K7VPrAFeAVhz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. 라이브러리\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "# ✅ 2. 데이터 로딩\n",
        "train = pd.read_csv(\"/kaggle/input/datafile1/train.csv\")\n",
        "test = pd.read_csv(\"/kaggle/input/datafile1/test.csv\")\n",
        "submission = pd.read_csv(\"/kaggle/input/datafile1/sample_submission.csv\")\n",
        "\n",
        "# ✅ 3. Feature Engineering (중요한 분자 feature 추출)\n",
        "def featurize(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return [0]*6\n",
        "    return [\n",
        "        Descriptors.MolWt(mol),         # 분자량\n",
        "        Descriptors.MolLogP(mol),       # 지용성\n",
        "        Descriptors.NumHDonors(mol),    # 수소 결합 donor 수\n",
        "        Descriptors.NumHAcceptors(mol), # 수소 결합 acceptor 수\n",
        "        Descriptors.TPSA(mol),          # 극성 표면적\n",
        "        Descriptors.NumRotatableBonds(mol) # 회전 가능한 결합 수\n",
        "    ]\n",
        "\n",
        "feature_names = ['MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'RotatableBonds']\n",
        "\n",
        "train_feats = train['Canonical_Smiles'].apply(featurize)\n",
        "test_feats = test['Canonical_Smiles'].apply(featurize)\n",
        "\n",
        "X = pd.DataFrame(train_feats.tolist(), columns=feature_names)\n",
        "X_test = pd.DataFrame(test_feats.tolist(), columns=feature_names)\n",
        "y = np.log1p(train['Inhibition'])  # ✅ log1p 변환\n",
        "\n",
        "# ✅ 4. 모델 정의 (Stacking)\n",
        "base_models = [\n",
        "    ('xgb', XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
        "    ('lgbm', LGBMRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
        "    ('cat', CatBoostRegressor(verbose=0, n_estimators=100, learning_rate=0.1, depth=3, random_state=42))\n",
        "]\n",
        "\n",
        "meta_model = RidgeCV()\n",
        "\n",
        "model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ✅ 5. 학습\n",
        "model.fit(X, y)\n",
        "\n",
        "# ✅ 6. 예측 및 역변환\n",
        "preds = model.predict(X_test)\n",
        "submission['Inhibition'] = np.clip(np.expm1(preds), 0, 100)\n",
        "submission.to_csv(\"submission_stacking_log1p_feats.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9YYnnqyBAVhz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.ensemble import StackingRegressor\n",
        "# from sklearn.linear_model import RidgeCV\n",
        "# from xgboost import XGBRegressor\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from catboost import CatBoostRegressor\n",
        "# from sklearn.metrics import mean_squared_error, mean_absolute_error # MAE도 함께 확인 권장\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, AllChem # AllChem 임포트\n",
        "# import warnings\n",
        "# from rdkit import rdBase\n",
        "\n",
        "# warnings.filterwarnings('ignore')\n",
        "# rdBase.DisableLog('rdApp.error') # RDKit 에러 메시지 억제\n",
        "\n",
        "# # ✅ 2. 데이터 로딩\n",
        "# train = pd.read_csv(\"/kaggle/input/datafile1/train.csv\")\n",
        "# test = pd.read_csv(\"/kaggle/input/datafile1/test.csv\")\n",
        "# submission = pd.read_csv(\"/kaggle/input/datafile1/sample_submission.csv\")\n",
        "\n",
        "# # ✅ 3. Feature Engineering (RDKit Descriptors + Morgan Fingerprints로 변경)\n",
        "# def featurize_all(smiles_string, n_bits=2048, radius=2):\n",
        "#     mol = Chem.MolFromSmiles(str(smiles_string))\n",
        "#     if mol is None:\n",
        "#         # 분자 생성 실패 시 모든 피처를 0으로 채움\n",
        "#         num_descriptors = len(Descriptors._descList)\n",
        "#         return np.zeros(num_descriptors + n_bits, dtype=float)\n",
        "\n",
        "#     descriptor_values = []\n",
        "#     for desc_name, desc_func in Descriptors._descList:\n",
        "#         try:\n",
        "#             val = desc_func(mol)\n",
        "#             if np.isnan(val) or np.isinf(val):\n",
        "#                 descriptor_values.append(0.0) # NaN 또는 Inf는 0으로 처리\n",
        "#             else:\n",
        "#                 descriptor_values.append(val)\n",
        "#         except:\n",
        "#             descriptor_values.append(0.0) # 계산 오류 시 0으로 처리\n",
        "\n",
        "#     try:\n",
        "#         fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "#         morgan_fp_values = np.array(fp, dtype=float)\n",
        "#     except:\n",
        "#         morgan_fp_values = np.zeros(n_bits, dtype=float) # 지문 계산 오류 시 0으로 처리\n",
        "\n",
        "#     return np.concatenate([descriptor_values, morgan_fp_values])\n",
        "\n",
        "# print(\"피처 추출 시작 (RDKit Descriptors + Morgan Fingerprints)...\")\n",
        "\n",
        "# train_features_raw = train['Canonical_Smiles'].apply(featurize_all)\n",
        "# test_features_raw = test['Canonical_Smiles'].apply(featurize_all)\n",
        "\n",
        "# # DataFrame 변환\n",
        "# # 컬럼 이름 생성\n",
        "# descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
        "# fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "# feature_names_all = descriptor_names + fp_columns\n",
        "\n",
        "# X = pd.DataFrame(train_features_raw.tolist(), columns=feature_names_all)\n",
        "# X_test = pd.DataFrame(test_features_raw.tolist(), columns=feature_names_all)\n",
        "# y = train['Inhibition'] # ✅ log1p 변환은 Stacking 모델에 따라 선택적으로 적용\n",
        "\n",
        "# print(f\"훈련 데이터 피처 형태: {X.shape}\")\n",
        "# print(f\"테스트 데이터 피처 형태: {X_test.shape}\")\n",
        "\n",
        "\n",
        "# # ✅ 4. 모델 정의 (Stacking) - StandardScaler 파이프라인에 추가\n",
        "# # 피처 수가 많아지면 스케일링이 중요해집니다.\n",
        "# base_models = [\n",
        "#     ('xgb', XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.05, random_state=42, n_jobs=-1)),\n",
        "#     ('lgbm', LGBMRegressor(n_estimators=300, max_depth=5, learning_rate=0.05, random_state=42, n_jobs=-1, verbose=-1)),\n",
        "#     ('cat', CatBoostRegressor(verbose=0, n_estimators=300, learning_rate=0.05, depth=6, random_state=42))\n",
        "# ]\n",
        "\n",
        "# meta_model = RidgeCV(cv=5) # RidgeCV도 내부적으로 CV를 수행합니다.\n",
        "\n",
        "# # Pipeline을 사용하여 스케일러와 스태킹 모델을 연결\n",
        "# # StackingRegressor는 내부적으로 자체 CV를 수행하므로, 파이프라인의 CV와 혼동하지 않도록 주의.\n",
        "# # 여기서는 피처 스케일링을 파이프라인에 포함시키는 것이 목적.\n",
        "# pipeline_model = Pipeline([\n",
        "#     ('scaler', StandardScaler()), # 데이터 스케일링 추가\n",
        "#     ('stacking', StackingRegressor(\n",
        "#         estimators=base_models,\n",
        "#         final_estimator=meta_model,\n",
        "#         cv=KFold(n_splits=5, shuffle=True, random_state=42), # 명시적으로 KFold 사용 (GroupKFold 문제 우회)\n",
        "#         n_jobs=-1,\n",
        "#         passthrough=True # 기본 모델의 예측 외에 원본 피처도 meta_model에 전달\n",
        "#     ))\n",
        "# ])\n",
        "\n",
        "# # ✅ 5. 학습\n",
        "# print(\"\\n모델 학습 시작...\")\n",
        "# pipeline_model.fit(X, y) # y는 이미 스케일링되지 않은 원래 값을 사용\n",
        "\n",
        "# # ✅ 6. 예측 및 역변환\n",
        "# print(\"예측 수행 및 제출 파일 생성...\")\n",
        "# preds = pipeline_model.predict(X_test)\n",
        "# submission['Inhibition'] = np.clip(preds, 0, 100) # 예측값 클리핑 (log1p를 사용하지 않았으므로 expm1 불필요)\n",
        "# submission.to_csv(\"submission_stacking_enhanced_feats.csv\", index=False)\n",
        "\n",
        "# print(\"\\n제출 파일 생성 완료 → submission_stacking_enhanced_feats.csv\")\n",
        "# print(submission.head())\n",
        "\n",
        "# # (선택 사항) 훈련 데이터에 대한 성능 확인\n",
        "# train_preds = pipeline_model.predict(X)\n",
        "# train_mae = mean_absolute_error(y, np.clip(train_preds, 0, 100))\n",
        "# train_rmse = np.sqrt(mean_squared_error(y, np.clip(train_preds, 0, 100)))\n",
        "# print(f\"훈련 데이터 MAE: {train_mae:.4f}\")\n",
        "# print(f\"훈련 데이터 RMSE: {train_rmse:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "5VhnSsTdAVhz"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}