{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p3yacEnfyNGeB1znN7SjeBRDZtjT-zM3",
      "authorship_tag": "ABX9TyNa5znH2uOYa4xtw0Y9RSf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/dacon_new_drug/blob/main/start(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ouk01fRgJCpo",
        "outputId": "977edf87-5de2-4827-cf86-cb811f80810a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# google Drive ÎßàÏö¥Ìä∏\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/data.zip'\n",
        "extract_path = '/content/project_data'  # ÏõêÌïòÎäî Í≤ΩÎ°ú\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ],
      "metadata": {
        "id": "5GSwOCNKKgHv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = '/content/project_data/'  # ÏïïÏ∂ï Ìï¥Ï†ú Í≤ΩÎ°ú\n",
        "train = pd.read_csv(path + 'train.csv')\n",
        "test = pd.read_csv(path + 'test.csv')\n",
        "submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "print(train.shape, test.shape, submission.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gor_BqOTLB9m",
        "outputId": "5e922b90-ca9a-4a09-b24d-6981f9fbbc13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1681, 3) (100, 2) (100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDKit ÏÑ§Ïπò (ColabÏóêÏÑú Í∞ÄÎä•)\n",
        "!pip uninstall -y rdkit-pypi\n",
        "!pip install rdkit-pypi==2022.9.5 optuna xgboost\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oVRgT8ZLQue",
        "outputId": "0ed09ae6-374f-4067-f424-5b3a16c4a876"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rdkit-pypi 2022.9.5\n",
            "Uninstalling rdkit-pypi-2022.9.5:\n",
            "  Successfully uninstalled rdkit-pypi-2022.9.5\n",
            "Collecting rdkit-pypi==2022.9.5\n",
            "  Using cached rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (11.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Using cached rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ‚úÖ RDKit + XGBoost + Optuna Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Lipinski, MACCSkeys, AllChem\n",
        "# import xgboost as xgb\n",
        "# import optuna\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # ‚úÖ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "# path = '/content/project_data/'\n",
        "# train = pd.read_csv(path + 'train.csv')\n",
        "# test = pd.read_csv(path + 'test.csv')\n",
        "# submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "# # ‚úÖ Î∂ÑÏûê ÌäπÏÑ± Ï∂îÏ∂ú Ìï®Ïàò\n",
        "# def get_molecule_descriptors(smiles):\n",
        "#     try:\n",
        "#         mol = Chem.MolFromSmiles(smiles)\n",
        "#         if mol is None: return [0] * 2232\n",
        "#         basic = [\n",
        "#             Descriptors.MolWt(mol), Descriptors.MolLogP(mol),\n",
        "#             Descriptors.NumHAcceptors(mol), Descriptors.NumHDonors(mol),\n",
        "#             Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol),\n",
        "#             Descriptors.NumAromaticRings(mol), Descriptors.NumHeteroatoms(mol),\n",
        "#             Descriptors.FractionCSP3(mol), Descriptors.NumAliphaticRings(mol),\n",
        "#             Lipinski.NumAromaticHeterocycles(mol), Lipinski.NumSaturatedHeterocycles(mol),\n",
        "#             Lipinski.NumAliphaticHeterocycles(mol), Descriptors.HeavyAtomCount(mol),\n",
        "#             Descriptors.RingCount(mol), Descriptors.NOCount(mol),\n",
        "#             Descriptors.NHOHCount(mol), Descriptors.NumRadicalElectrons(mol)\n",
        "#         ]\n",
        "#         morgan = [int(b) for b in AllChem.GetMorganFingerprintAsBitVect(mol, 2, 2048).ToBitString()]\n",
        "#         maccs = [int(b) for b in MACCSkeys.GenMACCSKeys(mol).ToBitString()]\n",
        "#         return basic + morgan + maccs\n",
        "#     except:\n",
        "#         return [0] * 2232\n",
        "\n",
        "# # ‚úÖ ÌîºÏ≤ò ÏÉùÏÑ±\n",
        "# train['features'] = train['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# test['features'] = test['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# X_train = np.array(train['features'].tolist())\n",
        "# y_train = train['Inhibition'].values\n",
        "# X_test = np.array(test['features'].tolist())\n",
        "\n",
        "# # ‚úÖ Ï†ïÍ∑úÌôî\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ‚úÖ ÌèâÍ∞Ä Ìï®Ïàò\n",
        "# def normalized_rmse(y_true, y_pred):\n",
        "#     return np.sqrt(mean_squared_error(y_true, y_pred)) / (np.max(y_true) - np.min(y_true))\n",
        "# def pearson_correlation(y_true, y_pred):\n",
        "#     return np.clip(np.corrcoef(y_true, y_pred)[0, 1], 0, 1)\n",
        "# def competition_score(y_true, y_pred):\n",
        "#     return 0.5 * (1 - min(normalized_rmse(y_true, y_pred), 1)) + 0.5 * pearson_correlation(y_true, y_pred)\n",
        "\n",
        "# # ‚úÖ Optuna ÌäúÎãù\n",
        "# def objective(trial):\n",
        "#     params = {\n",
        "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "#         \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 2),\n",
        "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 2),\n",
        "#         \"random_state\": 42\n",
        "#     }\n",
        "#     X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
        "#     model = xgb.XGBRegressor(**params)\n",
        "#     model.fit(X_tr, y_tr, verbose=False)\n",
        "#     y_val_pred = model.predict(X_val)\n",
        "#     return -competition_score(y_val, y_val_pred)\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=30)\n",
        "\n",
        "# # ‚úÖ ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
        "# final_model = xgb.XGBRegressor(**study.best_params)\n",
        "# final_model.fit(X_train_scaled, y_train)\n",
        "# submission[\"Inhibition\"] = final_model.predict(X_test_scaled)\n",
        "# submission.to_csv(\"submission_optuna_xgb.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "CdBw7KOpLQyt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # ‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Lipinski, MACCSkeys, AllChem\n",
        "# import xgboost as xgb\n",
        "# import optuna\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # ‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
        "# path = '/content/project_data/'\n",
        "# train = pd.read_csv(path + 'train.csv')\n",
        "# test = pd.read_csv(path + 'test.csv')\n",
        "# submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "# # ‚úÖ Î∂ÑÏûê ÌäπÏÑ± Ï∂îÏ∂ú Ìï®Ïàò\n",
        "# def get_molecule_descriptors(smiles):\n",
        "#     try:\n",
        "#         mol = Chem.MolFromSmiles(smiles)\n",
        "#         if mol is None: return [0] * 2232\n",
        "#         basic = [\n",
        "#             Descriptors.MolWt(mol), Descriptors.MolLogP(mol),\n",
        "#             Descriptors.NumHAcceptors(mol), Descriptors.NumHDonors(mol),\n",
        "#             Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol),\n",
        "#             Descriptors.NumAromaticRings(mol), Descriptors.NumHeteroatoms(mol),\n",
        "#             Descriptors.FractionCSP3(mol), Descriptors.NumAliphaticRings(mol),\n",
        "#             Lipinski.NumAromaticHeterocycles(mol), Lipinski.NumSaturatedHeterocycles(mol),\n",
        "#             Lipinski.NumAliphaticHeterocycles(mol), Descriptors.HeavyAtomCount(mol),\n",
        "#             Descriptors.RingCount(mol), Descriptors.NOCount(mol),\n",
        "#             Descriptors.NHOHCount(mol), Descriptors.NumRadicalElectrons(mol)\n",
        "#         ]\n",
        "#         morgan = [int(b) for b in AllChem.GetMorganFingerprintAsBitVect(mol, 2, 2048).ToBitString()]\n",
        "#         maccs = [int(b) for b in MACCSkeys.GenMACCSKeys(mol).ToBitString()]\n",
        "#         return basic + morgan + maccs\n",
        "#     except:\n",
        "#         return [0] * 2232\n",
        "\n",
        "# # ‚úÖ ÌîºÏ≤ò ÏÉùÏÑ±\n",
        "# train['features'] = train['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# test['features'] = test['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# X = np.array(train['features'].tolist())\n",
        "# y = train['Inhibition'].values\n",
        "# X_test = np.array(test['features'].tolist())\n",
        "\n",
        "# # ‚úÖ Ï†ïÍ∑úÌôî\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # ‚úÖ ÌèâÍ∞Ä Ìï®Ïàò\n",
        "# def normalized_rmse(y_true, y_pred):\n",
        "#     return np.sqrt(mean_squared_error(y_true, y_pred)) / (np.max(y_true) - np.min(y_true))\n",
        "# def pearson_correlation(y_true, y_pred):\n",
        "#     return np.clip(np.corrcoef(y_true, y_pred)[0, 1], 0, 1)\n",
        "# def competition_score(y_true, y_pred):\n",
        "#     return 0.5 * (1 - min(normalized_rmse(y_true, y_pred), 1)) + 0.5 * pearson_correlation(y_true, y_pred)\n",
        "\n",
        "# # ‚úÖ Optuna ÌäúÎãù (1ÌöåÎßå, Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)\n",
        "# def objective(trial):\n",
        "#     params = {\n",
        "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "#         \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 2),\n",
        "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 2),\n",
        "#         \"random_state\": 42\n",
        "#     }\n",
        "#     X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#     model = xgb.XGBRegressor(**params)\n",
        "#     model.fit(X_tr, y_tr, verbose=False)\n",
        "#     y_val_pred = model.predict(X_val)\n",
        "#     return -competition_score(y_val, y_val_pred)\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=30)\n",
        "# best_params = study.best_params\n",
        "\n",
        "# # ‚úÖ KFold ÏïôÏÉÅÎ∏î ÌïôÏäµ + ÏòàÏ∏°\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# test_preds = np.zeros(X_test.shape[0])\n",
        "# scores = []\n",
        "\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     print(f\"üìÇ Fold {fold + 1}\")\n",
        "#     X_tr, X_val = X[tr_idx], X[val_idx]\n",
        "#     y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "#     model = xgb.XGBRegressor(**best_params)\n",
        "#     model.fit(X_tr, y_tr, verbose=False)\n",
        "#     y_val_pred = model.predict(X_val)\n",
        "#     score = competition_score(y_val, y_val_pred)\n",
        "#     print(f\"Fold {fold + 1} Score: {score:.4f}\")\n",
        "#     scores.append(score)\n",
        "\n",
        "#     test_preds += model.predict(X_test) / 5  # ÏïôÏÉÅÎ∏î ÌèâÍ∑†\n",
        "\n",
        "# # ‚úÖ ÌõÑÏ≤òÎ¶¨: 0~100 ÌÅ¥Î¶¨Ìïë + float32Î°ú Ï†ÄÏû•\n",
        "# test_preds = np.clip(test_preds, 0, 100)\n",
        "# submission['Inhibition'] = test_preds.astype(np.float32)\n",
        "# submission.to_csv('submission_kfold_optuna.csv', index=False)\n",
        "\n",
        "# print(f\"\\n‚úÖ ÌèâÍ∑† Score: {np.mean(scores):.4f}\")\n",
        "# print(\"üìÅ Ï†úÏ∂úÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: submission_kfold_optuna.csv\")\n"
      ],
      "metadata": {
        "id": "Yadj52TPMuHv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Lipinski, MACCSkeys, AllChem\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# # ‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
        "# path = '/content/project_data/'\n",
        "# train = pd.read_csv(path + 'train.csv')\n",
        "# test = pd.read_csv(path + 'test.csv')\n",
        "# submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "\n",
        "# # ‚úÖ Feature Ï∂îÏ∂ú Ìï®Ïàò\n",
        "# def get_molecule_descriptors(smiles):\n",
        "#     try:\n",
        "#         mol = Chem.MolFromSmiles(smiles)\n",
        "#         if mol is None:\n",
        "#             return [0] * 2232\n",
        "\n",
        "#         basic_descriptors = [\n",
        "#             Descriptors.MolWt(mol),\n",
        "#             Descriptors.MolLogP(mol),\n",
        "#             Descriptors.NumHAcceptors(mol),\n",
        "#             Descriptors.NumHDonors(mol),\n",
        "#             Descriptors.TPSA(mol),\n",
        "#             Descriptors.NumRotatableBonds(mol),\n",
        "#             Descriptors.NumAromaticRings(mol),\n",
        "#             Descriptors.NumHeteroatoms(mol),\n",
        "#             Descriptors.FractionCSP3(mol),\n",
        "#             Descriptors.NumAliphaticRings(mol),\n",
        "#             Lipinski.NumAromaticHeterocycles(mol),\n",
        "#             Lipinski.NumSaturatedHeterocycles(mol),\n",
        "#             Lipinski.NumAliphaticHeterocycles(mol),\n",
        "#             Descriptors.HeavyAtomCount(mol),\n",
        "#             Descriptors.RingCount(mol),\n",
        "#             Descriptors.NOCount(mol),\n",
        "#             Descriptors.NHOHCount(mol),\n",
        "#             Descriptors.NumRadicalElectrons(mol),\n",
        "#         ]\n",
        "\n",
        "#         morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
        "#         morgan_features = [int(bit) for bit in morgan_fp.ToBitString()]\n",
        "#         maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "#         maccs_features = [int(bit) for bit in maccs_fp.ToBitString()]\n",
        "\n",
        "#         return basic_descriptors + morgan_features + maccs_features\n",
        "#     except:\n",
        "#         return [0] * 2232\n",
        "\n",
        "# # ‚úÖ Feature ÏÉùÏÑ±\n",
        "# train['features'] = train['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# test['features'] = test['Canonical_Smiles'].apply(get_molecule_descriptors)\n",
        "# X = np.array(train['features'].tolist())\n",
        "# y = train['Inhibition'].values\n",
        "# X_test = np.array(test['features'].tolist())\n",
        "\n",
        "# # ‚úÖ Ïä§ÏºÄÏùºÎßÅ\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ‚úÖ Metric\n",
        "# def normalized_rmse(y_true, y_pred):\n",
        "#     return np.sqrt(mean_squared_error(y_true, y_pred)) / (np.max(y_true) - np.min(y_true))\n",
        "\n",
        "# def pearson_correlation(y_true, y_pred):\n",
        "#     return np.clip(np.corrcoef(y_true, y_pred)[0,1], 0, 1)\n",
        "\n",
        "# def competition_score(y_true, y_pred):\n",
        "#     return 0.5 * (1 - normalized_rmse(y_true, y_pred)) + 0.5 * pearson_correlation(y_true, y_pred)\n",
        "\n",
        "# # ‚úÖ Î™®Îç∏ Ï†ïÏùò\n",
        "# model = LGBMRegressor(\n",
        "#     n_estimators=500, learning_rate=0.05,\n",
        "#     max_depth=6, num_leaves=31,\n",
        "#     subsample=0.8, colsample_bytree=0.8,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# # ‚úÖ 5-Fold ÍµêÏ∞®Í≤ÄÏ¶ù + ÏòàÏ∏°\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# val_scores = []\n",
        "# test_preds = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
        "#     X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
        "#     y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "#     model.fit(X_train_fold, y_train_fold)\n",
        "#     val_pred = model.predict(X_val_fold)\n",
        "#     score = competition_score(y_val_fold, val_pred)\n",
        "#     val_scores.append(score)\n",
        "#     print(f\"[Fold {fold+1}] Í≤ÄÏ¶ù Ï†êÏàò: {score:.4f}\")\n",
        "\n",
        "#     test_pred = model.predict(X_test_scaled)\n",
        "#     test_preds.append(test_pred)\n",
        "\n",
        "# # ‚úÖ ÏïôÏÉÅÎ∏î (ÌèâÍ∑†)\n",
        "# final_test_pred = np.mean(test_preds, axis=0)\n",
        "\n",
        "# # ‚úÖ ÌõÑÏ≤òÎ¶¨: 0~100 ÌÅ¥Î¶¨Ìïë\n",
        "# final_test_pred = np.clip(final_test_pred, 0, 100)\n",
        "\n",
        "# # ‚úÖ Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
        "# submission['Inhibition'] = final_test_pred\n",
        "# submission.to_csv(\"submission_kfold_lgbm.csv\", index=False)\n",
        "# print(\"‚úÖ Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: submission_kfold_lgbm.csv\")\n",
        "# print(f\"ÌèâÍ∑† Í≤ÄÏ¶ù Ï†êÏàò: {np.mean(val_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "Uw_KQRr0Pdw5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Îü∞ÌÉÄÏûÑ Ïû¨ÏãúÏûë (ÌïÑÏàò)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl-mgMeVuco_",
        "outputId": "dcb78b7e-8568-4ee3-e769-f1c55a00ea9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puyCFz0vtumx",
        "outputId": "b8cf60b7-c38e-4dc3-871c-83fe7d2385bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.11/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.2.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎî©\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys, Descriptors, rdMolDescriptors\n",
        "\n",
        "# ‚úÖ Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
        "path = '/content/project_data/'\n",
        "train = pd.read_csv(path + 'train.csv')\n",
        "test = pd.read_csv(path + 'test.csv')\n",
        "submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "# ‚úÖ 1. SMILES Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨\n",
        "def valid_smiles(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return mol is not None and mol.GetNumAtoms() > 1\n",
        "\n",
        "train = train[train['Canonical_Smiles'].apply(valid_smiles)].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ 2. Ï§ëÎ≥µ SMILES ÌèâÍ∑†Ï≤òÎ¶¨\n",
        "train = train.groupby(\"Canonical_Smiles\").agg({\n",
        "    \"Inhibition\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "# ‚úÖ 3. Canonical SMILES Ïû¨Ï†ïÏùò\n",
        "def canonicalize(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return Chem.MolToSmiles(mol, canonical=True) if mol else smiles\n",
        "\n",
        "train[\"Canonical_Smiles\"] = train[\"Canonical_Smiles\"].apply(canonicalize)\n",
        "test[\"Canonical_Smiles\"] = test[\"Canonical_Smiles\"].apply(canonicalize)\n",
        "\n",
        "# ‚úÖ 4. ÌôïÏû•Îêú Descriptors Ìï®Ïàò Ï†ïÏùò\n",
        "def get_expanded_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return [0]*5\n",
        "    return [\n",
        "        Descriptors.MolMR(mol),\n",
        "        rdMolDescriptors.CalcExactMolWt(mol),  # MolVol ‚Üí Î∂ÑÏûêÎüâÏúºÎ°ú ÎåÄÏ≤¥\n",
        "        Descriptors.BalabanJ(mol),\n",
        "        Descriptors.Chi0(mol),\n",
        "        Descriptors.Kappa1(mol),\n",
        "    ]\n",
        "\n",
        "# ‚úÖ 5. Feature ÏÉùÏÑ± Ìï®Ïàò Ï†ïÏùò\n",
        "def get_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(512)\n",
        "\n",
        "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=256)\n",
        "    maccs = MACCSkeys.GenMACCSKeys(mol)\n",
        "    extra = get_expanded_descriptors(smiles)\n",
        "\n",
        "    return np.concatenate([\n",
        "        np.array(morgan),\n",
        "        np.array(maccs),\n",
        "        np.array(extra)\n",
        "    ])\n",
        "\n",
        "# ‚úÖ Ï†ÑÏ≤¥ feature Î≥ÄÌôò\n",
        "X = np.array([get_features(s) for s in train['Canonical_Smiles']])\n",
        "X_test = np.array([get_features(s) for s in test['Canonical_Smiles']])\n",
        "y = train['Inhibition'].values\n",
        "\n",
        "# ‚úÖ ÌëúÏ§ÄÌôî\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ‚úÖ Î™®Îç∏ Ï†ïÏùò\n",
        "base_models = [\n",
        "    ('xgb', XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42)),\n",
        "    ('lgb', LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42)),\n",
        "    ('ridge', Ridge(alpha=1.0))\n",
        "]\n",
        "meta_model = Ridge()\n",
        "\n",
        "stack_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ‚úÖ KFold ÌïôÏäµ + Ï∂îÎ°†\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "preds = np.zeros(len(X_test_scaled))\n",
        "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_scaled, y)):\n",
        "    X_train, X_val = X_scaled[tr_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "    stack_model.fit(X_train, y_train)\n",
        "    val_pred = stack_model.predict(X_val)\n",
        "    print(f'Fold {fold+1} MAE:', mean_absolute_error(y_val, val_pred))\n",
        "\n",
        "    preds += stack_model.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "# ‚úÖ Í≤∞Í≥º Ï†ÄÏû•\n",
        "submission['Inhibition'] = preds\n",
        "submission.to_csv('/content/submission_stack.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVepFr7osHeT",
        "outputId": "d3baf7aa-577b-4517-bfa7-7140e472eb62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 MAE: 20.50232411022973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 MAE: 20.056287300003262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 MAE: 19.282389806215498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 MAE: 21.050464016475864\n",
            "Fold 5 MAE: 21.122982024070637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, MACCSkeys, AllChem\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú\n",
        "path = '/content/project_data/'\n",
        "train = pd.read_csv(path + 'train.csv')\n",
        "test = pd.read_csv(path + 'test.csv')\n",
        "submission = pd.read_csv(path + 'sample_submission.csv')\n",
        "\n",
        "# ‚úÖ Ïú†Ìö®Ìïú SMILES ÌïÑÌÑ∞ÎßÅ\n",
        "def valid_smiles(smi):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    return mol is not None and mol.GetNumAtoms() > 1\n",
        "\n",
        "train = train[train['Canonical_Smiles'].apply(valid_smiles)].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Canonicalization\n",
        "def canonicalize(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return Chem.MolToSmiles(mol, canonical=True) if mol else smiles\n",
        "\n",
        "train['Canonical_Smiles'] = train['Canonical_Smiles'].apply(canonicalize)\n",
        "test['Canonical_Smiles'] = test['Canonical_Smiles'].apply(canonicalize)\n",
        "\n",
        "# ‚úÖ Feature ÏÉùÏÑ±\n",
        "def get_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(512 + 167 + 5)\n",
        "\n",
        "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=512)\n",
        "    morgan_np = np.array(morgan)\n",
        "\n",
        "    maccs = MACCSkeys.GenMACCSKeys(mol)\n",
        "    maccs_np = np.array(maccs)[1:]\n",
        "\n",
        "    others = np.array([\n",
        "        Descriptors.MolMR(mol),\n",
        "        Descriptors.BalabanJ(mol),\n",
        "        Descriptors.Chi0(mol),\n",
        "        Descriptors.Chi1(mol),\n",
        "        Descriptors.Kappa1(mol)\n",
        "    ])\n",
        "\n",
        "    return np.concatenate([morgan_np, maccs_np, others])\n",
        "\n",
        "X = np.array([get_features(smi) for smi in train['Canonical_Smiles']])\n",
        "X_test = np.array([get_features(smi) for smi in test['Canonical_Smiles']])\n",
        "y = train['Inhibition'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ‚úÖ KFold ÏÑ§Ï†ï\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_stacking(meta_model, file_name):\n",
        "    base_models = [\n",
        "        ('xgb', XGBRegressor(n_estimators=300, random_state=42)),\n",
        "        ('lgbm', LGBMRegressor(n_estimators=300, random_state=42)),\n",
        "        ('cat', CatBoostRegressor(n_estimators=300, verbose=0, random_state=42))\n",
        "    ]\n",
        "\n",
        "    stack = StackingRegressor(\n",
        "        estimators=base_models,\n",
        "        final_estimator=meta_model,\n",
        "        passthrough=True,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    preds = np.zeros(len(X_test))\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_tr, X_val = X[train_idx], X[val_idx]\n",
        "        y_tr, y_val = y[train_idx], y[val_idx]\n",
        "        stack.fit(X_tr, y_tr)\n",
        "        preds += stack.predict(X_test) / kf.n_splits\n",
        "\n",
        "    submission['Inhibition'] = preds\n",
        "    submission.to_csv(file_name, index=False)\n",
        "\n",
        "# ‚úÖ Ï†ÑÎûµ 2: Meta ‚Üí LinearRegression\n",
        "run_stacking(LinearRegression(), 'submission_stack_linear.csv')\n",
        "\n",
        "# ‚úÖ Ï†ÑÎûµ 3: Meta ‚Üí LGBM (CatBoost Ìè¨Ìï®)\n",
        "run_stacking(LGBMRegressor(n_estimators=100, random_state=42), 'submission_stack_lgbm.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dUA8GTOwuQm",
        "outputId": "48b2132e-2962-4d10-d626-99282ae68983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016211 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3975\n",
            "[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 653\n",
            "[LightGBM] [Info] Start training from score 33.391242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015918 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3969\n",
            "[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 651\n",
            "[LightGBM] [Info] Start training from score 33.637152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3972\n",
            "[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 652\n",
            "[LightGBM] [Info] Start training from score 33.054065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}