{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12337654,
          "sourceType": "datasetVersion",
          "datasetId": 7777606
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook90dc24bd09",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/dacon_new_drug/blob/main/20250708.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "75yI2AXdBPkO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "biniroun_drug_data_path = kagglehub.dataset_download('biniroun/drug-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "QLl1HZ0DBPkT"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Kaggle 노트북 상단에 RDKit 설치 명령어 추가\n",
        "# !pip install -q rdkit-pypi\n",
        "\n",
        "# # ✅ numpy와 scipy의 호환 가능한 버전으로 설치\n",
        "# !pip install numpy==1.23.5 scipy==1.10.1 --force-reinstall --no-cache-dir\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T01:13:47.443144Z",
          "iopub.execute_input": "2025-07-04T01:13:47.443361Z",
          "iopub.status.idle": "2025-07-04T01:14:08.787465Z",
          "shell.execute_reply.started": "2025-07-04T01:13:47.443342Z",
          "shell.execute_reply": "2025-07-04T01:14:08.786274Z"
        },
        "id": "S6xujMERBPkU",
        "outputId": "9205a8d1-0e9d-47e8-a4dd-e7692d22710e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nCollecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m261.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scipy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\ncvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nchex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nxarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5 scipy-1.10.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 필요한 경우 다른 라이브러리도 함께 업그레이드/설치\n",
        "# !pip install --upgrade scikit-learn pandas numpy xgboost lightgbm catboost\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T01:14:08.788728Z",
          "iopub.execute_input": "2025-07-04T01:14:08.789656Z",
          "iopub.status.idle": "2025-07-04T01:14:39.141184Z",
          "shell.execute_reply.started": "2025-07-04T01:14:08.789624Z",
          "shell.execute_reply": "2025-07-04T01:14:39.140137Z"
        },
        "id": "4VH1ldxxBPkV",
        "outputId": "cb89ea54-4cca-4a7d-95d4-2e7cb50989aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting pandas\n  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.23.5)\nCollecting numpy\n  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nCollecting xgboost\n  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.10.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nCollecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\nDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, pandas, xgboost, scikit-learn, lightgbm\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 4.5.0\n    Uninstalling lightgbm-4.5.0:\n      Successfully uninstalled lightgbm-4.5.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ncvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lightgbm-4.6.0 numpy-1.26.4 pandas-2.3.0 scikit-learn-1.7.0 xgboost-3.0.2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.17Z"
        },
        "id": "nnlvgBOyBPkV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Draw\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from collections import Counter\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from scipy.stats import pearsonr"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.171Z"
        },
        "id": "DgooVKOvBPkW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 학습 데이터 불러오기\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# df = pd.read_csv(os.path.join(path, 'train.csv'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.171Z"
        },
        "id": "aprrKbjLBPkW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# smiles = df['Canonical_Smiles'][1] # idx\n",
        "\n",
        "# # SMILES string을 RDKit molecule object 변환\n",
        "# mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "# # 변환이 잘 되었는지 확인\n",
        "# if mol is not None:\n",
        "#     # 분자 구조 이미지 파일로 그리기\n",
        "#     img = Draw.MolToImage(mol)\n",
        "#     # 2D 분자 구조 이미지 저장\n",
        "#     img.save(\"molecule.png\")\n",
        "# else:\n",
        "#     print(\"Invalid SMILES string\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.171Z"
        },
        "id": "C0f47U31BPkX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 각 SMILES에서 원자 정보 추출\n",
        "# element_counter = Counter()\n",
        "# atom_counts = []\n",
        "# invalid_smiles = []\n",
        "# mol_weights = []\n",
        "\n",
        "# for idx, smi in enumerate(df['Canonical_Smiles']):\n",
        "#     mol = Chem.MolFromSmiles(smi)\n",
        "#     if mol is None:\n",
        "#         invalid_smiles.append((idx, smi))\n",
        "#         continue\n",
        "#     atoms = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
        "#     mol_weight = Descriptors.MolWt(mol)\n",
        "#     element_counter.update(atoms)\n",
        "#     atom_counts.append(len(atoms))\n",
        "#     mol_weights.append(mol_weight)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.171Z"
        },
        "id": "IoqzmpKEBPkX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 평균 원자 수 (H 제외(heavy atoms), SMILES에만 포함된 원자)\n",
        "# df['atom_count'] = atom_counts\n",
        "# df['mol_weight'] = mol_weights\n",
        "\n",
        "# mean_atoms = np.mean([c for c in atom_counts if c is not None])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.171Z"
        },
        "id": "VI7H7xdLBPkX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 원소 등장 빈도 (H 제외)\n",
        "# # plt.figure(figsize=(12, 6))\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "# elements, counts = zip(*element_counter.most_common())\n",
        "# bars = ax.bar(elements, counts, color='skyblue')\n",
        "# plt.title('Atom Frequency Distribution')\n",
        "# plt.xlabel('Atom')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     plt.text(bar.get_x() + bar.get_width()/2, height,\n",
        "#              f'{height}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "paZpkNo_BPkY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 화합물당 원자 수 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.hist(atom_counts, bins=20, color='orange', edgecolor='black')\n",
        "# plt.title('Distribution of heavy atoms per compound')\n",
        "# plt.xlabel('Number of heavy atoms')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "mlsQM_F7BPkY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inhibition 값 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# # 5% 단위로 분리\n",
        "# plt.hist(df['Inhibition'], bins=20, color='green', edgecolor='black')\n",
        "# plt.title('Target Distribution')\n",
        "# plt.xlabel('Inhibition ')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "3QXTP7aOBPkY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 분자량 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.hist(df['mol_weight'], bins=20, color='red', edgecolor='black')\n",
        "# plt.title('Mol Weight Distribution')\n",
        "# plt.xlabel('mol_weight')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "xTEXJLI_BPkY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 통계 정보\n",
        "# print(f\"Number of valid SMILES: {len(atom_counts)} / {len(df)}\")\n",
        "# print(f\"Number of Invalid SMILES: {len(invalid_smiles)}\")\n",
        "# print(f\"Average number of heavy atoms: {mean_atoms:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "5sebCzx8BPkZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터프레임: df['smiles'], df['target'] 가정\n",
        "# df['smiles_length'] = df['Canonical_Smiles'].apply(len)\n",
        "\n",
        "# # 상관계수 계산\n",
        "# corr, p_value = pearsonr(df['smiles_length'], df['Inhibition'])\n",
        "\n",
        "# print(f\" Pearson 상관계수: {corr:.3f}\")\n",
        "# print(f\" p-value: {p_value:.3e}\")\n",
        "# print(\"=\"*120)\n",
        "\n",
        "# # 산점도 시각화\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(df['smiles_length'], df['Inhibition'], alpha=0.6, color='teal', edgecolors='k')\n",
        "# plt.title('SMILES Length vs Inhibition')\n",
        "# plt.xlabel('SMILES Length')\n",
        "# plt.ylabel('Inhibition')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "J2JP_xVNBPkZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 상관계수 계산\n",
        "# corr, p_value = pearsonr(df['mol_weight'], df['Inhibition'])\n",
        "\n",
        "# print(f\" Pearson 상관계수: {corr:.3f}\")\n",
        "# print(f\" p-value: {p_value:.3e}\")\n",
        "# print(\"=\"*120)\n",
        "\n",
        "# # 산점도 시각화\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(df['mol_weight'], df['Inhibition'], alpha=0.6, color='steelblue', edgecolors='k')\n",
        "# plt.title('Mol Weight vs Inhibition')\n",
        "# plt.xlabel('Mol Weight')\n",
        "# plt.ylabel('Inhibition')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.172Z"
        },
        "id": "BRo6g2zaBPkZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.columns.tolist())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "cJ0fMVtoBPkZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "\n",
        "# # ✅ RDKit 파생변수 생성 함수\n",
        "# def featurize(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol is None:\n",
        "#         return np.nan, np.nan\n",
        "#     mol_wt = Descriptors.MolWt(mol)\n",
        "#     tpsa = Descriptors.TPSA(mol)\n",
        "#     return mol_wt, tpsa\n",
        "\n",
        "# train[['MolWt', 'TPSA']] = train['Canonical_Smiles'].apply(\n",
        "#     lambda x: pd.Series(featurize(x))\n",
        "# )\n",
        "\n",
        "# # ✅ 파생 변수 생성\n",
        "# train['smiles_length'] = train['Canonical_Smiles'].apply(len)\n",
        "# train['is_heavy_mol'] = (train['MolWt'] > 500).astype(int)\n",
        "# train['is_long_smiles'] = (train['smiles_length'] > 60).astype(int)\n",
        "# train['is_low_inhibition'] = (train['Inhibition'] < 10).astype(int)\n",
        "# train['log_inhibition'] = np.log1p(train['Inhibition'])\n",
        "\n",
        "# # ✅ Feature / Target 설정\n",
        "# features = ['MolWt', 'TPSA', 'smiles_length', 'is_heavy_mol', 'is_long_smiles', 'is_low_inhibition']\n",
        "# X = train[features]\n",
        "# y = train['log_inhibition']\n",
        "\n",
        "# # ✅ KFold + Quantile Loss\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_preds = np.zeros(len(train))\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "#     X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "#     model = LGBMRegressor(\n",
        "#         objective='quantile',\n",
        "#         alpha=0.5,\n",
        "#         n_estimators=200,\n",
        "#         learning_rate=0.05,\n",
        "#         random_state=42\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     oof_preds[val_idx] = model.predict(X_val)\n",
        "\n",
        "# # ✅ 평가 (역변환)\n",
        "# rmse = np.sqrt(mean_squared_error(np.expm1(y), np.expm1(oof_preds)))\n",
        "# print(f\"✅ RMSE (Quantile Loss + log1p 역변환): {rmse:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "SHmZx4UqBPkZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# # ✅ 데이터 로드\n",
        "# path = '/kaggle/input/drug-data'  # Colab이면 로컬 경로로 수정\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "\n",
        "# # ✅ 파생변수 생성\n",
        "# train['smiles_length'] = train['Canonical_Smiles'].apply(len)\n",
        "# train['is_long_smiles'] = (train['smiles_length'] > 60).astype(int)\n",
        "# train['is_low_inhibition'] = (train['Inhibition'] < 10).astype(int)\n",
        "# train['is_high_inhibition'] = (train['Inhibition'] > 90).astype(int)\n",
        "\n",
        "# # ✅ log1p 타깃 변환\n",
        "# train['log_inhibition'] = np.log1p(train['Inhibition'])\n",
        "\n",
        "# # ✅ 입력 특성 선택\n",
        "# features = ['smiles_length', 'is_long_smiles', 'is_low_inhibition', 'is_high_inhibition']\n",
        "# X = train[features]\n",
        "# y = train['log_inhibition']\n",
        "\n",
        "# # ✅ KFold 교차검증\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_preds = np.zeros(len(train))\n",
        "\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "#     y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
        "\n",
        "#     train_pool = Pool(X_tr, y_tr)\n",
        "#     val_pool = Pool(X_val, y_val)\n",
        "\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=1000,\n",
        "#         learning_rate=0.05,\n",
        "#         depth=6,\n",
        "#         loss_function='Quantile:alpha=0.5',  # Median\n",
        "#         random_seed=42,\n",
        "#         verbose=0\n",
        "#     )\n",
        "\n",
        "#     model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50)\n",
        "#     oof_preds[val_idx] = model.predict(X_val)\n",
        "\n",
        "# # ✅ 평가 (log1p 역변환 후 RMSE)\n",
        "# rmse = np.sqrt(mean_squared_error(np.expm1(y), np.expm1(oof_preds)))\n",
        "# print(f\"✅ CatBoost RMSE (exp scale): {rmse:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "9ksyuv-aBPka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # test.csv 로드 및 동일한 파생변수 생성\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# test['smiles_length'] = test['Canonical_Smiles'].apply(len)\n",
        "# test['is_long_smiles'] = (test['smiles_length'] > 60).astype(int)\n",
        "# test['is_low_inhibition'] = 0  # 예측 대상이므로 실제 값 없음\n",
        "# test['is_high_inhibition'] = 0\n",
        "\n",
        "# test_X = test[features]\n",
        "\n",
        "# # 5개 모델 평균 앙상블\n",
        "# preds = np.zeros(len(test_X))\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=1000,\n",
        "#         learning_rate=0.05,\n",
        "#         depth=6,\n",
        "#         loss_function='Quantile:alpha=0.5',\n",
        "#         random_seed=42,\n",
        "#         verbose=0\n",
        "#     )\n",
        "#     model.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
        "#     preds += model.predict(test_X) / kf.get_n_splits()\n",
        "\n",
        "# # log1p 역변환 후 제출\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "# submission['Inhibition'] = np.expm1(preds)\n",
        "# submission.to_csv(\"submission_catboost.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "xWAoPn-5BPka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 확실한 예측 및 저장 코드\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "# submission['Inhibition'] = np.expm1(preds)  # 로그 역변환 필수\n",
        "# print(submission['Inhibition'].describe())  # 분포 확인\n",
        "\n",
        "# submission.to_csv(\"submission_catboost_fixed.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "MSG3Gjq5BPka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# plt.hist(submission['Inhibition'], bins=30, color='skyblue', edgecolor='black')\n",
        "# plt.title(\"Distribution of Predicted Inhibition Values\")\n",
        "# plt.xlabel(\"Inhibition\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.173Z"
        },
        "id": "drRWFRtUBPka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "\n",
        "# # ✅ SMILES → RDKit mol 객체로 변환\n",
        "# def smiles_to_mol(smiles):\n",
        "#     try:\n",
        "#         return Chem.MolFromSmiles(smiles)\n",
        "#     except:\n",
        "#         return None\n",
        "\n",
        "# train[\"mol\"] = train[\"Canonical_Smiles\"].apply(smiles_to_mol)\n",
        "# test[\"mol\"] = test[\"Canonical_Smiles\"].apply(smiles_to_mol)\n",
        "\n",
        "# # ✅ 파생변수 생성\n",
        "# def make_features(df, is_train=True):\n",
        "#     df[\"mol_wt\"] = df[\"mol\"].apply(lambda m: Descriptors.MolWt(m) if m else 0)\n",
        "#     df[\"smiles_len\"] = df[\"Canonical_Smiles\"].apply(len)\n",
        "#     df[\"is_heavy_mol\"] = (df[\"mol_wt\"] > 500).astype(int)\n",
        "#     df[\"long_smiles\"] = (df[\"smiles_len\"] > 70).astype(int)\n",
        "#     if is_train:\n",
        "#         df[\"low_inhibition\"] = (df[\"Inhibition\"] < 30).astype(int)\n",
        "#     return df\n",
        "\n",
        "# train = make_features(train, is_train=True)\n",
        "# test = make_features(test, is_train=False)\n",
        "\n",
        "# # ✅ Target log1p 변환\n",
        "# train[\"target\"] = np.log1p(train[\"Inhibition\"])\n",
        "\n",
        "# # ✅ 모델 학습\n",
        "# features = [\"mol_wt\", \"smiles_len\", \"is_heavy_mol\", \"long_smiles\"]\n",
        "# X = train[features]\n",
        "# y = train[\"target\"]\n",
        "# X_test = test[features]\n",
        "\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model = CatBoostRegressor(\n",
        "#     iterations=1000,\n",
        "#     learning_rate=0.03,\n",
        "#     depth=6,\n",
        "#     loss_function=\"Quantile:alpha=0.5\",  # Quantile Regression\n",
        "#     early_stopping_rounds=30,\n",
        "#     random_seed=42,\n",
        "#     verbose=100\n",
        "# )\n",
        "\n",
        "# model.fit(Pool(X_train, y_train), eval_set=Pool(X_valid, y_valid))\n",
        "\n",
        "# # ✅ 예측 및 역변환\n",
        "# preds = model.predict(X_test)\n",
        "# submission[\"Inhibition\"] = np.expm1(preds)  # log1p 역변환\n",
        "\n",
        "# # ✅ 저장\n",
        "# submission.to_csv(\"submission_catboost_v2.csv\", index=False)\n",
        "\n",
        "# # ✅ 예측 분포 시각화\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# plt.hist(submission[\"Inhibition\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
        "# plt.title(\"Distribution of Predicted Inhibition Values\")\n",
        "# plt.xlabel(\"Inhibition\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "V3PH30H1BPka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "\n",
        "\n",
        "# # 로그 변환\n",
        "# train[\"Inhibition_log\"] = np.log1p(train[\"Inhibition\"])\n",
        "\n",
        "# # 서브구조 파생변수 추가 함수\n",
        "# def add_smiles_features(df):\n",
        "#     features = ['Cl', 'Br', 'F', 'N=', 'C#C', 'c1ccccc1']  # 예시\n",
        "#     for f in features:\n",
        "#         df[f\"has_{f}\"] = df[\"Canonical_Smiles\"].str.contains(f, regex=False).astype(int)\n",
        "#     return df\n",
        "\n",
        "# train = add_smiles_features(train)\n",
        "# test = add_smiles_features(test)\n",
        "\n",
        "# # 피처 및 타겟 분리\n",
        "# X = train.drop(columns=[\"ID\", \"Inhibition\", \"Inhibition_log\", \"Canonical_Smiles\"])\n",
        "# y = train[\"Inhibition_log\"]\n",
        "# X_test = test.drop(columns=[\"ID\", \"Canonical_Smiles\"])\n",
        "\n",
        "# # 이상치 가중치\n",
        "# sample_weight = np.where(train[\"Inhibition\"] > 45, 2.0, 1.0)\n",
        "\n",
        "# # train/valid split\n",
        "# X_train, X_valid, y_train, y_valid, sw_train, sw_valid = train_test_split(\n",
        "#     X, y, sample_weight, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # 모델 학습 (alpha=0.9)\n",
        "# model_q90 = CatBoostRegressor(\n",
        "#     loss_function='Quantile:alpha=0.9',\n",
        "#     iterations=500,\n",
        "#     learning_rate=0.05,\n",
        "#     depth=6,\n",
        "#     random_state=42,\n",
        "#     verbose=100\n",
        "# )\n",
        "\n",
        "# model_q90.fit(Pool(X_train, y_train, weight=sw_train),\n",
        "#               eval_set=Pool(X_valid, y_valid, weight=sw_valid))\n",
        "\n",
        "# # 예측 및 복원\n",
        "# preds = model_q90.predict(X_test)\n",
        "# submission = pd.DataFrame({\n",
        "#     \"ID\": test[\"ID\"],\n",
        "#     \"Inhibition\": np.expm1(preds)  # 로그 복원\n",
        "# })\n",
        "\n",
        "# # 저장\n",
        "# submission.to_csv(\"submission_catboost_quantile090.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "W_xIMDVGBPkb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 라이브러리\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from catboost import CatBoostRegressor\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ 파생변수 생성 함수\n",
        "# def smiles_to_descriptors(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol:\n",
        "#         return {\n",
        "#             'MolWt': Descriptors.MolWt(mol),\n",
        "#             'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "#             'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "#             'TPSA': Descriptors.TPSA(mol),\n",
        "#             'LogP': Descriptors.MolLogP(mol)\n",
        "#         }\n",
        "#     else:\n",
        "#         return {'MolWt': 0, 'NumHDonors': 0, 'NumHAcceptors': 0, 'TPSA': 0, 'LogP': 0}\n",
        "\n",
        "# # ✅ 파생변수 적용\n",
        "# train_desc = train['Canonical_Smiles'].apply(smiles_to_descriptors).apply(pd.Series)\n",
        "# test_desc = test['Canonical_Smiles'].apply(smiles_to_descriptors).apply(pd.Series)\n",
        "\n",
        "# train = pd.concat([train, train_desc], axis=1)\n",
        "# test = pd.concat([test, test_desc], axis=1)\n",
        "\n",
        "# # ✅ 모델 입력 설정\n",
        "# features = ['MolWt', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'LogP']\n",
        "# X = train[features].values\n",
        "# X_test = test[features].values\n",
        "# y = train['Inhibition'].values\n",
        "\n",
        "# # ✅ 극단값 가중치 부여 (예: 40 이상인 경우 가중치 ↑)\n",
        "# sample_weight = np.where(y >= 40, 2.0, 1.0)\n",
        "\n",
        "# # ✅ OOF 및 테스트 예측 저장용\n",
        "# oof_cat = np.zeros(len(train))\n",
        "# oof_lgb = np.zeros(len(train))\n",
        "# preds_cat = np.zeros(len(test))\n",
        "# preds_lgb = np.zeros(len(test))\n",
        "\n",
        "# # ✅ KFold 설정\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# # ✅ 개별 모델 학습 및 예측\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "#     X_tr, X_val = X[train_idx], X[val_idx]\n",
        "#     y_tr, y_val = y[train_idx], y[val_idx]\n",
        "#     w_tr = sample_weight[train_idx]\n",
        "\n",
        "#     # ✅ CatBoost\n",
        "#     cat = CatBoostRegressor(verbose=0, iterations=500, learning_rate=0.05)\n",
        "#     cat.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     preds_cat += cat.predict(X_test) / kf.n_splits\n",
        "\n",
        "#     # ✅ LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=500, learning_rate=0.05)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     preds_lgb += lgb.predict(X_test) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타모델 입력 구성\n",
        "# X_meta = np.vstack([oof_cat, oof_lgb]).T\n",
        "# X_test_meta = np.vstack([preds_cat, preds_lgb]).T\n",
        "\n",
        "# # ✅ 메타모델 (Ridge)\n",
        "# meta_model = Ridge()\n",
        "# meta_model.fit(X_meta, y, sample_weight=sample_weight)\n",
        "# final_preds = meta_model.predict(X_test_meta)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking.csv', index=False)\n",
        "# print(\"✅ 최종 제출 파일 저장 완료.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "jiKm9QluBPkb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.linear_model import QuantileRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import KFold\n",
        "# import optuna\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ 전처리 완료된 특징 불러오기 (RDKit 기반)\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Optuna Objective 정의\n",
        "# def objective(trial):\n",
        "#     alpha = trial.suggest_float(\"alpha\", 0.7, 0.95)\n",
        "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "#     maes = []\n",
        "\n",
        "#     for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#         X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#         y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "#         model = QuantileRegressor(quantile=alpha, alpha=0, solver='highs')\n",
        "#         model.fit(X_tr, y_tr)\n",
        "#         y_pred = model.predict(X_val)\n",
        "#         maes.append(mean_absolute_error(y_val, y_pred))\n",
        "\n",
        "#     return np.mean(maes)\n",
        "\n",
        "# # ✅ Optuna 실행\n",
        "# study = optuna.create_study(direction='minimize')\n",
        "# study.optimize(objective, n_trials=30)\n",
        "# best_alpha = study.best_params['alpha']\n",
        "\n",
        "# # ✅ 최종 모델 학습\n",
        "# final_model = QuantileRegressor(quantile=best_alpha, alpha=0, solver='highs')\n",
        "# final_model.fit(X_train_scaled, y_train)\n",
        "# preds = final_model.predict(X_test_scaled)\n",
        "\n",
        "# # ✅ 제출 파일 저장\n",
        "# submission['Inhibition'] = preds\n",
        "# submission.to_csv('submission_quantile_optuna.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "hjKJKBFkBPkb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출 (이전 단계 코드 활용)\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 스케일링\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Sample Weight 정의 (높은 억제율에 더 높은 가중치 부여 예시)\n",
        "# sample_weight = np.log1p(y_train)  # or y_train**1.5 등\n",
        "\n",
        "# # ✅ 모델 학습 (5-Fold CV)\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# preds = np.zeros(len(X_test))\n",
        "# val_mae_list = []\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#     X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=2000,\n",
        "#         learning_rate=0.03,\n",
        "#         depth=6,\n",
        "#         eval_metric='MAE',\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=0,\n",
        "#         random_state=42\n",
        "#     )\n",
        "\n",
        "#     train_pool = Pool(X_tr, y_tr, weight=w_tr)\n",
        "#     val_pool = Pool(X_val, y_val)\n",
        "#     model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "#     val_pred = model.predict(X_val)\n",
        "#     val_mae = mean_absolute_error(y_val, val_pred)\n",
        "#     val_mae_list.append(val_mae)\n",
        "\n",
        "#     preds += model.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "# # ✅ 성능 출력 및 제출 파일 저장\n",
        "# print(f\"평균 MAE: {np.mean(val_mae_list):.4f}\")\n",
        "\n",
        "# submission['Inhibition'] = preds\n",
        "# submission.to_csv(\"submission_catboost_weighted.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "7RZCaMhhBPkb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from xgboost import XGBRegressor\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Sample Weight 정의\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking: Base 모델 예측값을 모아 최종 모델 학습\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#     X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, random_state=42, verbosity=0)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델 학습 (Linear Regression)\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = LinearRegression()\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_cat_lgb_xgb.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "AwiEEj4XBPkc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import Ridge\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_train.columns)\n",
        "\n",
        "# # ✅ Sample Weight 정의\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델 학습\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = Ridge(alpha=1.0)\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_final.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.174Z"
        },
        "id": "8RK84jjoBPkc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit Feature 확장\n",
        "# def extract_rdkit_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [Descriptors.MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [Crippen.MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [Lipinski.NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [Lipinski.NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [rdMolDescriptors.CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [Descriptors.NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols],\n",
        "#         'HeavyAtomCount': [mol.GetNumHeavyAtoms() if mol else np.nan for mol in mols],\n",
        "#         'FractionCSP3': [rdMolDescriptors.CalcFractionCSP3(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAliphaticRings': [rdMolDescriptors.CalcNumAliphaticRings(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAromaticRings': [rdMolDescriptors.CalcNumAromaticRings(mol) if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_rdkit_features(train)\n",
        "# X_test = extract_rdkit_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# # ✅ Sample Weight\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = Ridge(alpha=1.0)\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_rdkit_expanded.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.175Z"
        },
        "id": "QonU03zyBPkc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 기본 라이브러리 및 데이터 불러오기\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import AllChem\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # ✅ 경로 설정\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ Morgan Fingerprint 추출 함수\n",
        "# def get_morgan_fingerprint(smiles_string, n_bits=2048, radius=2):\n",
        "#     mol = Chem.MolFromSmiles(smiles_string)\n",
        "#     if mol is not None:\n",
        "#         fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "#         return np.array(fp)\n",
        "#     else:\n",
        "#         return np.zeros(n_bits, dtype=int)\n",
        "\n",
        "# # ✅ Fingerprint 생성\n",
        "# train_fps = train['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "# test_fps = test['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "\n",
        "# fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "# X_train = pd.DataFrame(train_fps.tolist(), columns=fp_columns)\n",
        "# X_test = pd.DataFrame(test_fps.tolist(), columns=fp_columns)\n",
        "# X_train['Inhibition'] = train['Inhibition']\n",
        "\n",
        "# # ✅ PyCaret 설치 (Kaggle 환경용)\n",
        "# !pip install -q pycaret\n",
        "\n",
        "# # ✅ PyCaret 설정\n",
        "# from pycaret.regression import *\n",
        "# s = setup(data=X_train, target='Inhibition', session_id=42, normalize=True, normalize_method='zscore', silent=True, verbose=False)\n",
        "\n",
        "# # ✅ 트리 기반 모델만 비교\n",
        "# best = compare_models(include=['lightgbm', 'xgboost', 'catboost', 'rf'], sort='RMSE', n_select=1)\n",
        "\n",
        "# # ✅ 모델 튜닝\n",
        "# tuned = tune_model(best, optimize='RMSE')\n",
        "\n",
        "# # ✅ 모델 앙상블 (스태킹)\n",
        "# top3 = compare_models(include=['lightgbm', 'xgboost', 'catboost'], n_select=3)\n",
        "# blended = blend_models(estimator_list=top3)\n",
        "# final_model = finalize_model(blended)\n",
        "\n",
        "# # ✅ 예측\n",
        "# preds = predict_model(final_model, data=X_test)\n",
        "\n",
        "# # ✅ 제출파일 저장\n",
        "# submission['Inhibition'] = np.clip(preds['prediction_label'], 0, 100)\n",
        "# submission.to_csv('submission_pycaret_stack.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.175Z"
        },
        "id": "Eq5-CNsVBPkd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors, AllChem\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ 1. RDKit 화학 특성 추출\n",
        "# def extract_rdkit_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [Descriptors.MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [Crippen.MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [Lipinski.NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [Lipinski.NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [rdMolDescriptors.CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [Descriptors.NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols],\n",
        "#         'HeavyAtomCount': [mol.GetNumHeavyAtoms() if mol else np.nan for mol in mols],\n",
        "#         'FractionCSP3': [rdMolDescriptors.CalcFractionCSP3(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAliphaticRings': [rdMolDescriptors.CalcNumAliphaticRings(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAromaticRings': [rdMolDescriptors.CalcNumAromaticRings(mol) if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# # ✅ 2. Morgan Fingerprint (2048-bit)\n",
        "# def get_morgan_fingerprint(smiles, radius=2, nBits=2048):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol:\n",
        "#         fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
        "#         return np.array(fp)\n",
        "#     else:\n",
        "#         return np.zeros(nBits)\n",
        "\n",
        "# def extract_morgan_df(df, nBits=2048):\n",
        "#     fps = df['Canonical_Smiles'].apply(lambda x: get_morgan_fingerprint(x, nBits=nBits))\n",
        "#     return pd.DataFrame(fps.tolist(), columns=[f'MFP_{i}' for i in range(nBits)])\n",
        "\n",
        "# # ✅ 3. Feature 결합\n",
        "# X_train_rdkit = extract_rdkit_features(train)\n",
        "# X_test_rdkit = extract_rdkit_features(test)\n",
        "# X_train_morgan = extract_morgan_df(train)\n",
        "# X_test_morgan = extract_morgan_df(test)\n",
        "\n",
        "# # 결합 (RDKit + Morgan)\n",
        "# X_train = pd.concat([X_train_rdkit, X_train_morgan], axis=1)\n",
        "# X_test = pd.concat([X_test_rdkit, X_test_morgan], axis=1)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 4. 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# # ✅ 5. Sample Weighting\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 6. Stacking 앙상블\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 7. 메타 모델\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = Ridge(alpha=1.0)\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 8. 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_rdkit_morgan.csv', index=False)\n",
        "# print(\"✅ 'submission_stacking_rdkit_morgan.csv' 생성 완료!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.175Z"
        },
        "id": "d1vdn5eeBPkd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 설치 (케글 환경은 sudo 사용 불가하므로 --user 필수)\n",
        "# !pip install --user torch torchvision torchaudio\n",
        "# !pip install --user torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T01:14:54.439735Z",
          "iopub.execute_input": "2025-07-04T01:14:54.440498Z",
          "iopub.status.idle": "2025-07-04T01:16:05.586037Z",
          "shell.execute_reply.started": "2025-07-04T01:14:54.440472Z",
          "shell.execute_reply": "2025-07-04T01:16:05.585146Z"
        },
        "id": "Qp55I44ZBPkd",
        "outputId": "799576a8-6b8f-4052-a5f0-0980a2f855bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nLooking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m219.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter, torch-sparse, torch-geometric\nSuccessfully installed torch-geometric-2.6.1 torch-scatter-2.1.2+pt20cpu torch-sparse-0.6.18+pt20cpu\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# print(torch.__version__)  # 예: 2.0.0\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.175Z"
        },
        "id": "Akrph-EWBPkd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --user torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n",
        "#     -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T01:16:30.02512Z",
          "iopub.execute_input": "2025-07-04T01:16:30.025435Z"
        },
        "id": "RkIrK3jYBPke",
        "outputId": "b499836c-4cc5-454f-ddb8-7459f23a5202"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nRequirement already satisfied: torch-scatter in /root/.local/lib/python3.11/site-packages (2.1.2+pt20cpu)\nRequirement already satisfied: torch-sparse in /root/.local/lib/python3.11/site-packages (0.6.18+pt20cpu)\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch-spline-conv\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch-geometric in /root/.local/lib/python3.11/site-packages (2.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ PyTorch 버전 확인 (cu124 = CUDA 12.4, 이걸 기반으로 설치)\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu124.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.2.0+cu124.html\n",
        "# !pip install -q torch-geometric\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.176Z"
        },
        "id": "4D_-xp-TBPke"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 📦 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import torch\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors, AllChem\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "# from torch_geometric.loader import DataLoader\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.model_selection import KFold\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "\n",
        "# # ✅ 경로 및 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ RDKit Features\n",
        "# def extract_rdkit_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [Descriptors.MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [Crippen.MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [Lipinski.NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [Lipinski.NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [rdMolDescriptors.CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [Descriptors.NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols],\n",
        "#         'HeavyAtomCount': [mol.GetNumHeavyAtoms() if mol else np.nan for mol in mols],\n",
        "#         'FractionCSP3': [rdMolDescriptors.CalcFractionCSP3(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAliphaticRings': [rdMolDescriptors.CalcNumAliphaticRings(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAromaticRings': [rdMolDescriptors.CalcNumAromaticRings(mol) if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# # ✅ Morgan Fingerprint\n",
        "# def get_morgan_fingerprint(smiles, radius=2, nBits=512):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol:\n",
        "#         fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
        "#         return np.array(fp)\n",
        "#     else:\n",
        "#         return np.zeros(nBits)\n",
        "\n",
        "# def extract_morgan_df(df, nBits=512):\n",
        "#     fps = df['Canonical_Smiles'].apply(lambda x: get_morgan_fingerprint(x, nBits=nBits))\n",
        "#     return pd.DataFrame(fps.tolist(), columns=[f'MFP_{i}' for i in range(nBits)])\n",
        "\n",
        "# # ✅ GNN용 그래프 변환\n",
        "# def mol_to_graph(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol is None:\n",
        "#         return None\n",
        "#     node_feats = [[atom.GetAtomicNum()] for atom in mol.GetAtoms()]\n",
        "#     edge_index = []\n",
        "#     for bond in mol.GetBonds():\n",
        "#         a1, a2 = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "#         edge_index.extend([[a1, a2], [a2, a1]])\n",
        "#     return Data(\n",
        "#         x=torch.tensor(node_feats, dtype=torch.float),\n",
        "#         edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "#     )\n",
        "\n",
        "# # ✅ GCN 모델 정의\n",
        "# class SimpleGCN(torch.nn.Module):\n",
        "#     def __init__(self, hidden_dim=64):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = GCNConv(1, hidden_dim)\n",
        "#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "#         self.pool = global_mean_pool\n",
        "#     def forward(self, data):\n",
        "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "#         x = torch.relu(self.conv1(x, edge_index))\n",
        "#         x = torch.relu(self.conv2(x, edge_index))\n",
        "#         return self.pool(x, batch)\n",
        "\n",
        "# # ✅ GNN 임베딩 추출 함수\n",
        "# def extract_gnn_features(df, batch_size=128):\n",
        "#     graphs = [mol_to_graph(smi) for smi in df['Canonical_Smiles']]\n",
        "#     graphs = [g for g in graphs if g is not None]\n",
        "#     for i, g in enumerate(graphs):\n",
        "#         g.batch = torch.tensor([i]*g.x.size(0))  # batch 인덱스 수동 지정\n",
        "#     loader = DataLoader(graphs, batch_size=batch_size)\n",
        "#     model = SimpleGCN()\n",
        "#     model.eval()\n",
        "#     emb_list = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in loader:\n",
        "#             emb = model(batch)\n",
        "#             emb_list.append(emb)\n",
        "#     return torch.cat(emb_list, dim=0).numpy()\n",
        "\n",
        "# # ✅ Feature 추출\n",
        "# X_train_rdkit = extract_rdkit_features(train)\n",
        "# X_test_rdkit = extract_rdkit_features(test)\n",
        "# X_train_morgan = extract_morgan_df(train)\n",
        "# X_test_morgan = extract_morgan_df(test)\n",
        "# X_train_gnn = extract_gnn_features(train)\n",
        "# X_test_gnn = extract_gnn_features(test)\n",
        "\n",
        "# # ✅ 병합\n",
        "# X_train = pd.concat([\n",
        "#     X_train_rdkit.reset_index(drop=True),\n",
        "#     X_train_morgan.reset_index(drop=True),\n",
        "#     pd.DataFrame(X_train_gnn, columns=[f\"GNN_{i}\" for i in range(X_train_gnn.shape[1])])\n",
        "# ], axis=1)\n",
        "\n",
        "# X_test = pd.concat([\n",
        "#     X_test_rdkit.reset_index(drop=True),\n",
        "#     X_test_morgan.reset_index(drop=True),\n",
        "#     pd.DataFrame(X_test_gnn, columns=[f\"GNN_{i}\" for i in range(X_test_gnn.shape[1])])\n",
        "# ], axis=1)\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# # ✅ Stacking\n",
        "# sample_weight = np.log1p(y_train)\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat, oof_lgb, oof_xgb = np.zeros(len(X_train_df)), np.zeros(len(X_train_df)), np.zeros(len(X_train_df))\n",
        "# test_cat, test_lgb, test_xgb = np.zeros(len(X_test_df)), np.zeros(len(X_test_df)), np.zeros(len(X_test_df))\n",
        "\n",
        "# for tr_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[tr_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[tr_idx]\n",
        "\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta = Ridge(alpha=1.0)\n",
        "# meta.fit(stacked_train, y_train)\n",
        "# final_preds = meta.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_with_gnn.csv', index=False)\n",
        "# print(\"✅ 최종 제출 파일 'submission_stacking_with_gnn.csv' 생성 완료!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T01:14:42.176Z"
        },
        "id": "MAfzyIUkBPke"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rdkit-pypi\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T00:48:19.090711Z",
          "iopub.execute_input": "2025-07-08T00:48:19.091127Z",
          "iopub.status.idle": "2025-07-08T00:48:25.68263Z",
          "shell.execute_reply.started": "2025-07-08T00:48:19.091097Z",
          "shell.execute_reply": "2025-07-08T00:48:25.681568Z"
        },
        "id": "opoMTHRFBPke",
        "outputId": "6da23e4c-9210-464f-bf3a-9ed95719a893"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas, numpy, tqdm, sklearn, catboost, xgboost, lightgbm, optuna\n",
        "from rdkit import Chem\n",
        "print(\"✅ All essential packages are available in Kaggle.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T00:48:34.600496Z",
          "iopub.execute_input": "2025-07-08T00:48:34.601008Z",
          "iopub.status.idle": "2025-07-08T00:48:34.71319Z",
          "shell.execute_reply.started": "2025-07-08T00:48:34.600965Z",
          "shell.execute_reply": "2025-07-08T00:48:34.712254Z"
        },
        "id": "6QXes0jpBPkj",
        "outputId": "701a9280-a7fa-44d5-ed33-6741ea6e5ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ All essential packages are available in Kaggle.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 라이브러리\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import optuna\n",
        "\n",
        "# ✅ 경로 및 데이터 로딩\n",
        "path = '/kaggle/input/drug-data'\n",
        "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "y = train_df[\"Inhibition\"]\n",
        "\n",
        "# ✅ Morgan Fingerprint\n",
        "def smiles_to_morgan(smiles, radius=2, nBits=1024):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(nBits)\n",
        "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n",
        "\n",
        "# ✅ RDKit Feature 확장\n",
        "def extract_physchem_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(15)\n",
        "    return np.array([\n",
        "        Descriptors.MolWt(mol),\n",
        "        Descriptors.TPSA(mol),\n",
        "        Descriptors.MolLogP(mol),\n",
        "        Descriptors.NumHDonors(mol),\n",
        "        Descriptors.NumHAcceptors(mol),\n",
        "        Descriptors.NumRotatableBonds(mol),\n",
        "        Descriptors.RingCount(mol),\n",
        "        Descriptors.HeavyAtomCount(mol),\n",
        "        rdMolDescriptors.CalcFractionCSP3(mol),\n",
        "        rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
        "        rdMolDescriptors.CalcNumAromaticRings(mol),\n",
        "        rdMolDescriptors.CalcNumSaturatedRings(mol),\n",
        "        Descriptors.MaxEStateIndex(mol),\n",
        "        Descriptors.MinEStateIndex(mol),\n",
        "        Descriptors.FractionCSP3(mol)\n",
        "    ])\n",
        "\n",
        "# ✅ 전체 피처 생성\n",
        "def extract_features(df, radius=2, nBits=1024):\n",
        "    features = []\n",
        "    for smi in tqdm(df['Canonical_Smiles']):\n",
        "        morgan = smiles_to_morgan(smi, radius=radius, nBits=nBits)\n",
        "        physchem = extract_physchem_features(smi)\n",
        "        full_feat = np.concatenate([morgan, physchem])\n",
        "        features.append(full_feat)\n",
        "    return np.array(features)\n",
        "\n",
        "# ✅ 피처 생성\n",
        "X = extract_features(train_df)\n",
        "X_test = extract_features(test_df)\n",
        "\n",
        "# ✅ Optuna 튜닝 (CatBoost)\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"iterations\": 1000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 20),\n",
        "        \"random_seed\": 42,\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X))\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        model = CatBoostRegressor(**params)\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, use_best_model=True)\n",
        "        oof[val_idx] = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y, oof))\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "best_params = study.best_params\n",
        "\n",
        "# ✅ 앙상블 학습 및 예측 (Cat + XGB + LGBM)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    model_cat = CatBoostRegressor(**best_params, iterations=1000, loss_function='RMSE', random_seed=fold, verbose=0)\n",
        "    model_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.03, random_state=fold)\n",
        "    model_lgb = LGBMRegressor(n_estimators=1000, learning_rate=0.03, random_state=fold)\n",
        "\n",
        "    model_cat.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
        "    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
        "    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
        "\n",
        "    preds_cat = model_cat.predict(X_val)\n",
        "    preds_xgb = model_xgb.predict(X_val)\n",
        "    preds_lgb = model_lgb.predict(X_val)\n",
        "\n",
        "    oof_preds[val_idx] = (preds_cat + preds_xgb + preds_lgb) / 3\n",
        "    test_preds += (model_cat.predict(X_test) + model_xgb.predict(X_test) + model_lgb.predict(X_test)) / (3 * kf.n_splits)\n",
        "\n",
        "# ✅ 결과 저장\n",
        "rmse = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"\\n✅ 최종 CV RMSE: {rmse:.4f}\")\n",
        "submission[\"Inhibition\"] = test_preds\n",
        "submission.to_csv(\"submission_ensemble_optuna.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T00:48:37.509845Z",
          "iopub.execute_input": "2025-07-08T00:48:37.510227Z",
          "iopub.status.idle": "2025-07-08T02:11:31.615882Z",
          "shell.execute_reply.started": "2025-07-08T00:48:37.5102Z",
          "shell.execute_reply": "2025-07-08T02:11:31.614632Z"
        },
        "id": "Y0xrrQ7NBPkj",
        "outputId": "fb439bfe-36d7-4335-dcce-fc3eeda1812f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 1681/1681 [00:04<00:00, 337.23it/s]\n100%|██████████| 100/100 [00:00<00:00, 341.29it/s]\n[I 2025-07-08 00:48:42,925] A new study created in memory with name: no-name-e96ceae7-5953-4278-aa6d-14169dfd25d6\n[I 2025-07-08 00:48:53,566] Trial 0 finished with value: 23.96204415689353 and parameters: {'learning_rate': 0.04811902658941962, 'depth': 4, 'l2_leaf_reg': 10}. Best is trial 0 with value: 23.96204415689353.\n[I 2025-07-08 00:49:02,312] Trial 1 finished with value: 23.96728729775542 and parameters: {'learning_rate': 0.16012661554387822, 'depth': 5, 'l2_leaf_reg': 19}. Best is trial 0 with value: 23.96204415689353.\n[I 2025-07-08 00:49:12,909] Trial 2 finished with value: 23.97149059702661 and parameters: {'learning_rate': 0.0567847731742868, 'depth': 4, 'l2_leaf_reg': 11}. Best is trial 0 with value: 23.96204415689353.\n[I 2025-07-08 00:49:21,071] Trial 3 finished with value: 24.016458221555624 and parameters: {'learning_rate': 0.07729534351003525, 'depth': 4, 'l2_leaf_reg': 7}. Best is trial 0 with value: 23.96204415689353.\n[I 2025-07-08 00:50:23,792] Trial 4 finished with value: 24.004126446194537 and parameters: {'learning_rate': 0.11225072972509746, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 0 with value: 23.96204415689353.\n[I 2025-07-08 00:54:04,293] Trial 5 finished with value: 23.835450595390945 and parameters: {'learning_rate': 0.010707252388039776, 'depth': 9, 'l2_leaf_reg': 6}. Best is trial 5 with value: 23.835450595390945.\n[I 2025-07-08 00:54:23,033] Trial 6 finished with value: 23.875756243472495 and parameters: {'learning_rate': 0.05135456257176041, 'depth': 6, 'l2_leaf_reg': 16}. Best is trial 5 with value: 23.835450595390945.\n[I 2025-07-08 00:54:39,342] Trial 7 finished with value: 23.94023909536288 and parameters: {'learning_rate': 0.12439656967750928, 'depth': 7, 'l2_leaf_reg': 19}. Best is trial 5 with value: 23.835450595390945.\n[I 2025-07-08 00:54:47,306] Trial 8 finished with value: 24.050433276999087 and parameters: {'learning_rate': 0.1893276347530481, 'depth': 5, 'l2_leaf_reg': 6}. Best is trial 5 with value: 23.835450595390945.\n[I 2025-07-08 00:54:55,819] Trial 9 finished with value: 23.973595350267836 and parameters: {'learning_rate': 0.18358953769850725, 'depth': 5, 'l2_leaf_reg': 11}. Best is trial 5 with value: 23.835450595390945.\n[I 2025-07-08 00:59:12,614] Trial 10 finished with value: 23.767320513856493 and parameters: {'learning_rate': 0.016093780891727756, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 23.767320513856493.\n[I 2025-07-08 01:04:32,894] Trial 11 finished with value: 23.75576860001904 and parameters: {'learning_rate': 0.01198905798089538, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:07:10,955] Trial 12 finished with value: 23.886591012518455 and parameters: {'learning_rate': 0.011841374773294838, 'depth': 9, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:10:01,806] Trial 13 finished with value: 23.80896756040849 and parameters: {'learning_rate': 0.027784213420334, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:10:30,123] Trial 14 finished with value: 23.910955747248117 and parameters: {'learning_rate': 0.08410958296719975, 'depth': 8, 'l2_leaf_reg': 3}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:11:53,930] Trial 15 finished with value: 23.879109001081737 and parameters: {'learning_rate': 0.03582037515750114, 'depth': 9, 'l2_leaf_reg': 3}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:12:24,672] Trial 16 finished with value: 23.940308624529038 and parameters: {'learning_rate': 0.08153226564437345, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:16:25,426] Trial 17 finished with value: 23.881493475777408 and parameters: {'learning_rate': 0.02615946546010398, 'depth': 10, 'l2_leaf_reg': 15}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:16:45,625] Trial 18 finished with value: 23.96375815778446 and parameters: {'learning_rate': 0.13132003809540524, 'depth': 8, 'l2_leaf_reg': 3}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:17:09,592] Trial 19 finished with value: 23.7636685033902 and parameters: {'learning_rate': 0.0655376850669583, 'depth': 7, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:17:31,989] Trial 20 finished with value: 23.934296891962063 and parameters: {'learning_rate': 0.0668903053866931, 'depth': 7, 'l2_leaf_reg': 13}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:17:42,712] Trial 21 finished with value: 24.084384405500185 and parameters: {'learning_rate': 0.09739057957815696, 'depth': 6, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:18:59,181] Trial 22 finished with value: 23.796157830860423 and parameters: {'learning_rate': 0.03975992138327307, 'depth': 9, 'l2_leaf_reg': 4}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:19:33,605] Trial 23 finished with value: 23.952026121136768 and parameters: {'learning_rate': 0.029792184928182794, 'depth': 7, 'l2_leaf_reg': 8}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:21:11,154] Trial 24 finished with value: 23.840633344709474 and parameters: {'learning_rate': 0.06454337935829604, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:23:04,563] Trial 25 finished with value: 23.910401179013878 and parameters: {'learning_rate': 0.011087201354574062, 'depth': 8, 'l2_leaf_reg': 4}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:23:21,600] Trial 26 finished with value: 23.95763185163219 and parameters: {'learning_rate': 0.041941810820923756, 'depth': 6, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:25:07,396] Trial 27 finished with value: 23.933513838554624 and parameters: {'learning_rate': 0.023922755904465527, 'depth': 9, 'l2_leaf_reg': 5}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:25:22,451] Trial 28 finished with value: 24.00553193846691 and parameters: {'learning_rate': 0.147799924412303, 'depth': 7, 'l2_leaf_reg': 3}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:26:29,684] Trial 29 finished with value: 24.035696294719184 and parameters: {'learning_rate': 0.09644702712187574, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:27:08,270] Trial 30 finished with value: 23.843635859328355 and parameters: {'learning_rate': 0.05390493204657855, 'depth': 8, 'l2_leaf_reg': 5}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:28:18,545] Trial 31 finished with value: 23.886913006552202 and parameters: {'learning_rate': 0.03915530226137139, 'depth': 9, 'l2_leaf_reg': 4}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:30:21,981] Trial 32 finished with value: 23.761423578445346 and parameters: {'learning_rate': 0.04586468561043747, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:33:37,820] Trial 33 finished with value: 23.794528439007458 and parameters: {'learning_rate': 0.021919228978846766, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:35:07,578] Trial 34 finished with value: 23.846954017913095 and parameters: {'learning_rate': 0.06801552162509937, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:36:25,114] Trial 35 finished with value: 23.828375348063066 and parameters: {'learning_rate': 0.04986656125239383, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:40:30,673] Trial 36 finished with value: 23.766280039253186 and parameters: {'learning_rate': 0.01820148407677329, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:40:47,878] Trial 37 finished with value: 23.867568585975018 and parameters: {'learning_rate': 0.056634843854240564, 'depth': 6, 'l2_leaf_reg': 6}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:43:00,804] Trial 38 finished with value: 23.886863887420066 and parameters: {'learning_rate': 0.0445445036026776, 'depth': 10, 'l2_leaf_reg': 20}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:44:32,817] Trial 39 finished with value: 23.87366438453858 and parameters: {'learning_rate': 0.033326172554704374, 'depth': 9, 'l2_leaf_reg': 10}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:44:41,182] Trial 40 finished with value: 24.02480478022059 and parameters: {'learning_rate': 0.07417828750316045, 'depth': 4, 'l2_leaf_reg': 13}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:48:31,324] Trial 41 finished with value: 23.77715008912138 and parameters: {'learning_rate': 0.017952435829324844, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:52:23,199] Trial 42 finished with value: 23.777232212170315 and parameters: {'learning_rate': 0.01725253561481046, 'depth': 10, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:53:52,536] Trial 43 finished with value: 23.805741749430446 and parameters: {'learning_rate': 0.03152689542713088, 'depth': 9, 'l2_leaf_reg': 4}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 01:59:27,603] Trial 44 finished with value: 23.769008160744004 and parameters: {'learning_rate': 0.011228711766479351, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 02:01:18,366] Trial 45 finished with value: 23.858319464938337 and parameters: {'learning_rate': 0.0474925500246602, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 02:03:26,446] Trial 46 finished with value: 23.85152485181907 and parameters: {'learning_rate': 0.021793370288597028, 'depth': 9, 'l2_leaf_reg': 5}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 02:04:02,786] Trial 47 finished with value: 23.81742015984042 and parameters: {'learning_rate': 0.06250590567108544, 'depth': 8, 'l2_leaf_reg': 2}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 02:04:36,495] Trial 48 finished with value: 23.961865869381885 and parameters: {'learning_rate': 0.03589754290247933, 'depth': 7, 'l2_leaf_reg': 7}. Best is trial 11 with value: 23.75576860001904.\n[I 2025-07-08 02:05:22,229] Trial 49 finished with value: 24.058120095999314 and parameters: {'learning_rate': 0.1699895369856193, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 11 with value: 23.75576860001904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[0]\tvalidation_0-rmse:26.25158\n[1]\tvalidation_0-rmse:26.13754\n[2]\tvalidation_0-rmse:26.03001\n[3]\tvalidation_0-rmse:25.92582\n[4]\tvalidation_0-rmse:25.81782\n[5]\tvalidation_0-rmse:25.70508\n[6]\tvalidation_0-rmse:25.60856\n[7]\tvalidation_0-rmse:25.53344\n[8]\tvalidation_0-rmse:25.45474\n[9]\tvalidation_0-rmse:25.35978\n[10]\tvalidation_0-rmse:25.29304\n[11]\tvalidation_0-rmse:25.21516\n[12]\tvalidation_0-rmse:25.17620\n[13]\tvalidation_0-rmse:25.11465\n[14]\tvalidation_0-rmse:25.05049\n[15]\tvalidation_0-rmse:24.99020\n[16]\tvalidation_0-rmse:24.95336\n[17]\tvalidation_0-rmse:24.87492\n[18]\tvalidation_0-rmse:24.81332\n[19]\tvalidation_0-rmse:24.73895\n[20]\tvalidation_0-rmse:24.67571\n[21]\tvalidation_0-rmse:24.61831\n[22]\tvalidation_0-rmse:24.58418\n[23]\tvalidation_0-rmse:24.55455\n[24]\tvalidation_0-rmse:24.51883\n[25]\tvalidation_0-rmse:24.46918\n[26]\tvalidation_0-rmse:24.44335\n[27]\tvalidation_0-rmse:24.40189\n[28]\tvalidation_0-rmse:24.37022\n[29]\tvalidation_0-rmse:24.32467\n[30]\tvalidation_0-rmse:24.30783\n[31]\tvalidation_0-rmse:24.29457\n[32]\tvalidation_0-rmse:24.26752\n[33]\tvalidation_0-rmse:24.23806\n[34]\tvalidation_0-rmse:24.22110\n[35]\tvalidation_0-rmse:24.19272\n[36]\tvalidation_0-rmse:24.18871\n[37]\tvalidation_0-rmse:24.16247\n[38]\tvalidation_0-rmse:24.16755\n[39]\tvalidation_0-rmse:24.14812\n[40]\tvalidation_0-rmse:24.15455\n[41]\tvalidation_0-rmse:24.12661\n[42]\tvalidation_0-rmse:24.10916\n[43]\tvalidation_0-rmse:24.12236\n[44]\tvalidation_0-rmse:24.11714\n[45]\tvalidation_0-rmse:24.10249\n[46]\tvalidation_0-rmse:24.08155\n[47]\tvalidation_0-rmse:24.08893\n[48]\tvalidation_0-rmse:24.05656\n[49]\tvalidation_0-rmse:24.04900\n[50]\tvalidation_0-rmse:24.04548\n[51]\tvalidation_0-rmse:24.03859\n[52]\tvalidation_0-rmse:24.03516\n[53]\tvalidation_0-rmse:24.02716\n[54]\tvalidation_0-rmse:24.03295\n[55]\tvalidation_0-rmse:24.02296\n[56]\tvalidation_0-rmse:24.00796\n[57]\tvalidation_0-rmse:23.98280\n[58]\tvalidation_0-rmse:23.98481\n[59]\tvalidation_0-rmse:23.98639\n[60]\tvalidation_0-rmse:23.98514\n[61]\tvalidation_0-rmse:23.99634\n[62]\tvalidation_0-rmse:23.98320\n[63]\tvalidation_0-rmse:23.98604\n[64]\tvalidation_0-rmse:23.97529\n[65]\tvalidation_0-rmse:23.96116\n[66]\tvalidation_0-rmse:23.95482\n[67]\tvalidation_0-rmse:23.94560\n[68]\tvalidation_0-rmse:23.93785\n[69]\tvalidation_0-rmse:23.94407\n[70]\tvalidation_0-rmse:23.92539\n[71]\tvalidation_0-rmse:23.92391\n[72]\tvalidation_0-rmse:23.91688\n[73]\tvalidation_0-rmse:23.89931\n[74]\tvalidation_0-rmse:23.89590\n[75]\tvalidation_0-rmse:23.87985\n[76]\tvalidation_0-rmse:23.85364\n[77]\tvalidation_0-rmse:23.84418\n[78]\tvalidation_0-rmse:23.84155\n[79]\tvalidation_0-rmse:23.82966\n[80]\tvalidation_0-rmse:23.83967\n[81]\tvalidation_0-rmse:23.83895\n[82]\tvalidation_0-rmse:23.83394\n[83]\tvalidation_0-rmse:23.84384\n[84]\tvalidation_0-rmse:23.84801\n[85]\tvalidation_0-rmse:23.84050\n[86]\tvalidation_0-rmse:23.83810\n[87]\tvalidation_0-rmse:23.83245\n[88]\tvalidation_0-rmse:23.83224\n[89]\tvalidation_0-rmse:23.83896\n[90]\tvalidation_0-rmse:23.84767\n[91]\tvalidation_0-rmse:23.83654\n[92]\tvalidation_0-rmse:23.82892\n[93]\tvalidation_0-rmse:23.83120\n[94]\tvalidation_0-rmse:23.82622\n[95]\tvalidation_0-rmse:23.82848\n[96]\tvalidation_0-rmse:23.82670\n[97]\tvalidation_0-rmse:23.82159\n[98]\tvalidation_0-rmse:23.82132\n[99]\tvalidation_0-rmse:23.82088\n[100]\tvalidation_0-rmse:23.82933\n[101]\tvalidation_0-rmse:23.81355\n[102]\tvalidation_0-rmse:23.81060\n[103]\tvalidation_0-rmse:23.80030\n[104]\tvalidation_0-rmse:23.80468\n[105]\tvalidation_0-rmse:23.80292\n[106]\tvalidation_0-rmse:23.78593\n[107]\tvalidation_0-rmse:23.78651\n[108]\tvalidation_0-rmse:23.78429\n[109]\tvalidation_0-rmse:23.79115\n[110]\tvalidation_0-rmse:23.79298\n[111]\tvalidation_0-rmse:23.79138\n[112]\tvalidation_0-rmse:23.78332\n[113]\tvalidation_0-rmse:23.78618\n[114]\tvalidation_0-rmse:23.78977\n[115]\tvalidation_0-rmse:23.79436\n[116]\tvalidation_0-rmse:23.78477\n[117]\tvalidation_0-rmse:23.78303\n[118]\tvalidation_0-rmse:23.78304\n[119]\tvalidation_0-rmse:23.79041\n[120]\tvalidation_0-rmse:23.78154\n[121]\tvalidation_0-rmse:23.79089\n[122]\tvalidation_0-rmse:23.77828\n[123]\tvalidation_0-rmse:23.77930\n[124]\tvalidation_0-rmse:23.77471\n[125]\tvalidation_0-rmse:23.77515\n[126]\tvalidation_0-rmse:23.78550\n[127]\tvalidation_0-rmse:23.78584\n[128]\tvalidation_0-rmse:23.79052\n[129]\tvalidation_0-rmse:23.79216\n[130]\tvalidation_0-rmse:23.80710\n[131]\tvalidation_0-rmse:23.82179\n[132]\tvalidation_0-rmse:23.81169\n[133]\tvalidation_0-rmse:23.81766\n[134]\tvalidation_0-rmse:23.81594\n[135]\tvalidation_0-rmse:23.81848\n[136]\tvalidation_0-rmse:23.81676\n[137]\tvalidation_0-rmse:23.82529\n[138]\tvalidation_0-rmse:23.83607\n[139]\tvalidation_0-rmse:23.83288\n[140]\tvalidation_0-rmse:23.82977\n[141]\tvalidation_0-rmse:23.82076\n[142]\tvalidation_0-rmse:23.81763\n[143]\tvalidation_0-rmse:23.81697\n[144]\tvalidation_0-rmse:23.81391\n[145]\tvalidation_0-rmse:23.81829\n[146]\tvalidation_0-rmse:23.80916\n[147]\tvalidation_0-rmse:23.81248\n[148]\tvalidation_0-rmse:23.81583\n[149]\tvalidation_0-rmse:23.81316\n[150]\tvalidation_0-rmse:23.81174\n[151]\tvalidation_0-rmse:23.81313\n[152]\tvalidation_0-rmse:23.81270\n[153]\tvalidation_0-rmse:23.82069\n[154]\tvalidation_0-rmse:23.81885\n[155]\tvalidation_0-rmse:23.82742\n[156]\tvalidation_0-rmse:23.83101\n[157]\tvalidation_0-rmse:23.82934\n[158]\tvalidation_0-rmse:23.82230\n[159]\tvalidation_0-rmse:23.82474\n[160]\tvalidation_0-rmse:23.82470\n[161]\tvalidation_0-rmse:23.82730\n[162]\tvalidation_0-rmse:23.82905\n[163]\tvalidation_0-rmse:23.83092\n[164]\tvalidation_0-rmse:23.82538\n[165]\tvalidation_0-rmse:23.82364\n[166]\tvalidation_0-rmse:23.82384\n[167]\tvalidation_0-rmse:23.81901\n[168]\tvalidation_0-rmse:23.81306\n[169]\tvalidation_0-rmse:23.82409\n[170]\tvalidation_0-rmse:23.82674\n[171]\tvalidation_0-rmse:23.83187\n[172]\tvalidation_0-rmse:23.82662\n[173]\tvalidation_0-rmse:23.82342\n[174]\tvalidation_0-rmse:23.81602\n[175]\tvalidation_0-rmse:23.81284\n[176]\tvalidation_0-rmse:23.81143\n[177]\tvalidation_0-rmse:23.81245\n[178]\tvalidation_0-rmse:23.80522\n[179]\tvalidation_0-rmse:23.80298\n[180]\tvalidation_0-rmse:23.81312\n[181]\tvalidation_0-rmse:23.81862\n[182]\tvalidation_0-rmse:23.81879\n[183]\tvalidation_0-rmse:23.81982\n[184]\tvalidation_0-rmse:23.81500\n[185]\tvalidation_0-rmse:23.82029\n[186]\tvalidation_0-rmse:23.82378\n[187]\tvalidation_0-rmse:23.82230\n[188]\tvalidation_0-rmse:23.82878\n[189]\tvalidation_0-rmse:23.83246\n[190]\tvalidation_0-rmse:23.83144\n[191]\tvalidation_0-rmse:23.82316\n[192]\tvalidation_0-rmse:23.81507\n[193]\tvalidation_0-rmse:23.81887\n[194]\tvalidation_0-rmse:23.81737\n[195]\tvalidation_0-rmse:23.82498\n[196]\tvalidation_0-rmse:23.82532\n[197]\tvalidation_0-rmse:23.82777\n[198]\tvalidation_0-rmse:23.82338\n[199]\tvalidation_0-rmse:23.82640\n[200]\tvalidation_0-rmse:23.82708\n[201]\tvalidation_0-rmse:23.83454\n[202]\tvalidation_0-rmse:23.83225\n[203]\tvalidation_0-rmse:23.83411\n[204]\tvalidation_0-rmse:23.83730\n[205]\tvalidation_0-rmse:23.83774\n[206]\tvalidation_0-rmse:23.84303\n[207]\tvalidation_0-rmse:23.85176\n[208]\tvalidation_0-rmse:23.84588\n[209]\tvalidation_0-rmse:23.84893\n[210]\tvalidation_0-rmse:23.84663\n[211]\tvalidation_0-rmse:23.84105\n[212]\tvalidation_0-rmse:23.84004\n[213]\tvalidation_0-rmse:23.84078\n[214]\tvalidation_0-rmse:23.82839\n[215]\tvalidation_0-rmse:23.83141\n[216]\tvalidation_0-rmse:23.83801\n[217]\tvalidation_0-rmse:23.83864\n[218]\tvalidation_0-rmse:23.83675\n[219]\tvalidation_0-rmse:23.83217\n[220]\tvalidation_0-rmse:23.83646\n[221]\tvalidation_0-rmse:23.83727\n[222]\tvalidation_0-rmse:23.84915\n[223]\tvalidation_0-rmse:23.84203\n[224]\tvalidation_0-rmse:23.84168\n[225]\tvalidation_0-rmse:23.83907\n[226]\tvalidation_0-rmse:23.83643\n[227]\tvalidation_0-rmse:23.83796\n[228]\tvalidation_0-rmse:23.84098\n[229]\tvalidation_0-rmse:23.84180\n[230]\tvalidation_0-rmse:23.84108\n[231]\tvalidation_0-rmse:23.83947\n[232]\tvalidation_0-rmse:23.83536\n[233]\tvalidation_0-rmse:23.83555\n[234]\tvalidation_0-rmse:23.83887\n[235]\tvalidation_0-rmse:23.83936\n[236]\tvalidation_0-rmse:23.83771\n[237]\tvalidation_0-rmse:23.83576\n[238]\tvalidation_0-rmse:23.83745\n[239]\tvalidation_0-rmse:23.82936\n[240]\tvalidation_0-rmse:23.82447\n[241]\tvalidation_0-rmse:23.82107\n[242]\tvalidation_0-rmse:23.81576\n[243]\tvalidation_0-rmse:23.81194\n[244]\tvalidation_0-rmse:23.81191\n[245]\tvalidation_0-rmse:23.81564\n[246]\tvalidation_0-rmse:23.81363\n[247]\tvalidation_0-rmse:23.81563\n[248]\tvalidation_0-rmse:23.81520\n[249]\tvalidation_0-rmse:23.80667\n[250]\tvalidation_0-rmse:23.80331\n[251]\tvalidation_0-rmse:23.79412\n[252]\tvalidation_0-rmse:23.78628\n[253]\tvalidation_0-rmse:23.78634\n[254]\tvalidation_0-rmse:23.79128\n[255]\tvalidation_0-rmse:23.78594\n[256]\tvalidation_0-rmse:23.78300\n[257]\tvalidation_0-rmse:23.78764\n[258]\tvalidation_0-rmse:23.79354\n[259]\tvalidation_0-rmse:23.79451\n[260]\tvalidation_0-rmse:23.79897\n[261]\tvalidation_0-rmse:23.79066\n[262]\tvalidation_0-rmse:23.78794\n[263]\tvalidation_0-rmse:23.78519\n[264]\tvalidation_0-rmse:23.78322\n[265]\tvalidation_0-rmse:23.78434\n[266]\tvalidation_0-rmse:23.78810\n[267]\tvalidation_0-rmse:23.79104\n[268]\tvalidation_0-rmse:23.78946\n[269]\tvalidation_0-rmse:23.79370\n[270]\tvalidation_0-rmse:23.78586\n[271]\tvalidation_0-rmse:23.78290\n[272]\tvalidation_0-rmse:23.77729\n[273]\tvalidation_0-rmse:23.77607\n[274]\tvalidation_0-rmse:23.77147\n[275]\tvalidation_0-rmse:23.77362\n[276]\tvalidation_0-rmse:23.76816\n[277]\tvalidation_0-rmse:23.76853\n[278]\tvalidation_0-rmse:23.76488\n[279]\tvalidation_0-rmse:23.76195\n[280]\tvalidation_0-rmse:23.76424\n[281]\tvalidation_0-rmse:23.76065\n[282]\tvalidation_0-rmse:23.75991\n[283]\tvalidation_0-rmse:23.76352\n[284]\tvalidation_0-rmse:23.76026\n[285]\tvalidation_0-rmse:23.76661\n[286]\tvalidation_0-rmse:23.76567\n[287]\tvalidation_0-rmse:23.76733\n[288]\tvalidation_0-rmse:23.76462\n[289]\tvalidation_0-rmse:23.75710\n[290]\tvalidation_0-rmse:23.75466\n[291]\tvalidation_0-rmse:23.74724\n[292]\tvalidation_0-rmse:23.74219\n[293]\tvalidation_0-rmse:23.74578\n[294]\tvalidation_0-rmse:23.74999\n[295]\tvalidation_0-rmse:23.75671\n[296]\tvalidation_0-rmse:23.75458\n[297]\tvalidation_0-rmse:23.75651\n[298]\tvalidation_0-rmse:23.75605\n[299]\tvalidation_0-rmse:23.75600\n[300]\tvalidation_0-rmse:23.75264\n[301]\tvalidation_0-rmse:23.75090\n[302]\tvalidation_0-rmse:23.75631\n[303]\tvalidation_0-rmse:23.76055\n[304]\tvalidation_0-rmse:23.75780\n[305]\tvalidation_0-rmse:23.75958\n[306]\tvalidation_0-rmse:23.75831\n[307]\tvalidation_0-rmse:23.75451\n[308]\tvalidation_0-rmse:23.74985\n[309]\tvalidation_0-rmse:23.75295\n[310]\tvalidation_0-rmse:23.75676\n[311]\tvalidation_0-rmse:23.75635\n[312]\tvalidation_0-rmse:23.76441\n[313]\tvalidation_0-rmse:23.76077\n[314]\tvalidation_0-rmse:23.76052\n[315]\tvalidation_0-rmse:23.75589\n[316]\tvalidation_0-rmse:23.76108\n[317]\tvalidation_0-rmse:23.75835\n[318]\tvalidation_0-rmse:23.75383\n[319]\tvalidation_0-rmse:23.75117\n[320]\tvalidation_0-rmse:23.75023\n[321]\tvalidation_0-rmse:23.74764\n[322]\tvalidation_0-rmse:23.74584\n[323]\tvalidation_0-rmse:23.74942\n[324]\tvalidation_0-rmse:23.74865\n[325]\tvalidation_0-rmse:23.74655\n[326]\tvalidation_0-rmse:23.74645\n[327]\tvalidation_0-rmse:23.75243\n[328]\tvalidation_0-rmse:23.75872\n[329]\tvalidation_0-rmse:23.75546\n[330]\tvalidation_0-rmse:23.75849\n[331]\tvalidation_0-rmse:23.75902\n[332]\tvalidation_0-rmse:23.76435\n[333]\tvalidation_0-rmse:23.76605\n[334]\tvalidation_0-rmse:23.76600\n[335]\tvalidation_0-rmse:23.77217\n[336]\tvalidation_0-rmse:23.77801\n[337]\tvalidation_0-rmse:23.77351\n[338]\tvalidation_0-rmse:23.76885\n[339]\tvalidation_0-rmse:23.76568\n[340]\tvalidation_0-rmse:23.76791\n[341]\tvalidation_0-rmse:23.76867\n[342]\tvalidation_0-rmse:23.76928\n[343]\tvalidation_0-rmse:23.77076\n[344]\tvalidation_0-rmse:23.76082\n[345]\tvalidation_0-rmse:23.76746\n[346]\tvalidation_0-rmse:23.76894\n[347]\tvalidation_0-rmse:23.76948\n[348]\tvalidation_0-rmse:23.77078\n[349]\tvalidation_0-rmse:23.77079\n[350]\tvalidation_0-rmse:23.77276\n[351]\tvalidation_0-rmse:23.77416\n[352]\tvalidation_0-rmse:23.77285\n[353]\tvalidation_0-rmse:23.77055\n[354]\tvalidation_0-rmse:23.76609\n[355]\tvalidation_0-rmse:23.76301\n[356]\tvalidation_0-rmse:23.76227\n[357]\tvalidation_0-rmse:23.76108\n[358]\tvalidation_0-rmse:23.75613\n[359]\tvalidation_0-rmse:23.75644\n[360]\tvalidation_0-rmse:23.76157\n[361]\tvalidation_0-rmse:23.76048\n[362]\tvalidation_0-rmse:23.76091\n[363]\tvalidation_0-rmse:23.75530\n[364]\tvalidation_0-rmse:23.75608\n[365]\tvalidation_0-rmse:23.75447\n[366]\tvalidation_0-rmse:23.75914\n[367]\tvalidation_0-rmse:23.75487\n[368]\tvalidation_0-rmse:23.75698\n[369]\tvalidation_0-rmse:23.75759\n[370]\tvalidation_0-rmse:23.76309\n[371]\tvalidation_0-rmse:23.76016\n[372]\tvalidation_0-rmse:23.76504\n[373]\tvalidation_0-rmse:23.76544\n[374]\tvalidation_0-rmse:23.76757\n[375]\tvalidation_0-rmse:23.77108\n[376]\tvalidation_0-rmse:23.77487\n[377]\tvalidation_0-rmse:23.78136\n[378]\tvalidation_0-rmse:23.78549\n[379]\tvalidation_0-rmse:23.79102\n[380]\tvalidation_0-rmse:23.79510\n[381]\tvalidation_0-rmse:23.79713\n[382]\tvalidation_0-rmse:23.79252\n[383]\tvalidation_0-rmse:23.78746\n[384]\tvalidation_0-rmse:23.78228\n[385]\tvalidation_0-rmse:23.78139\n[386]\tvalidation_0-rmse:23.78429\n[387]\tvalidation_0-rmse:23.78348\n[388]\tvalidation_0-rmse:23.78995\n[389]\tvalidation_0-rmse:23.79278\n[390]\tvalidation_0-rmse:23.78844\n[391]\tvalidation_0-rmse:23.78645\n[392]\tvalidation_0-rmse:23.79271\n[393]\tvalidation_0-rmse:23.79448\n[394]\tvalidation_0-rmse:23.79157\n[395]\tvalidation_0-rmse:23.79329\n[396]\tvalidation_0-rmse:23.80026\n[397]\tvalidation_0-rmse:23.80205\n[398]\tvalidation_0-rmse:23.79988\n[399]\tvalidation_0-rmse:23.80064\n[400]\tvalidation_0-rmse:23.79316\n[401]\tvalidation_0-rmse:23.79191\n[402]\tvalidation_0-rmse:23.79309\n[403]\tvalidation_0-rmse:23.79073\n[404]\tvalidation_0-rmse:23.78860\n[405]\tvalidation_0-rmse:23.79005\n[406]\tvalidation_0-rmse:23.78246\n[407]\tvalidation_0-rmse:23.77505\n[408]\tvalidation_0-rmse:23.77989\n[409]\tvalidation_0-rmse:23.78171\n[410]\tvalidation_0-rmse:23.78241\n[411]\tvalidation_0-rmse:23.78384\n[412]\tvalidation_0-rmse:23.78332\n[413]\tvalidation_0-rmse:23.78846\n[414]\tvalidation_0-rmse:23.79355\n[415]\tvalidation_0-rmse:23.79655\n[416]\tvalidation_0-rmse:23.79634\n[417]\tvalidation_0-rmse:23.79845\n[418]\tvalidation_0-rmse:23.80115\n[419]\tvalidation_0-rmse:23.80395\n[420]\tvalidation_0-rmse:23.79679\n[421]\tvalidation_0-rmse:23.80060\n[422]\tvalidation_0-rmse:23.79882\n[423]\tvalidation_0-rmse:23.79650\n[424]\tvalidation_0-rmse:23.79510\n[425]\tvalidation_0-rmse:23.79162\n[426]\tvalidation_0-rmse:23.79244\n[427]\tvalidation_0-rmse:23.79407\n[428]\tvalidation_0-rmse:23.79403\n[429]\tvalidation_0-rmse:23.79234\n[430]\tvalidation_0-rmse:23.79604\n[431]\tvalidation_0-rmse:23.79839\n[432]\tvalidation_0-rmse:23.80029\n[433]\tvalidation_0-rmse:23.80367\n[434]\tvalidation_0-rmse:23.79930\n[435]\tvalidation_0-rmse:23.79688\n[436]\tvalidation_0-rmse:23.79837\n[437]\tvalidation_0-rmse:23.80191\n[438]\tvalidation_0-rmse:23.80213\n[439]\tvalidation_0-rmse:23.79939\n[440]\tvalidation_0-rmse:23.79353\n[441]\tvalidation_0-rmse:23.79394\n[442]\tvalidation_0-rmse:23.79974\n[443]\tvalidation_0-rmse:23.79608\n[444]\tvalidation_0-rmse:23.79573\n[445]\tvalidation_0-rmse:23.79728\n[446]\tvalidation_0-rmse:23.79720\n[447]\tvalidation_0-rmse:23.80130\n[448]\tvalidation_0-rmse:23.79932\n[449]\tvalidation_0-rmse:23.80800\n[450]\tvalidation_0-rmse:23.80864\n[451]\tvalidation_0-rmse:23.80593\n[452]\tvalidation_0-rmse:23.81138\n[453]\tvalidation_0-rmse:23.81294\n[454]\tvalidation_0-rmse:23.81330\n[455]\tvalidation_0-rmse:23.81706\n[456]\tvalidation_0-rmse:23.81480\n[457]\tvalidation_0-rmse:23.82328\n[458]\tvalidation_0-rmse:23.82400\n[459]\tvalidation_0-rmse:23.82739\n[460]\tvalidation_0-rmse:23.82823\n[461]\tvalidation_0-rmse:23.82984\n[462]\tvalidation_0-rmse:23.83413\n[463]\tvalidation_0-rmse:23.83490\n[464]\tvalidation_0-rmse:23.83592\n[465]\tvalidation_0-rmse:23.84328\n[466]\tvalidation_0-rmse:23.84246\n[467]\tvalidation_0-rmse:23.84319\n[468]\tvalidation_0-rmse:23.84305\n[469]\tvalidation_0-rmse:23.84150\n[470]\tvalidation_0-rmse:23.84588\n[471]\tvalidation_0-rmse:23.84526\n[472]\tvalidation_0-rmse:23.84349\n[473]\tvalidation_0-rmse:23.84321\n[474]\tvalidation_0-rmse:23.84361\n[475]\tvalidation_0-rmse:23.84875\n[476]\tvalidation_0-rmse:23.85559\n[477]\tvalidation_0-rmse:23.85761\n[478]\tvalidation_0-rmse:23.85468\n[479]\tvalidation_0-rmse:23.85686\n[480]\tvalidation_0-rmse:23.85811\n[481]\tvalidation_0-rmse:23.85471\n[482]\tvalidation_0-rmse:23.85346\n[483]\tvalidation_0-rmse:23.85509\n[484]\tvalidation_0-rmse:23.85100\n[485]\tvalidation_0-rmse:23.85070\n[486]\tvalidation_0-rmse:23.84428\n[487]\tvalidation_0-rmse:23.84456\n[488]\tvalidation_0-rmse:23.84973\n[489]\tvalidation_0-rmse:23.85377\n[490]\tvalidation_0-rmse:23.85569\n[491]\tvalidation_0-rmse:23.85385\n[492]\tvalidation_0-rmse:23.85459\n[493]\tvalidation_0-rmse:23.85312\n[494]\tvalidation_0-rmse:23.85213\n[495]\tvalidation_0-rmse:23.85185\n[496]\tvalidation_0-rmse:23.85164\n[497]\tvalidation_0-rmse:23.85982\n[498]\tvalidation_0-rmse:23.85794\n[499]\tvalidation_0-rmse:23.85397\n[500]\tvalidation_0-rmse:23.85859\n[501]\tvalidation_0-rmse:23.86073\n[502]\tvalidation_0-rmse:23.86094\n[503]\tvalidation_0-rmse:23.86087\n[504]\tvalidation_0-rmse:23.86141\n[505]\tvalidation_0-rmse:23.86367\n[506]\tvalidation_0-rmse:23.85910\n[507]\tvalidation_0-rmse:23.85793\n[508]\tvalidation_0-rmse:23.85259\n[509]\tvalidation_0-rmse:23.85076\n[510]\tvalidation_0-rmse:23.84773\n[511]\tvalidation_0-rmse:23.84460\n[512]\tvalidation_0-rmse:23.84558\n[513]\tvalidation_0-rmse:23.84299\n[514]\tvalidation_0-rmse:23.84956\n[515]\tvalidation_0-rmse:23.85395\n[516]\tvalidation_0-rmse:23.85552\n[517]\tvalidation_0-rmse:23.85845\n[518]\tvalidation_0-rmse:23.85819\n[519]\tvalidation_0-rmse:23.85724\n[520]\tvalidation_0-rmse:23.85881\n[521]\tvalidation_0-rmse:23.86188\n[522]\tvalidation_0-rmse:23.87094\n[523]\tvalidation_0-rmse:23.86778\n[524]\tvalidation_0-rmse:23.86787\n[525]\tvalidation_0-rmse:23.86940\n[526]\tvalidation_0-rmse:23.86791\n[527]\tvalidation_0-rmse:23.87147\n[528]\tvalidation_0-rmse:23.87414\n[529]\tvalidation_0-rmse:23.87444\n[530]\tvalidation_0-rmse:23.87605\n[531]\tvalidation_0-rmse:23.87245\n[532]\tvalidation_0-rmse:23.87243\n[533]\tvalidation_0-rmse:23.87032\n[534]\tvalidation_0-rmse:23.87200\n[535]\tvalidation_0-rmse:23.87134\n[536]\tvalidation_0-rmse:23.86741\n[537]\tvalidation_0-rmse:23.86890\n[538]\tvalidation_0-rmse:23.86929\n[539]\tvalidation_0-rmse:23.87011\n[540]\tvalidation_0-rmse:23.86807\n[541]\tvalidation_0-rmse:23.86688\n[542]\tvalidation_0-rmse:23.87130\n[543]\tvalidation_0-rmse:23.87518\n[544]\tvalidation_0-rmse:23.87466\n[545]\tvalidation_0-rmse:23.87528\n[546]\tvalidation_0-rmse:23.87386\n[547]\tvalidation_0-rmse:23.87110\n[548]\tvalidation_0-rmse:23.87141\n[549]\tvalidation_0-rmse:23.87263\n[550]\tvalidation_0-rmse:23.87296\n[551]\tvalidation_0-rmse:23.87233\n[552]\tvalidation_0-rmse:23.87219\n[553]\tvalidation_0-rmse:23.87864\n[554]\tvalidation_0-rmse:23.87740\n[555]\tvalidation_0-rmse:23.87621\n[556]\tvalidation_0-rmse:23.87615\n[557]\tvalidation_0-rmse:23.87838\n[558]\tvalidation_0-rmse:23.87997\n[559]\tvalidation_0-rmse:23.88049\n[560]\tvalidation_0-rmse:23.87912\n[561]\tvalidation_0-rmse:23.88058\n[562]\tvalidation_0-rmse:23.88198\n[563]\tvalidation_0-rmse:23.88695\n[564]\tvalidation_0-rmse:23.89138\n[565]\tvalidation_0-rmse:23.89055\n[566]\tvalidation_0-rmse:23.89345\n[567]\tvalidation_0-rmse:23.89625\n[568]\tvalidation_0-rmse:23.89970\n[569]\tvalidation_0-rmse:23.89899\n[570]\tvalidation_0-rmse:23.90013\n[571]\tvalidation_0-rmse:23.90229\n[572]\tvalidation_0-rmse:23.90155\n[573]\tvalidation_0-rmse:23.90079\n[574]\tvalidation_0-rmse:23.90375\n[575]\tvalidation_0-rmse:23.90369\n[576]\tvalidation_0-rmse:23.90482\n[577]\tvalidation_0-rmse:23.90461\n[578]\tvalidation_0-rmse:23.90729\n[579]\tvalidation_0-rmse:23.90872\n[580]\tvalidation_0-rmse:23.90996\n[581]\tvalidation_0-rmse:23.90972\n[582]\tvalidation_0-rmse:23.90977\n[583]\tvalidation_0-rmse:23.90723\n[584]\tvalidation_0-rmse:23.90992\n[585]\tvalidation_0-rmse:23.90785\n[586]\tvalidation_0-rmse:23.90733\n[587]\tvalidation_0-rmse:23.90927\n[588]\tvalidation_0-rmse:23.90995\n[589]\tvalidation_0-rmse:23.90995\n[590]\tvalidation_0-rmse:23.91358\n[591]\tvalidation_0-rmse:23.91077\n[592]\tvalidation_0-rmse:23.91257\n[593]\tvalidation_0-rmse:23.91249\n[594]\tvalidation_0-rmse:23.91439\n[595]\tvalidation_0-rmse:23.91400\n[596]\tvalidation_0-rmse:23.91478\n[597]\tvalidation_0-rmse:23.91918\n[598]\tvalidation_0-rmse:23.91929\n[599]\tvalidation_0-rmse:23.92353\n[600]\tvalidation_0-rmse:23.92453\n[601]\tvalidation_0-rmse:23.92046\n[602]\tvalidation_0-rmse:23.92328\n[603]\tvalidation_0-rmse:23.92336\n[604]\tvalidation_0-rmse:23.92536\n[605]\tvalidation_0-rmse:23.92442\n[606]\tvalidation_0-rmse:23.92364\n[607]\tvalidation_0-rmse:23.92258\n[608]\tvalidation_0-rmse:23.92438\n[609]\tvalidation_0-rmse:23.92736\n[610]\tvalidation_0-rmse:23.92768\n[611]\tvalidation_0-rmse:23.92676\n[612]\tvalidation_0-rmse:23.92685\n[613]\tvalidation_0-rmse:23.92638\n[614]\tvalidation_0-rmse:23.92850\n[615]\tvalidation_0-rmse:23.93097\n[616]\tvalidation_0-rmse:23.92695\n[617]\tvalidation_0-rmse:23.92936\n[618]\tvalidation_0-rmse:23.92880\n[619]\tvalidation_0-rmse:23.92821\n[620]\tvalidation_0-rmse:23.92705\n[621]\tvalidation_0-rmse:23.92641\n[622]\tvalidation_0-rmse:23.92415\n[623]\tvalidation_0-rmse:23.92693\n[624]\tvalidation_0-rmse:23.92657\n[625]\tvalidation_0-rmse:23.92359\n[626]\tvalidation_0-rmse:23.92820\n[627]\tvalidation_0-rmse:23.93152\n[628]\tvalidation_0-rmse:23.93288\n[629]\tvalidation_0-rmse:23.93257\n[630]\tvalidation_0-rmse:23.93500\n[631]\tvalidation_0-rmse:23.93537\n[632]\tvalidation_0-rmse:23.93611\n[633]\tvalidation_0-rmse:23.93733\n[634]\tvalidation_0-rmse:23.93551\n[635]\tvalidation_0-rmse:23.93655\n[636]\tvalidation_0-rmse:23.93368\n[637]\tvalidation_0-rmse:23.93354\n[638]\tvalidation_0-rmse:23.93219\n[639]\tvalidation_0-rmse:23.93449\n[640]\tvalidation_0-rmse:23.93288\n[641]\tvalidation_0-rmse:23.93267\n[642]\tvalidation_0-rmse:23.93396\n[643]\tvalidation_0-rmse:23.93307\n[644]\tvalidation_0-rmse:23.93283\n[645]\tvalidation_0-rmse:23.93665\n[646]\tvalidation_0-rmse:23.93797\n[647]\tvalidation_0-rmse:23.93677\n[648]\tvalidation_0-rmse:23.93762\n[649]\tvalidation_0-rmse:23.93661\n[650]\tvalidation_0-rmse:23.93318\n[651]\tvalidation_0-rmse:23.93514\n[652]\tvalidation_0-rmse:23.92842\n[653]\tvalidation_0-rmse:23.92627\n[654]\tvalidation_0-rmse:23.92749\n[655]\tvalidation_0-rmse:23.93017\n[656]\tvalidation_0-rmse:23.93097\n[657]\tvalidation_0-rmse:23.93195\n[658]\tvalidation_0-rmse:23.93494\n[659]\tvalidation_0-rmse:23.93311\n[660]\tvalidation_0-rmse:23.93264\n[661]\tvalidation_0-rmse:23.92834\n[662]\tvalidation_0-rmse:23.92964\n[663]\tvalidation_0-rmse:23.92505\n[664]\tvalidation_0-rmse:23.92671\n[665]\tvalidation_0-rmse:23.92796\n[666]\tvalidation_0-rmse:23.92905\n[667]\tvalidation_0-rmse:23.92855\n[668]\tvalidation_0-rmse:23.93049\n[669]\tvalidation_0-rmse:23.93177\n[670]\tvalidation_0-rmse:23.93169\n[671]\tvalidation_0-rmse:23.93214\n[672]\tvalidation_0-rmse:23.93350\n[673]\tvalidation_0-rmse:23.93502\n[674]\tvalidation_0-rmse:23.93559\n[675]\tvalidation_0-rmse:23.93152\n[676]\tvalidation_0-rmse:23.93059\n[677]\tvalidation_0-rmse:23.92879\n[678]\tvalidation_0-rmse:23.92835\n[679]\tvalidation_0-rmse:23.93003\n[680]\tvalidation_0-rmse:23.92918\n[681]\tvalidation_0-rmse:23.93031\n[682]\tvalidation_0-rmse:23.92931\n[683]\tvalidation_0-rmse:23.93153\n[684]\tvalidation_0-rmse:23.93274\n[685]\tvalidation_0-rmse:23.92853\n[686]\tvalidation_0-rmse:23.92840\n[687]\tvalidation_0-rmse:23.93052\n[688]\tvalidation_0-rmse:23.93095\n[689]\tvalidation_0-rmse:23.93306\n[690]\tvalidation_0-rmse:23.93283\n[691]\tvalidation_0-rmse:23.93481\n[692]\tvalidation_0-rmse:23.93652\n[693]\tvalidation_0-rmse:23.93659\n[694]\tvalidation_0-rmse:23.93840\n[695]\tvalidation_0-rmse:23.93989\n[696]\tvalidation_0-rmse:23.93957\n[697]\tvalidation_0-rmse:23.94265\n[698]\tvalidation_0-rmse:23.94211\n[699]\tvalidation_0-rmse:23.94306\n[700]\tvalidation_0-rmse:23.94072\n[701]\tvalidation_0-rmse:23.94182\n[702]\tvalidation_0-rmse:23.94251\n[703]\tvalidation_0-rmse:23.94279\n[704]\tvalidation_0-rmse:23.94265\n[705]\tvalidation_0-rmse:23.94430\n[706]\tvalidation_0-rmse:23.94641\n[707]\tvalidation_0-rmse:23.94780\n[708]\tvalidation_0-rmse:23.94998\n[709]\tvalidation_0-rmse:23.95384\n[710]\tvalidation_0-rmse:23.95479\n[711]\tvalidation_0-rmse:23.94874\n[712]\tvalidation_0-rmse:23.94630\n[713]\tvalidation_0-rmse:23.94139\n[714]\tvalidation_0-rmse:23.94298\n[715]\tvalidation_0-rmse:23.94411\n[716]\tvalidation_0-rmse:23.94294\n[717]\tvalidation_0-rmse:23.94282\n[718]\tvalidation_0-rmse:23.93944\n[719]\tvalidation_0-rmse:23.93990\n[720]\tvalidation_0-rmse:23.93923\n[721]\tvalidation_0-rmse:23.94040\n[722]\tvalidation_0-rmse:23.94084\n[723]\tvalidation_0-rmse:23.94288\n[724]\tvalidation_0-rmse:23.94151\n[725]\tvalidation_0-rmse:23.94017\n[726]\tvalidation_0-rmse:23.93908\n[727]\tvalidation_0-rmse:23.93697\n[728]\tvalidation_0-rmse:23.93734\n[729]\tvalidation_0-rmse:23.93801\n[730]\tvalidation_0-rmse:23.93778\n[731]\tvalidation_0-rmse:23.93948\n[732]\tvalidation_0-rmse:23.93726\n[733]\tvalidation_0-rmse:23.93802\n[734]\tvalidation_0-rmse:23.93884\n[735]\tvalidation_0-rmse:23.94034\n[736]\tvalidation_0-rmse:23.94190\n[737]\tvalidation_0-rmse:23.93898\n[738]\tvalidation_0-rmse:23.93886\n[739]\tvalidation_0-rmse:23.93968\n[740]\tvalidation_0-rmse:23.93904\n[741]\tvalidation_0-rmse:23.93974\n[742]\tvalidation_0-rmse:23.94529\n[743]\tvalidation_0-rmse:23.94230\n[744]\tvalidation_0-rmse:23.94052\n[745]\tvalidation_0-rmse:23.94263\n[746]\tvalidation_0-rmse:23.94223\n[747]\tvalidation_0-rmse:23.94280\n[748]\tvalidation_0-rmse:23.94360\n[749]\tvalidation_0-rmse:23.94429\n[750]\tvalidation_0-rmse:23.94622\n[751]\tvalidation_0-rmse:23.94696\n[752]\tvalidation_0-rmse:23.94943\n[753]\tvalidation_0-rmse:23.95104\n[754]\tvalidation_0-rmse:23.95242\n[755]\tvalidation_0-rmse:23.95228\n[756]\tvalidation_0-rmse:23.95355\n[757]\tvalidation_0-rmse:23.95611\n[758]\tvalidation_0-rmse:23.95730\n[759]\tvalidation_0-rmse:23.95946\n[760]\tvalidation_0-rmse:23.95390\n[761]\tvalidation_0-rmse:23.95508\n[762]\tvalidation_0-rmse:23.95326\n[763]\tvalidation_0-rmse:23.95407\n[764]\tvalidation_0-rmse:23.95455\n[765]\tvalidation_0-rmse:23.95835\n[766]\tvalidation_0-rmse:23.95901\n[767]\tvalidation_0-rmse:23.95954\n[768]\tvalidation_0-rmse:23.96231\n[769]\tvalidation_0-rmse:23.96059\n[770]\tvalidation_0-rmse:23.96026\n[771]\tvalidation_0-rmse:23.95712\n[772]\tvalidation_0-rmse:23.95829\n[773]\tvalidation_0-rmse:23.95861\n[774]\tvalidation_0-rmse:23.95816\n[775]\tvalidation_0-rmse:23.95559\n[776]\tvalidation_0-rmse:23.95612\n[777]\tvalidation_0-rmse:23.95676\n[778]\tvalidation_0-rmse:23.95663\n[779]\tvalidation_0-rmse:23.95586\n[780]\tvalidation_0-rmse:23.95306\n[781]\tvalidation_0-rmse:23.95665\n[782]\tvalidation_0-rmse:23.95762\n[783]\tvalidation_0-rmse:23.95711\n[784]\tvalidation_0-rmse:23.95875\n[785]\tvalidation_0-rmse:23.95926\n[786]\tvalidation_0-rmse:23.96133\n[787]\tvalidation_0-rmse:23.96079\n[788]\tvalidation_0-rmse:23.96067\n[789]\tvalidation_0-rmse:23.96131\n[790]\tvalidation_0-rmse:23.96188\n[791]\tvalidation_0-rmse:23.96124\n[792]\tvalidation_0-rmse:23.96025\n[793]\tvalidation_0-rmse:23.95853\n[794]\tvalidation_0-rmse:23.95775\n[795]\tvalidation_0-rmse:23.95703\n[796]\tvalidation_0-rmse:23.95922\n[797]\tvalidation_0-rmse:23.95773\n[798]\tvalidation_0-rmse:23.95800\n[799]\tvalidation_0-rmse:23.95984\n[800]\tvalidation_0-rmse:23.96116\n[801]\tvalidation_0-rmse:23.96295\n[802]\tvalidation_0-rmse:23.96175\n[803]\tvalidation_0-rmse:23.96106\n[804]\tvalidation_0-rmse:23.96117\n[805]\tvalidation_0-rmse:23.96330\n[806]\tvalidation_0-rmse:23.96209\n[807]\tvalidation_0-rmse:23.96051\n[808]\tvalidation_0-rmse:23.95608\n[809]\tvalidation_0-rmse:23.95827\n[810]\tvalidation_0-rmse:23.95916\n[811]\tvalidation_0-rmse:23.95766\n[812]\tvalidation_0-rmse:23.95851\n[813]\tvalidation_0-rmse:23.95612\n[814]\tvalidation_0-rmse:23.95974\n[815]\tvalidation_0-rmse:23.95886\n[816]\tvalidation_0-rmse:23.95838\n[817]\tvalidation_0-rmse:23.95872\n[818]\tvalidation_0-rmse:23.95838\n[819]\tvalidation_0-rmse:23.95886\n[820]\tvalidation_0-rmse:23.95667\n[821]\tvalidation_0-rmse:23.95523\n[822]\tvalidation_0-rmse:23.95437\n[823]\tvalidation_0-rmse:23.95507\n[824]\tvalidation_0-rmse:23.95418\n[825]\tvalidation_0-rmse:23.95401\n[826]\tvalidation_0-rmse:23.95249\n[827]\tvalidation_0-rmse:23.95314\n[828]\tvalidation_0-rmse:23.94995\n[829]\tvalidation_0-rmse:23.94953\n[830]\tvalidation_0-rmse:23.95193\n[831]\tvalidation_0-rmse:23.95159\n[832]\tvalidation_0-rmse:23.95119\n[833]\tvalidation_0-rmse:23.95041\n[834]\tvalidation_0-rmse:23.95160\n[835]\tvalidation_0-rmse:23.95078\n[836]\tvalidation_0-rmse:23.94892\n[837]\tvalidation_0-rmse:23.94608\n[838]\tvalidation_0-rmse:23.94404\n[839]\tvalidation_0-rmse:23.94603\n[840]\tvalidation_0-rmse:23.94874\n[841]\tvalidation_0-rmse:23.94920\n[842]\tvalidation_0-rmse:23.94882\n[843]\tvalidation_0-rmse:23.94713\n[844]\tvalidation_0-rmse:23.94772\n[845]\tvalidation_0-rmse:23.95061\n[846]\tvalidation_0-rmse:23.94677\n[847]\tvalidation_0-rmse:23.94299\n[848]\tvalidation_0-rmse:23.94481\n[849]\tvalidation_0-rmse:23.94540\n[850]\tvalidation_0-rmse:23.94432\n[851]\tvalidation_0-rmse:23.94222\n[852]\tvalidation_0-rmse:23.93957\n[853]\tvalidation_0-rmse:23.94018\n[854]\tvalidation_0-rmse:23.94024\n[855]\tvalidation_0-rmse:23.93870\n[856]\tvalidation_0-rmse:23.93879\n[857]\tvalidation_0-rmse:23.93903\n[858]\tvalidation_0-rmse:23.94011\n[859]\tvalidation_0-rmse:23.94076\n[860]\tvalidation_0-rmse:23.93985\n[861]\tvalidation_0-rmse:23.94032\n[862]\tvalidation_0-rmse:23.94065\n[863]\tvalidation_0-rmse:23.94122\n[864]\tvalidation_0-rmse:23.94091\n[865]\tvalidation_0-rmse:23.94057\n[866]\tvalidation_0-rmse:23.93868\n[867]\tvalidation_0-rmse:23.93563\n[868]\tvalidation_0-rmse:23.93945\n[869]\tvalidation_0-rmse:23.93813\n[870]\tvalidation_0-rmse:23.93750\n[871]\tvalidation_0-rmse:23.93653\n[872]\tvalidation_0-rmse:23.93946\n[873]\tvalidation_0-rmse:23.93976\n[874]\tvalidation_0-rmse:23.93933\n[875]\tvalidation_0-rmse:23.94096\n[876]\tvalidation_0-rmse:23.94264\n[877]\tvalidation_0-rmse:23.94549\n[878]\tvalidation_0-rmse:23.94618\n[879]\tvalidation_0-rmse:23.94785\n[880]\tvalidation_0-rmse:23.95045\n[881]\tvalidation_0-rmse:23.95072\n[882]\tvalidation_0-rmse:23.94952\n[883]\tvalidation_0-rmse:23.94968\n[884]\tvalidation_0-rmse:23.95061\n[885]\tvalidation_0-rmse:23.94983\n[886]\tvalidation_0-rmse:23.95165\n[887]\tvalidation_0-rmse:23.95278\n[888]\tvalidation_0-rmse:23.95480\n[889]\tvalidation_0-rmse:23.95481\n[890]\tvalidation_0-rmse:23.95574\n[891]\tvalidation_0-rmse:23.95456\n[892]\tvalidation_0-rmse:23.95503\n[893]\tvalidation_0-rmse:23.95360\n[894]\tvalidation_0-rmse:23.95403\n[895]\tvalidation_0-rmse:23.95480\n[896]\tvalidation_0-rmse:23.95569\n[897]\tvalidation_0-rmse:23.95476\n[898]\tvalidation_0-rmse:23.95267\n[899]\tvalidation_0-rmse:23.95297\n[900]\tvalidation_0-rmse:23.95179\n[901]\tvalidation_0-rmse:23.95283\n[902]\tvalidation_0-rmse:23.95107\n[903]\tvalidation_0-rmse:23.95065\n[904]\tvalidation_0-rmse:23.95044\n[905]\tvalidation_0-rmse:23.95235\n[906]\tvalidation_0-rmse:23.95243\n[907]\tvalidation_0-rmse:23.95536\n[908]\tvalidation_0-rmse:23.95540\n[909]\tvalidation_0-rmse:23.95598\n[910]\tvalidation_0-rmse:23.95813\n[911]\tvalidation_0-rmse:23.96024\n[912]\tvalidation_0-rmse:23.95936\n[913]\tvalidation_0-rmse:23.96073\n[914]\tvalidation_0-rmse:23.96138\n[915]\tvalidation_0-rmse:23.96301\n[916]\tvalidation_0-rmse:23.96537\n[917]\tvalidation_0-rmse:23.96467\n[918]\tvalidation_0-rmse:23.96374\n[919]\tvalidation_0-rmse:23.96484\n[920]\tvalidation_0-rmse:23.96533\n[921]\tvalidation_0-rmse:23.96400\n[922]\tvalidation_0-rmse:23.96423\n[923]\tvalidation_0-rmse:23.96636\n[924]\tvalidation_0-rmse:23.96859\n[925]\tvalidation_0-rmse:23.96846\n[926]\tvalidation_0-rmse:23.96510\n[927]\tvalidation_0-rmse:23.96688\n[928]\tvalidation_0-rmse:23.96614\n[929]\tvalidation_0-rmse:23.96665\n[930]\tvalidation_0-rmse:23.96769\n[931]\tvalidation_0-rmse:23.96759\n[932]\tvalidation_0-rmse:23.96691\n[933]\tvalidation_0-rmse:23.96754\n[934]\tvalidation_0-rmse:23.96756\n[935]\tvalidation_0-rmse:23.96766\n[936]\tvalidation_0-rmse:23.96706\n[937]\tvalidation_0-rmse:23.96736\n[938]\tvalidation_0-rmse:23.96730\n[939]\tvalidation_0-rmse:23.96942\n[940]\tvalidation_0-rmse:23.97047\n[941]\tvalidation_0-rmse:23.97297\n[942]\tvalidation_0-rmse:23.97196\n[943]\tvalidation_0-rmse:23.97113\n[944]\tvalidation_0-rmse:23.97042\n[945]\tvalidation_0-rmse:23.97106\n[946]\tvalidation_0-rmse:23.97105\n[947]\tvalidation_0-rmse:23.96958\n[948]\tvalidation_0-rmse:23.96971\n[949]\tvalidation_0-rmse:23.96994\n[950]\tvalidation_0-rmse:23.96898\n[951]\tvalidation_0-rmse:23.96925\n[952]\tvalidation_0-rmse:23.96696\n[953]\tvalidation_0-rmse:23.96847\n[954]\tvalidation_0-rmse:23.96892\n[955]\tvalidation_0-rmse:23.96780\n[956]\tvalidation_0-rmse:23.96770\n[957]\tvalidation_0-rmse:23.96769\n[958]\tvalidation_0-rmse:23.96650\n[959]\tvalidation_0-rmse:23.96889\n[960]\tvalidation_0-rmse:23.96858\n[961]\tvalidation_0-rmse:23.96633\n[962]\tvalidation_0-rmse:23.96518\n[963]\tvalidation_0-rmse:23.96831\n[964]\tvalidation_0-rmse:23.96864\n[965]\tvalidation_0-rmse:23.96979\n[966]\tvalidation_0-rmse:23.97074\n[967]\tvalidation_0-rmse:23.97016\n[968]\tvalidation_0-rmse:23.96891\n[969]\tvalidation_0-rmse:23.96844\n[970]\tvalidation_0-rmse:23.96990\n[971]\tvalidation_0-rmse:23.97363\n[972]\tvalidation_0-rmse:23.97514\n[973]\tvalidation_0-rmse:23.97479\n[974]\tvalidation_0-rmse:23.97574\n[975]\tvalidation_0-rmse:23.97690\n[976]\tvalidation_0-rmse:23.97734\n[977]\tvalidation_0-rmse:23.97808\n[978]\tvalidation_0-rmse:23.97839\n[979]\tvalidation_0-rmse:23.97600\n[980]\tvalidation_0-rmse:23.97529\n[981]\tvalidation_0-rmse:23.97375\n[982]\tvalidation_0-rmse:23.97342\n[983]\tvalidation_0-rmse:23.97315\n[984]\tvalidation_0-rmse:23.97344\n[985]\tvalidation_0-rmse:23.97276\n[986]\tvalidation_0-rmse:23.97195\n[987]\tvalidation_0-rmse:23.97293\n[988]\tvalidation_0-rmse:23.97150\n[989]\tvalidation_0-rmse:23.97367\n[990]\tvalidation_0-rmse:23.97521\n[991]\tvalidation_0-rmse:23.97534\n[992]\tvalidation_0-rmse:23.97482\n[993]\tvalidation_0-rmse:23.97313\n[994]\tvalidation_0-rmse:23.97242\n[995]\tvalidation_0-rmse:23.97276\n[996]\tvalidation_0-rmse:23.97396\n[997]\tvalidation_0-rmse:23.97462\n[998]\tvalidation_0-rmse:23.97386\n[999]\tvalidation_0-rmse:23.97354\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011475 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3159\n[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 775\n[LightGBM] [Info] Start training from score 33.391242\n[0]\tvalidation_0-rmse:25.36588\n[1]\tvalidation_0-rmse:25.27215\n[2]\tvalidation_0-rmse:25.20189\n[3]\tvalidation_0-rmse:25.10842\n[4]\tvalidation_0-rmse:25.02574\n[5]\tvalidation_0-rmse:24.95866\n[6]\tvalidation_0-rmse:24.89287\n[7]\tvalidation_0-rmse:24.84346\n[8]\tvalidation_0-rmse:24.78927\n[9]\tvalidation_0-rmse:24.70777\n[10]\tvalidation_0-rmse:24.65677\n[11]\tvalidation_0-rmse:24.58435\n[12]\tvalidation_0-rmse:24.54717\n[13]\tvalidation_0-rmse:24.49949\n[14]\tvalidation_0-rmse:24.43797\n[15]\tvalidation_0-rmse:24.38214\n[16]\tvalidation_0-rmse:24.31338\n[17]\tvalidation_0-rmse:24.28555\n[18]\tvalidation_0-rmse:24.24575\n[19]\tvalidation_0-rmse:24.19879\n[20]\tvalidation_0-rmse:24.17561\n[21]\tvalidation_0-rmse:24.12601\n[22]\tvalidation_0-rmse:24.08066\n[23]\tvalidation_0-rmse:24.04976\n[24]\tvalidation_0-rmse:24.00581\n[25]\tvalidation_0-rmse:23.97532\n[26]\tvalidation_0-rmse:23.95985\n[27]\tvalidation_0-rmse:23.93293\n[28]\tvalidation_0-rmse:23.92181\n[29]\tvalidation_0-rmse:23.89178\n[30]\tvalidation_0-rmse:23.85751\n[31]\tvalidation_0-rmse:23.82854\n[32]\tvalidation_0-rmse:23.80302\n[33]\tvalidation_0-rmse:23.79558\n[34]\tvalidation_0-rmse:23.79016\n[35]\tvalidation_0-rmse:23.76080\n[36]\tvalidation_0-rmse:23.75454\n[37]\tvalidation_0-rmse:23.72126\n[38]\tvalidation_0-rmse:23.70871\n[39]\tvalidation_0-rmse:23.67978\n[40]\tvalidation_0-rmse:23.65345\n[41]\tvalidation_0-rmse:23.62944\n[42]\tvalidation_0-rmse:23.61833\n[43]\tvalidation_0-rmse:23.60452\n[44]\tvalidation_0-rmse:23.58677\n[45]\tvalidation_0-rmse:23.54573\n[46]\tvalidation_0-rmse:23.54057\n[47]\tvalidation_0-rmse:23.51958\n[48]\tvalidation_0-rmse:23.51457\n[49]\tvalidation_0-rmse:23.50951\n[50]\tvalidation_0-rmse:23.49379\n[51]\tvalidation_0-rmse:23.48684\n[52]\tvalidation_0-rmse:23.48732\n[53]\tvalidation_0-rmse:23.48158\n[54]\tvalidation_0-rmse:23.48142\n[55]\tvalidation_0-rmse:23.46224\n[56]\tvalidation_0-rmse:23.46621\n[57]\tvalidation_0-rmse:23.45004\n[58]\tvalidation_0-rmse:23.43810\n[59]\tvalidation_0-rmse:23.43376\n[60]\tvalidation_0-rmse:23.44358\n[61]\tvalidation_0-rmse:23.44184\n[62]\tvalidation_0-rmse:23.43086\n[63]\tvalidation_0-rmse:23.42019\n[64]\tvalidation_0-rmse:23.41077\n[65]\tvalidation_0-rmse:23.41285\n[66]\tvalidation_0-rmse:23.40473\n[67]\tvalidation_0-rmse:23.40268\n[68]\tvalidation_0-rmse:23.39873\n[69]\tvalidation_0-rmse:23.39847\n[70]\tvalidation_0-rmse:23.41189\n[71]\tvalidation_0-rmse:23.42234\n[72]\tvalidation_0-rmse:23.42307\n[73]\tvalidation_0-rmse:23.42505\n[74]\tvalidation_0-rmse:23.42534\n[75]\tvalidation_0-rmse:23.43291\n[76]\tvalidation_0-rmse:23.43457\n[77]\tvalidation_0-rmse:23.42413\n[78]\tvalidation_0-rmse:23.42180\n[79]\tvalidation_0-rmse:23.40495\n[80]\tvalidation_0-rmse:23.39910\n[81]\tvalidation_0-rmse:23.38705\n[82]\tvalidation_0-rmse:23.39181\n[83]\tvalidation_0-rmse:23.39298\n[84]\tvalidation_0-rmse:23.38687\n[85]\tvalidation_0-rmse:23.38563\n[86]\tvalidation_0-rmse:23.39560\n[87]\tvalidation_0-rmse:23.39379\n[88]\tvalidation_0-rmse:23.39046\n[89]\tvalidation_0-rmse:23.38229\n[90]\tvalidation_0-rmse:23.39144\n[91]\tvalidation_0-rmse:23.39088\n[92]\tvalidation_0-rmse:23.38500\n[93]\tvalidation_0-rmse:23.40305\n[94]\tvalidation_0-rmse:23.39667\n[95]\tvalidation_0-rmse:23.40199\n[96]\tvalidation_0-rmse:23.41206\n[97]\tvalidation_0-rmse:23.40925\n[98]\tvalidation_0-rmse:23.39912\n[99]\tvalidation_0-rmse:23.40070\n[100]\tvalidation_0-rmse:23.41108\n[101]\tvalidation_0-rmse:23.40736\n[102]\tvalidation_0-rmse:23.40291\n[103]\tvalidation_0-rmse:23.41674\n[104]\tvalidation_0-rmse:23.41727\n[105]\tvalidation_0-rmse:23.42630\n[106]\tvalidation_0-rmse:23.41459\n[107]\tvalidation_0-rmse:23.39261\n[108]\tvalidation_0-rmse:23.40779\n[109]\tvalidation_0-rmse:23.40986\n[110]\tvalidation_0-rmse:23.41893\n[111]\tvalidation_0-rmse:23.43619\n[112]\tvalidation_0-rmse:23.44817\n[113]\tvalidation_0-rmse:23.44341\n[114]\tvalidation_0-rmse:23.43913\n[115]\tvalidation_0-rmse:23.43955\n[116]\tvalidation_0-rmse:23.44017\n[117]\tvalidation_0-rmse:23.45164\n[118]\tvalidation_0-rmse:23.44998\n[119]\tvalidation_0-rmse:23.45178\n[120]\tvalidation_0-rmse:23.44461\n[121]\tvalidation_0-rmse:23.45059\n[122]\tvalidation_0-rmse:23.45704\n[123]\tvalidation_0-rmse:23.45911\n[124]\tvalidation_0-rmse:23.47221\n[125]\tvalidation_0-rmse:23.47582\n[126]\tvalidation_0-rmse:23.47488\n[127]\tvalidation_0-rmse:23.47892\n[128]\tvalidation_0-rmse:23.48480\n[129]\tvalidation_0-rmse:23.48867\n[130]\tvalidation_0-rmse:23.49019\n[131]\tvalidation_0-rmse:23.47795\n[132]\tvalidation_0-rmse:23.47490\n[133]\tvalidation_0-rmse:23.48142\n[134]\tvalidation_0-rmse:23.47421\n[135]\tvalidation_0-rmse:23.47632\n[136]\tvalidation_0-rmse:23.47428\n[137]\tvalidation_0-rmse:23.47525\n[138]\tvalidation_0-rmse:23.48483\n[139]\tvalidation_0-rmse:23.49085\n[140]\tvalidation_0-rmse:23.49843\n[141]\tvalidation_0-rmse:23.49628\n[142]\tvalidation_0-rmse:23.50016\n[143]\tvalidation_0-rmse:23.49401\n[144]\tvalidation_0-rmse:23.50704\n[145]\tvalidation_0-rmse:23.50687\n[146]\tvalidation_0-rmse:23.51074\n[147]\tvalidation_0-rmse:23.51083\n[148]\tvalidation_0-rmse:23.50387\n[149]\tvalidation_0-rmse:23.50851\n[150]\tvalidation_0-rmse:23.50899\n[151]\tvalidation_0-rmse:23.50409\n[152]\tvalidation_0-rmse:23.50742\n[153]\tvalidation_0-rmse:23.51957\n[154]\tvalidation_0-rmse:23.52127\n[155]\tvalidation_0-rmse:23.51799\n[156]\tvalidation_0-rmse:23.52318\n[157]\tvalidation_0-rmse:23.52834\n[158]\tvalidation_0-rmse:23.53034\n[159]\tvalidation_0-rmse:23.54443\n[160]\tvalidation_0-rmse:23.54977\n[161]\tvalidation_0-rmse:23.54747\n[162]\tvalidation_0-rmse:23.53857\n[163]\tvalidation_0-rmse:23.53720\n[164]\tvalidation_0-rmse:23.53712\n[165]\tvalidation_0-rmse:23.53967\n[166]\tvalidation_0-rmse:23.54535\n[167]\tvalidation_0-rmse:23.54994\n[168]\tvalidation_0-rmse:23.54749\n[169]\tvalidation_0-rmse:23.55536\n[170]\tvalidation_0-rmse:23.55724\n[171]\tvalidation_0-rmse:23.55781\n[172]\tvalidation_0-rmse:23.56412\n[173]\tvalidation_0-rmse:23.56902\n[174]\tvalidation_0-rmse:23.57023\n[175]\tvalidation_0-rmse:23.57885\n[176]\tvalidation_0-rmse:23.57253\n[177]\tvalidation_0-rmse:23.57813\n[178]\tvalidation_0-rmse:23.57630\n[179]\tvalidation_0-rmse:23.58136\n[180]\tvalidation_0-rmse:23.58510\n[181]\tvalidation_0-rmse:23.58430\n[182]\tvalidation_0-rmse:23.59086\n[183]\tvalidation_0-rmse:23.59445\n[184]\tvalidation_0-rmse:23.60088\n[185]\tvalidation_0-rmse:23.60129\n[186]\tvalidation_0-rmse:23.60233\n[187]\tvalidation_0-rmse:23.61071\n[188]\tvalidation_0-rmse:23.62325\n[189]\tvalidation_0-rmse:23.63130\n[190]\tvalidation_0-rmse:23.63460\n[191]\tvalidation_0-rmse:23.63722\n[192]\tvalidation_0-rmse:23.63310\n[193]\tvalidation_0-rmse:23.63885\n[194]\tvalidation_0-rmse:23.63772\n[195]\tvalidation_0-rmse:23.63817\n[196]\tvalidation_0-rmse:23.64211\n[197]\tvalidation_0-rmse:23.64442\n[198]\tvalidation_0-rmse:23.64558\n[199]\tvalidation_0-rmse:23.64449\n[200]\tvalidation_0-rmse:23.65252\n[201]\tvalidation_0-rmse:23.65710\n[202]\tvalidation_0-rmse:23.65486\n[203]\tvalidation_0-rmse:23.65512\n[204]\tvalidation_0-rmse:23.66350\n[205]\tvalidation_0-rmse:23.66338\n[206]\tvalidation_0-rmse:23.66752\n[207]\tvalidation_0-rmse:23.66168\n[208]\tvalidation_0-rmse:23.66164\n[209]\tvalidation_0-rmse:23.66624\n[210]\tvalidation_0-rmse:23.67225\n[211]\tvalidation_0-rmse:23.67902\n[212]\tvalidation_0-rmse:23.68232\n[213]\tvalidation_0-rmse:23.68036\n[214]\tvalidation_0-rmse:23.68012\n[215]\tvalidation_0-rmse:23.68416\n[216]\tvalidation_0-rmse:23.68418\n[217]\tvalidation_0-rmse:23.68888\n[218]\tvalidation_0-rmse:23.69694\n[219]\tvalidation_0-rmse:23.70295\n[220]\tvalidation_0-rmse:23.70276\n[221]\tvalidation_0-rmse:23.70353\n[222]\tvalidation_0-rmse:23.71068\n[223]\tvalidation_0-rmse:23.70682\n[224]\tvalidation_0-rmse:23.70700\n[225]\tvalidation_0-rmse:23.70126\n[226]\tvalidation_0-rmse:23.70943\n[227]\tvalidation_0-rmse:23.71845\n[228]\tvalidation_0-rmse:23.72111\n[229]\tvalidation_0-rmse:23.71972\n[230]\tvalidation_0-rmse:23.73160\n[231]\tvalidation_0-rmse:23.73707\n[232]\tvalidation_0-rmse:23.73550\n[233]\tvalidation_0-rmse:23.73035\n[234]\tvalidation_0-rmse:23.72542\n[235]\tvalidation_0-rmse:23.72630\n[236]\tvalidation_0-rmse:23.72908\n[237]\tvalidation_0-rmse:23.73901\n[238]\tvalidation_0-rmse:23.73789\n[239]\tvalidation_0-rmse:23.74203\n[240]\tvalidation_0-rmse:23.73998\n[241]\tvalidation_0-rmse:23.73815\n[242]\tvalidation_0-rmse:23.74674\n[243]\tvalidation_0-rmse:23.74250\n[244]\tvalidation_0-rmse:23.74125\n[245]\tvalidation_0-rmse:23.73845\n[246]\tvalidation_0-rmse:23.74301\n[247]\tvalidation_0-rmse:23.75172\n[248]\tvalidation_0-rmse:23.75424\n[249]\tvalidation_0-rmse:23.75782\n[250]\tvalidation_0-rmse:23.76763\n[251]\tvalidation_0-rmse:23.77015\n[252]\tvalidation_0-rmse:23.77969\n[253]\tvalidation_0-rmse:23.78327\n[254]\tvalidation_0-rmse:23.78579\n[255]\tvalidation_0-rmse:23.78506\n[256]\tvalidation_0-rmse:23.78373\n[257]\tvalidation_0-rmse:23.78665\n[258]\tvalidation_0-rmse:23.78481\n[259]\tvalidation_0-rmse:23.78459\n[260]\tvalidation_0-rmse:23.78625\n[261]\tvalidation_0-rmse:23.78230\n[262]\tvalidation_0-rmse:23.79156\n[263]\tvalidation_0-rmse:23.78936\n[264]\tvalidation_0-rmse:23.78924\n[265]\tvalidation_0-rmse:23.79960\n[266]\tvalidation_0-rmse:23.80098\n[267]\tvalidation_0-rmse:23.80547\n[268]\tvalidation_0-rmse:23.80759\n[269]\tvalidation_0-rmse:23.80389\n[270]\tvalidation_0-rmse:23.80719\n[271]\tvalidation_0-rmse:23.81214\n[272]\tvalidation_0-rmse:23.81835\n[273]\tvalidation_0-rmse:23.82353\n[274]\tvalidation_0-rmse:23.81804\n[275]\tvalidation_0-rmse:23.82634\n[276]\tvalidation_0-rmse:23.83102\n[277]\tvalidation_0-rmse:23.83352\n[278]\tvalidation_0-rmse:23.83234\n[279]\tvalidation_0-rmse:23.82883\n[280]\tvalidation_0-rmse:23.83275\n[281]\tvalidation_0-rmse:23.83526\n[282]\tvalidation_0-rmse:23.84007\n[283]\tvalidation_0-rmse:23.84067\n[284]\tvalidation_0-rmse:23.84189\n[285]\tvalidation_0-rmse:23.84563\n[286]\tvalidation_0-rmse:23.85510\n[287]\tvalidation_0-rmse:23.85809\n[288]\tvalidation_0-rmse:23.86488\n[289]\tvalidation_0-rmse:23.86957\n[290]\tvalidation_0-rmse:23.87015\n[291]\tvalidation_0-rmse:23.87276\n[292]\tvalidation_0-rmse:23.87327\n[293]\tvalidation_0-rmse:23.86864\n[294]\tvalidation_0-rmse:23.87136\n[295]\tvalidation_0-rmse:23.87813\n[296]\tvalidation_0-rmse:23.88068\n[297]\tvalidation_0-rmse:23.88212\n[298]\tvalidation_0-rmse:23.88288\n[299]\tvalidation_0-rmse:23.88857\n[300]\tvalidation_0-rmse:23.89281\n[301]\tvalidation_0-rmse:23.89136\n[302]\tvalidation_0-rmse:23.89393\n[303]\tvalidation_0-rmse:23.89775\n[304]\tvalidation_0-rmse:23.91036\n[305]\tvalidation_0-rmse:23.91514\n[306]\tvalidation_0-rmse:23.92177\n[307]\tvalidation_0-rmse:23.92929\n[308]\tvalidation_0-rmse:23.92859\n[309]\tvalidation_0-rmse:23.92666\n[310]\tvalidation_0-rmse:23.93147\n[311]\tvalidation_0-rmse:23.93215\n[312]\tvalidation_0-rmse:23.93231\n[313]\tvalidation_0-rmse:23.93563\n[314]\tvalidation_0-rmse:23.93962\n[315]\tvalidation_0-rmse:23.94416\n[316]\tvalidation_0-rmse:23.94502\n[317]\tvalidation_0-rmse:23.94907\n[318]\tvalidation_0-rmse:23.94451\n[319]\tvalidation_0-rmse:23.94347\n[320]\tvalidation_0-rmse:23.94512\n[321]\tvalidation_0-rmse:23.94961\n[322]\tvalidation_0-rmse:23.94822\n[323]\tvalidation_0-rmse:23.94411\n[324]\tvalidation_0-rmse:23.94072\n[325]\tvalidation_0-rmse:23.93782\n[326]\tvalidation_0-rmse:23.94244\n[327]\tvalidation_0-rmse:23.94495\n[328]\tvalidation_0-rmse:23.94703\n[329]\tvalidation_0-rmse:23.94562\n[330]\tvalidation_0-rmse:23.94464\n[331]\tvalidation_0-rmse:23.94404\n[332]\tvalidation_0-rmse:23.94608\n[333]\tvalidation_0-rmse:23.93981\n[334]\tvalidation_0-rmse:23.94240\n[335]\tvalidation_0-rmse:23.94572\n[336]\tvalidation_0-rmse:23.94239\n[337]\tvalidation_0-rmse:23.94186\n[338]\tvalidation_0-rmse:23.94158\n[339]\tvalidation_0-rmse:23.94633\n[340]\tvalidation_0-rmse:23.94307\n[341]\tvalidation_0-rmse:23.94237\n[342]\tvalidation_0-rmse:23.94511\n[343]\tvalidation_0-rmse:23.95380\n[344]\tvalidation_0-rmse:23.95247\n[345]\tvalidation_0-rmse:23.96097\n[346]\tvalidation_0-rmse:23.96533\n[347]\tvalidation_0-rmse:23.96682\n[348]\tvalidation_0-rmse:23.96872\n[349]\tvalidation_0-rmse:23.96889\n[350]\tvalidation_0-rmse:23.97293\n[351]\tvalidation_0-rmse:23.97250\n[352]\tvalidation_0-rmse:23.97687\n[353]\tvalidation_0-rmse:23.98480\n[354]\tvalidation_0-rmse:23.98506\n[355]\tvalidation_0-rmse:23.98465\n[356]\tvalidation_0-rmse:23.98547\n[357]\tvalidation_0-rmse:23.98091\n[358]\tvalidation_0-rmse:23.98363\n[359]\tvalidation_0-rmse:23.99469\n[360]\tvalidation_0-rmse:23.99726\n[361]\tvalidation_0-rmse:24.00252\n[362]\tvalidation_0-rmse:23.99749\n[363]\tvalidation_0-rmse:23.99818\n[364]\tvalidation_0-rmse:23.99860\n[365]\tvalidation_0-rmse:24.00229\n[366]\tvalidation_0-rmse:24.00664\n[367]\tvalidation_0-rmse:24.00446\n[368]\tvalidation_0-rmse:24.01292\n[369]\tvalidation_0-rmse:24.01577\n[370]\tvalidation_0-rmse:24.02213\n[371]\tvalidation_0-rmse:24.02673\n[372]\tvalidation_0-rmse:24.03302\n[373]\tvalidation_0-rmse:24.03729\n[374]\tvalidation_0-rmse:24.03553\n[375]\tvalidation_0-rmse:24.03496\n[376]\tvalidation_0-rmse:24.04185\n[377]\tvalidation_0-rmse:24.04647\n[378]\tvalidation_0-rmse:24.04805\n[379]\tvalidation_0-rmse:24.05034\n[380]\tvalidation_0-rmse:24.05170\n[381]\tvalidation_0-rmse:24.05739\n[382]\tvalidation_0-rmse:24.05503\n[383]\tvalidation_0-rmse:24.06599\n[384]\tvalidation_0-rmse:24.06931\n[385]\tvalidation_0-rmse:24.06420\n[386]\tvalidation_0-rmse:24.07047\n[387]\tvalidation_0-rmse:24.07375\n[388]\tvalidation_0-rmse:24.07420\n[389]\tvalidation_0-rmse:24.08062\n[390]\tvalidation_0-rmse:24.08495\n[391]\tvalidation_0-rmse:24.08469\n[392]\tvalidation_0-rmse:24.08213\n[393]\tvalidation_0-rmse:24.07912\n[394]\tvalidation_0-rmse:24.07902\n[395]\tvalidation_0-rmse:24.08229\n[396]\tvalidation_0-rmse:24.08621\n[397]\tvalidation_0-rmse:24.08822\n[398]\tvalidation_0-rmse:24.09385\n[399]\tvalidation_0-rmse:24.09956\n[400]\tvalidation_0-rmse:24.10709\n[401]\tvalidation_0-rmse:24.11540\n[402]\tvalidation_0-rmse:24.11749\n[403]\tvalidation_0-rmse:24.11750\n[404]\tvalidation_0-rmse:24.11857\n[405]\tvalidation_0-rmse:24.12472\n[406]\tvalidation_0-rmse:24.12689\n[407]\tvalidation_0-rmse:24.12819\n[408]\tvalidation_0-rmse:24.13065\n[409]\tvalidation_0-rmse:24.13865\n[410]\tvalidation_0-rmse:24.13349\n[411]\tvalidation_0-rmse:24.14217\n[412]\tvalidation_0-rmse:24.14167\n[413]\tvalidation_0-rmse:24.14242\n[414]\tvalidation_0-rmse:24.14739\n[415]\tvalidation_0-rmse:24.14835\n[416]\tvalidation_0-rmse:24.15522\n[417]\tvalidation_0-rmse:24.15458\n[418]\tvalidation_0-rmse:24.15997\n[419]\tvalidation_0-rmse:24.16431\n[420]\tvalidation_0-rmse:24.16534\n[421]\tvalidation_0-rmse:24.16597\n[422]\tvalidation_0-rmse:24.17358\n[423]\tvalidation_0-rmse:24.17992\n[424]\tvalidation_0-rmse:24.17293\n[425]\tvalidation_0-rmse:24.17501\n[426]\tvalidation_0-rmse:24.17789\n[427]\tvalidation_0-rmse:24.18387\n[428]\tvalidation_0-rmse:24.18450\n[429]\tvalidation_0-rmse:24.18812\n[430]\tvalidation_0-rmse:24.19697\n[431]\tvalidation_0-rmse:24.19834\n[432]\tvalidation_0-rmse:24.19997\n[433]\tvalidation_0-rmse:24.19732\n[434]\tvalidation_0-rmse:24.19868\n[435]\tvalidation_0-rmse:24.20114\n[436]\tvalidation_0-rmse:24.19657\n[437]\tvalidation_0-rmse:24.19579\n[438]\tvalidation_0-rmse:24.20173\n[439]\tvalidation_0-rmse:24.20822\n[440]\tvalidation_0-rmse:24.20404\n[441]\tvalidation_0-rmse:24.20876\n[442]\tvalidation_0-rmse:24.21060\n[443]\tvalidation_0-rmse:24.21115\n[444]\tvalidation_0-rmse:24.21138\n[445]\tvalidation_0-rmse:24.20953\n[446]\tvalidation_0-rmse:24.20856\n[447]\tvalidation_0-rmse:24.21588\n[448]\tvalidation_0-rmse:24.21657\n[449]\tvalidation_0-rmse:24.22483\n[450]\tvalidation_0-rmse:24.22752\n[451]\tvalidation_0-rmse:24.23204\n[452]\tvalidation_0-rmse:24.23766\n[453]\tvalidation_0-rmse:24.24068\n[454]\tvalidation_0-rmse:24.24757\n[455]\tvalidation_0-rmse:24.24678\n[456]\tvalidation_0-rmse:24.24811\n[457]\tvalidation_0-rmse:24.24863\n[458]\tvalidation_0-rmse:24.24949\n[459]\tvalidation_0-rmse:24.24971\n[460]\tvalidation_0-rmse:24.25235\n[461]\tvalidation_0-rmse:24.25809\n[462]\tvalidation_0-rmse:24.26726\n[463]\tvalidation_0-rmse:24.27018\n[464]\tvalidation_0-rmse:24.27442\n[465]\tvalidation_0-rmse:24.27880\n[466]\tvalidation_0-rmse:24.28268\n[467]\tvalidation_0-rmse:24.28595\n[468]\tvalidation_0-rmse:24.28431\n[469]\tvalidation_0-rmse:24.28306\n[470]\tvalidation_0-rmse:24.28803\n[471]\tvalidation_0-rmse:24.29273\n[472]\tvalidation_0-rmse:24.28931\n[473]\tvalidation_0-rmse:24.29154\n[474]\tvalidation_0-rmse:24.29265\n[475]\tvalidation_0-rmse:24.29434\n[476]\tvalidation_0-rmse:24.28928\n[477]\tvalidation_0-rmse:24.29101\n[478]\tvalidation_0-rmse:24.29367\n[479]\tvalidation_0-rmse:24.29935\n[480]\tvalidation_0-rmse:24.30233\n[481]\tvalidation_0-rmse:24.30538\n[482]\tvalidation_0-rmse:24.30768\n[483]\tvalidation_0-rmse:24.30967\n[484]\tvalidation_0-rmse:24.31234\n[485]\tvalidation_0-rmse:24.31189\n[486]\tvalidation_0-rmse:24.31711\n[487]\tvalidation_0-rmse:24.31403\n[488]\tvalidation_0-rmse:24.31681\n[489]\tvalidation_0-rmse:24.31706\n[490]\tvalidation_0-rmse:24.31769\n[491]\tvalidation_0-rmse:24.32241\n[492]\tvalidation_0-rmse:24.32765\n[493]\tvalidation_0-rmse:24.33208\n[494]\tvalidation_0-rmse:24.33258\n[495]\tvalidation_0-rmse:24.33234\n[496]\tvalidation_0-rmse:24.32878\n[497]\tvalidation_0-rmse:24.32555\n[498]\tvalidation_0-rmse:24.32880\n[499]\tvalidation_0-rmse:24.33323\n[500]\tvalidation_0-rmse:24.33202\n[501]\tvalidation_0-rmse:24.33339\n[502]\tvalidation_0-rmse:24.33927\n[503]\tvalidation_0-rmse:24.34107\n[504]\tvalidation_0-rmse:24.34236\n[505]\tvalidation_0-rmse:24.34074\n[506]\tvalidation_0-rmse:24.34191\n[507]\tvalidation_0-rmse:24.33927\n[508]\tvalidation_0-rmse:24.34243\n[509]\tvalidation_0-rmse:24.34344\n[510]\tvalidation_0-rmse:24.34639\n[511]\tvalidation_0-rmse:24.34920\n[512]\tvalidation_0-rmse:24.35248\n[513]\tvalidation_0-rmse:24.35970\n[514]\tvalidation_0-rmse:24.36466\n[515]\tvalidation_0-rmse:24.36586\n[516]\tvalidation_0-rmse:24.37258\n[517]\tvalidation_0-rmse:24.37335\n[518]\tvalidation_0-rmse:24.37558\n[519]\tvalidation_0-rmse:24.37705\n[520]\tvalidation_0-rmse:24.37825\n[521]\tvalidation_0-rmse:24.38437\n[522]\tvalidation_0-rmse:24.38376\n[523]\tvalidation_0-rmse:24.39124\n[524]\tvalidation_0-rmse:24.39803\n[525]\tvalidation_0-rmse:24.40066\n[526]\tvalidation_0-rmse:24.39941\n[527]\tvalidation_0-rmse:24.39904\n[528]\tvalidation_0-rmse:24.39956\n[529]\tvalidation_0-rmse:24.39573\n[530]\tvalidation_0-rmse:24.40037\n[531]\tvalidation_0-rmse:24.40206\n[532]\tvalidation_0-rmse:24.40496\n[533]\tvalidation_0-rmse:24.40649\n[534]\tvalidation_0-rmse:24.41179\n[535]\tvalidation_0-rmse:24.41559\n[536]\tvalidation_0-rmse:24.41840\n[537]\tvalidation_0-rmse:24.42085\n[538]\tvalidation_0-rmse:24.42243\n[539]\tvalidation_0-rmse:24.42300\n[540]\tvalidation_0-rmse:24.42488\n[541]\tvalidation_0-rmse:24.42288\n[542]\tvalidation_0-rmse:24.41954\n[543]\tvalidation_0-rmse:24.41945\n[544]\tvalidation_0-rmse:24.42049\n[545]\tvalidation_0-rmse:24.42296\n[546]\tvalidation_0-rmse:24.42647\n[547]\tvalidation_0-rmse:24.42896\n[548]\tvalidation_0-rmse:24.43122\n[549]\tvalidation_0-rmse:24.43182\n[550]\tvalidation_0-rmse:24.42834\n[551]\tvalidation_0-rmse:24.43059\n[552]\tvalidation_0-rmse:24.43211\n[553]\tvalidation_0-rmse:24.43156\n[554]\tvalidation_0-rmse:24.43259\n[555]\tvalidation_0-rmse:24.43111\n[556]\tvalidation_0-rmse:24.42973\n[557]\tvalidation_0-rmse:24.43147\n[558]\tvalidation_0-rmse:24.43296\n[559]\tvalidation_0-rmse:24.43522\n[560]\tvalidation_0-rmse:24.44020\n[561]\tvalidation_0-rmse:24.44263\n[562]\tvalidation_0-rmse:24.44138\n[563]\tvalidation_0-rmse:24.44415\n[564]\tvalidation_0-rmse:24.44648\n[565]\tvalidation_0-rmse:24.44885\n[566]\tvalidation_0-rmse:24.45190\n[567]\tvalidation_0-rmse:24.45283\n[568]\tvalidation_0-rmse:24.45286\n[569]\tvalidation_0-rmse:24.45415\n[570]\tvalidation_0-rmse:24.45703\n[571]\tvalidation_0-rmse:24.45692\n[572]\tvalidation_0-rmse:24.45858\n[573]\tvalidation_0-rmse:24.45839\n[574]\tvalidation_0-rmse:24.45817\n[575]\tvalidation_0-rmse:24.45591\n[576]\tvalidation_0-rmse:24.45520\n[577]\tvalidation_0-rmse:24.45252\n[578]\tvalidation_0-rmse:24.45859\n[579]\tvalidation_0-rmse:24.46392\n[580]\tvalidation_0-rmse:24.46513\n[581]\tvalidation_0-rmse:24.46625\n[582]\tvalidation_0-rmse:24.46873\n[583]\tvalidation_0-rmse:24.46477\n[584]\tvalidation_0-rmse:24.46456\n[585]\tvalidation_0-rmse:24.46555\n[586]\tvalidation_0-rmse:24.46546\n[587]\tvalidation_0-rmse:24.46706\n[588]\tvalidation_0-rmse:24.46606\n[589]\tvalidation_0-rmse:24.46860\n[590]\tvalidation_0-rmse:24.47225\n[591]\tvalidation_0-rmse:24.47347\n[592]\tvalidation_0-rmse:24.47413\n[593]\tvalidation_0-rmse:24.47509\n[594]\tvalidation_0-rmse:24.47568\n[595]\tvalidation_0-rmse:24.47545\n[596]\tvalidation_0-rmse:24.47837\n[597]\tvalidation_0-rmse:24.47531\n[598]\tvalidation_0-rmse:24.48060\n[599]\tvalidation_0-rmse:24.48508\n[600]\tvalidation_0-rmse:24.48657\n[601]\tvalidation_0-rmse:24.48471\n[602]\tvalidation_0-rmse:24.49125\n[603]\tvalidation_0-rmse:24.49343\n[604]\tvalidation_0-rmse:24.49361\n[605]\tvalidation_0-rmse:24.49216\n[606]\tvalidation_0-rmse:24.49530\n[607]\tvalidation_0-rmse:24.49927\n[608]\tvalidation_0-rmse:24.50225\n[609]\tvalidation_0-rmse:24.50351\n[610]\tvalidation_0-rmse:24.50278\n[611]\tvalidation_0-rmse:24.50585\n[612]\tvalidation_0-rmse:24.50674\n[613]\tvalidation_0-rmse:24.50776\n[614]\tvalidation_0-rmse:24.50610\n[615]\tvalidation_0-rmse:24.50513\n[616]\tvalidation_0-rmse:24.50574\n[617]\tvalidation_0-rmse:24.50843\n[618]\tvalidation_0-rmse:24.50966\n[619]\tvalidation_0-rmse:24.50959\n[620]\tvalidation_0-rmse:24.50961\n[621]\tvalidation_0-rmse:24.51009\n[622]\tvalidation_0-rmse:24.51046\n[623]\tvalidation_0-rmse:24.51138\n[624]\tvalidation_0-rmse:24.51419\n[625]\tvalidation_0-rmse:24.51372\n[626]\tvalidation_0-rmse:24.51420\n[627]\tvalidation_0-rmse:24.51605\n[628]\tvalidation_0-rmse:24.51707\n[629]\tvalidation_0-rmse:24.51990\n[630]\tvalidation_0-rmse:24.52001\n[631]\tvalidation_0-rmse:24.51796\n[632]\tvalidation_0-rmse:24.51676\n[633]\tvalidation_0-rmse:24.51906\n[634]\tvalidation_0-rmse:24.51983\n[635]\tvalidation_0-rmse:24.52372\n[636]\tvalidation_0-rmse:24.52549\n[637]\tvalidation_0-rmse:24.52746\n[638]\tvalidation_0-rmse:24.52877\n[639]\tvalidation_0-rmse:24.52758\n[640]\tvalidation_0-rmse:24.52821\n[641]\tvalidation_0-rmse:24.53109\n[642]\tvalidation_0-rmse:24.53456\n[643]\tvalidation_0-rmse:24.53177\n[644]\tvalidation_0-rmse:24.53091\n[645]\tvalidation_0-rmse:24.53021\n[646]\tvalidation_0-rmse:24.52453\n[647]\tvalidation_0-rmse:24.52308\n[648]\tvalidation_0-rmse:24.52109\n[649]\tvalidation_0-rmse:24.52607\n[650]\tvalidation_0-rmse:24.53021\n[651]\tvalidation_0-rmse:24.52910\n[652]\tvalidation_0-rmse:24.52997\n[653]\tvalidation_0-rmse:24.53252\n[654]\tvalidation_0-rmse:24.53532\n[655]\tvalidation_0-rmse:24.53599\n[656]\tvalidation_0-rmse:24.53890\n[657]\tvalidation_0-rmse:24.53859\n[658]\tvalidation_0-rmse:24.54008\n[659]\tvalidation_0-rmse:24.54183\n[660]\tvalidation_0-rmse:24.54753\n[661]\tvalidation_0-rmse:24.54901\n[662]\tvalidation_0-rmse:24.54914\n[663]\tvalidation_0-rmse:24.55007\n[664]\tvalidation_0-rmse:24.54930\n[665]\tvalidation_0-rmse:24.54411\n[666]\tvalidation_0-rmse:24.54365\n[667]\tvalidation_0-rmse:24.54431\n[668]\tvalidation_0-rmse:24.54375\n[669]\tvalidation_0-rmse:24.54340\n[670]\tvalidation_0-rmse:24.54450\n[671]\tvalidation_0-rmse:24.54135\n[672]\tvalidation_0-rmse:24.54090\n[673]\tvalidation_0-rmse:24.54139\n[674]\tvalidation_0-rmse:24.54333\n[675]\tvalidation_0-rmse:24.54366\n[676]\tvalidation_0-rmse:24.54362\n[677]\tvalidation_0-rmse:24.54438\n[678]\tvalidation_0-rmse:24.54724\n[679]\tvalidation_0-rmse:24.54962\n[680]\tvalidation_0-rmse:24.54916\n[681]\tvalidation_0-rmse:24.55067\n[682]\tvalidation_0-rmse:24.54963\n[683]\tvalidation_0-rmse:24.54800\n[684]\tvalidation_0-rmse:24.54878\n[685]\tvalidation_0-rmse:24.54492\n[686]\tvalidation_0-rmse:24.54471\n[687]\tvalidation_0-rmse:24.54527\n[688]\tvalidation_0-rmse:24.54567\n[689]\tvalidation_0-rmse:24.54979\n[690]\tvalidation_0-rmse:24.55395\n[691]\tvalidation_0-rmse:24.55413\n[692]\tvalidation_0-rmse:24.55344\n[693]\tvalidation_0-rmse:24.55439\n[694]\tvalidation_0-rmse:24.55637\n[695]\tvalidation_0-rmse:24.55895\n[696]\tvalidation_0-rmse:24.55674\n[697]\tvalidation_0-rmse:24.55923\n[698]\tvalidation_0-rmse:24.55965\n[699]\tvalidation_0-rmse:24.56002\n[700]\tvalidation_0-rmse:24.56036\n[701]\tvalidation_0-rmse:24.56142\n[702]\tvalidation_0-rmse:24.56038\n[703]\tvalidation_0-rmse:24.56102\n[704]\tvalidation_0-rmse:24.55943\n[705]\tvalidation_0-rmse:24.56021\n[706]\tvalidation_0-rmse:24.56038\n[707]\tvalidation_0-rmse:24.56223\n[708]\tvalidation_0-rmse:24.56525\n[709]\tvalidation_0-rmse:24.56772\n[710]\tvalidation_0-rmse:24.56711\n[711]\tvalidation_0-rmse:24.56902\n[712]\tvalidation_0-rmse:24.56314\n[713]\tvalidation_0-rmse:24.55952\n[714]\tvalidation_0-rmse:24.56018\n[715]\tvalidation_0-rmse:24.56203\n[716]\tvalidation_0-rmse:24.56368\n[717]\tvalidation_0-rmse:24.56542\n[718]\tvalidation_0-rmse:24.56641\n[719]\tvalidation_0-rmse:24.56516\n[720]\tvalidation_0-rmse:24.56563\n[721]\tvalidation_0-rmse:24.56535\n[722]\tvalidation_0-rmse:24.56595\n[723]\tvalidation_0-rmse:24.56639\n[724]\tvalidation_0-rmse:24.57115\n[725]\tvalidation_0-rmse:24.57159\n[726]\tvalidation_0-rmse:24.57149\n[727]\tvalidation_0-rmse:24.57474\n[728]\tvalidation_0-rmse:24.57646\n[729]\tvalidation_0-rmse:24.57729\n[730]\tvalidation_0-rmse:24.57661\n[731]\tvalidation_0-rmse:24.57781\n[732]\tvalidation_0-rmse:24.57988\n[733]\tvalidation_0-rmse:24.58446\n[734]\tvalidation_0-rmse:24.58647\n[735]\tvalidation_0-rmse:24.58757\n[736]\tvalidation_0-rmse:24.58524\n[737]\tvalidation_0-rmse:24.58558\n[738]\tvalidation_0-rmse:24.58321\n[739]\tvalidation_0-rmse:24.58522\n[740]\tvalidation_0-rmse:24.58717\n[741]\tvalidation_0-rmse:24.58809\n[742]\tvalidation_0-rmse:24.58628\n[743]\tvalidation_0-rmse:24.58631\n[744]\tvalidation_0-rmse:24.58840\n[745]\tvalidation_0-rmse:24.58571\n[746]\tvalidation_0-rmse:24.58176\n[747]\tvalidation_0-rmse:24.58473\n[748]\tvalidation_0-rmse:24.58744\n[749]\tvalidation_0-rmse:24.59192\n[750]\tvalidation_0-rmse:24.59300\n[751]\tvalidation_0-rmse:24.59323\n[752]\tvalidation_0-rmse:24.59405\n[753]\tvalidation_0-rmse:24.59637\n[754]\tvalidation_0-rmse:24.59972\n[755]\tvalidation_0-rmse:24.60355\n[756]\tvalidation_0-rmse:24.60740\n[757]\tvalidation_0-rmse:24.61002\n[758]\tvalidation_0-rmse:24.60683\n[759]\tvalidation_0-rmse:24.60652\n[760]\tvalidation_0-rmse:24.60821\n[761]\tvalidation_0-rmse:24.60900\n[762]\tvalidation_0-rmse:24.61342\n[763]\tvalidation_0-rmse:24.61382\n[764]\tvalidation_0-rmse:24.61541\n[765]\tvalidation_0-rmse:24.61565\n[766]\tvalidation_0-rmse:24.61544\n[767]\tvalidation_0-rmse:24.61577\n[768]\tvalidation_0-rmse:24.61802\n[769]\tvalidation_0-rmse:24.61922\n[770]\tvalidation_0-rmse:24.61952\n[771]\tvalidation_0-rmse:24.61968\n[772]\tvalidation_0-rmse:24.61935\n[773]\tvalidation_0-rmse:24.62259\n[774]\tvalidation_0-rmse:24.62139\n[775]\tvalidation_0-rmse:24.61984\n[776]\tvalidation_0-rmse:24.62008\n[777]\tvalidation_0-rmse:24.61983\n[778]\tvalidation_0-rmse:24.61929\n[779]\tvalidation_0-rmse:24.61603\n[780]\tvalidation_0-rmse:24.61753\n[781]\tvalidation_0-rmse:24.61758\n[782]\tvalidation_0-rmse:24.61363\n[783]\tvalidation_0-rmse:24.61751\n[784]\tvalidation_0-rmse:24.61859\n[785]\tvalidation_0-rmse:24.61757\n[786]\tvalidation_0-rmse:24.61680\n[787]\tvalidation_0-rmse:24.61933\n[788]\tvalidation_0-rmse:24.61875\n[789]\tvalidation_0-rmse:24.61640\n[790]\tvalidation_0-rmse:24.61841\n[791]\tvalidation_0-rmse:24.62061\n[792]\tvalidation_0-rmse:24.62301\n[793]\tvalidation_0-rmse:24.62364\n[794]\tvalidation_0-rmse:24.62400\n[795]\tvalidation_0-rmse:24.62146\n[796]\tvalidation_0-rmse:24.61877\n[797]\tvalidation_0-rmse:24.62028\n[798]\tvalidation_0-rmse:24.62052\n[799]\tvalidation_0-rmse:24.62239\n[800]\tvalidation_0-rmse:24.62577\n[801]\tvalidation_0-rmse:24.62838\n[802]\tvalidation_0-rmse:24.62855\n[803]\tvalidation_0-rmse:24.62691\n[804]\tvalidation_0-rmse:24.62736\n[805]\tvalidation_0-rmse:24.62668\n[806]\tvalidation_0-rmse:24.62627\n[807]\tvalidation_0-rmse:24.62788\n[808]\tvalidation_0-rmse:24.63058\n[809]\tvalidation_0-rmse:24.63076\n[810]\tvalidation_0-rmse:24.63079\n[811]\tvalidation_0-rmse:24.63265\n[812]\tvalidation_0-rmse:24.63374\n[813]\tvalidation_0-rmse:24.63243\n[814]\tvalidation_0-rmse:24.63315\n[815]\tvalidation_0-rmse:24.63513\n[816]\tvalidation_0-rmse:24.63814\n[817]\tvalidation_0-rmse:24.63724\n[818]\tvalidation_0-rmse:24.63655\n[819]\tvalidation_0-rmse:24.63606\n[820]\tvalidation_0-rmse:24.63733\n[821]\tvalidation_0-rmse:24.63677\n[822]\tvalidation_0-rmse:24.63850\n[823]\tvalidation_0-rmse:24.63890\n[824]\tvalidation_0-rmse:24.63913\n[825]\tvalidation_0-rmse:24.64110\n[826]\tvalidation_0-rmse:24.64107\n[827]\tvalidation_0-rmse:24.64115\n[828]\tvalidation_0-rmse:24.64456\n[829]\tvalidation_0-rmse:24.64546\n[830]\tvalidation_0-rmse:24.64546\n[831]\tvalidation_0-rmse:24.64771\n[832]\tvalidation_0-rmse:24.64976\n[833]\tvalidation_0-rmse:24.65070\n[834]\tvalidation_0-rmse:24.65041\n[835]\tvalidation_0-rmse:24.65175\n[836]\tvalidation_0-rmse:24.65222\n[837]\tvalidation_0-rmse:24.65365\n[838]\tvalidation_0-rmse:24.65524\n[839]\tvalidation_0-rmse:24.65826\n[840]\tvalidation_0-rmse:24.65928\n[841]\tvalidation_0-rmse:24.66083\n[842]\tvalidation_0-rmse:24.66107\n[843]\tvalidation_0-rmse:24.66347\n[844]\tvalidation_0-rmse:24.66537\n[845]\tvalidation_0-rmse:24.66446\n[846]\tvalidation_0-rmse:24.66337\n[847]\tvalidation_0-rmse:24.66304\n[848]\tvalidation_0-rmse:24.66303\n[849]\tvalidation_0-rmse:24.66591\n[850]\tvalidation_0-rmse:24.66866\n[851]\tvalidation_0-rmse:24.66872\n[852]\tvalidation_0-rmse:24.66883\n[853]\tvalidation_0-rmse:24.66949\n[854]\tvalidation_0-rmse:24.67151\n[855]\tvalidation_0-rmse:24.67251\n[856]\tvalidation_0-rmse:24.67237\n[857]\tvalidation_0-rmse:24.67503\n[858]\tvalidation_0-rmse:24.67367\n[859]\tvalidation_0-rmse:24.67230\n[860]\tvalidation_0-rmse:24.67254\n[861]\tvalidation_0-rmse:24.67575\n[862]\tvalidation_0-rmse:24.67797\n[863]\tvalidation_0-rmse:24.67976\n[864]\tvalidation_0-rmse:24.68083\n[865]\tvalidation_0-rmse:24.67896\n[866]\tvalidation_0-rmse:24.68007\n[867]\tvalidation_0-rmse:24.68015\n[868]\tvalidation_0-rmse:24.68200\n[869]\tvalidation_0-rmse:24.68364\n[870]\tvalidation_0-rmse:24.68048\n[871]\tvalidation_0-rmse:24.68022\n[872]\tvalidation_0-rmse:24.68273\n[873]\tvalidation_0-rmse:24.68301\n[874]\tvalidation_0-rmse:24.68525\n[875]\tvalidation_0-rmse:24.68839\n[876]\tvalidation_0-rmse:24.68969\n[877]\tvalidation_0-rmse:24.69279\n[878]\tvalidation_0-rmse:24.69443\n[879]\tvalidation_0-rmse:24.69462\n[880]\tvalidation_0-rmse:24.69597\n[881]\tvalidation_0-rmse:24.69613\n[882]\tvalidation_0-rmse:24.69499\n[883]\tvalidation_0-rmse:24.69471\n[884]\tvalidation_0-rmse:24.69721\n[885]\tvalidation_0-rmse:24.69996\n[886]\tvalidation_0-rmse:24.70095\n[887]\tvalidation_0-rmse:24.70263\n[888]\tvalidation_0-rmse:24.70439\n[889]\tvalidation_0-rmse:24.70309\n[890]\tvalidation_0-rmse:24.70485\n[891]\tvalidation_0-rmse:24.70561\n[892]\tvalidation_0-rmse:24.70548\n[893]\tvalidation_0-rmse:24.70547\n[894]\tvalidation_0-rmse:24.70316\n[895]\tvalidation_0-rmse:24.70423\n[896]\tvalidation_0-rmse:24.70525\n[897]\tvalidation_0-rmse:24.70410\n[898]\tvalidation_0-rmse:24.70542\n[899]\tvalidation_0-rmse:24.70525\n[900]\tvalidation_0-rmse:24.70579\n[901]\tvalidation_0-rmse:24.70450\n[902]\tvalidation_0-rmse:24.70620\n[903]\tvalidation_0-rmse:24.70650\n[904]\tvalidation_0-rmse:24.70288\n[905]\tvalidation_0-rmse:24.70425\n[906]\tvalidation_0-rmse:24.70379\n[907]\tvalidation_0-rmse:24.70161\n[908]\tvalidation_0-rmse:24.70027\n[909]\tvalidation_0-rmse:24.69982\n[910]\tvalidation_0-rmse:24.70012\n[911]\tvalidation_0-rmse:24.70157\n[912]\tvalidation_0-rmse:24.69911\n[913]\tvalidation_0-rmse:24.70083\n[914]\tvalidation_0-rmse:24.70038\n[915]\tvalidation_0-rmse:24.70131\n[916]\tvalidation_0-rmse:24.70023\n[917]\tvalidation_0-rmse:24.70048\n[918]\tvalidation_0-rmse:24.70106\n[919]\tvalidation_0-rmse:24.70104\n[920]\tvalidation_0-rmse:24.70253\n[921]\tvalidation_0-rmse:24.70344\n[922]\tvalidation_0-rmse:24.70363\n[923]\tvalidation_0-rmse:24.70514\n[924]\tvalidation_0-rmse:24.70449\n[925]\tvalidation_0-rmse:24.70300\n[926]\tvalidation_0-rmse:24.70032\n[927]\tvalidation_0-rmse:24.69910\n[928]\tvalidation_0-rmse:24.69724\n[929]\tvalidation_0-rmse:24.70021\n[930]\tvalidation_0-rmse:24.70087\n[931]\tvalidation_0-rmse:24.70066\n[932]\tvalidation_0-rmse:24.70124\n[933]\tvalidation_0-rmse:24.70166\n[934]\tvalidation_0-rmse:24.70295\n[935]\tvalidation_0-rmse:24.70170\n[936]\tvalidation_0-rmse:24.70227\n[937]\tvalidation_0-rmse:24.70287\n[938]\tvalidation_0-rmse:24.70477\n[939]\tvalidation_0-rmse:24.70574\n[940]\tvalidation_0-rmse:24.70614\n[941]\tvalidation_0-rmse:24.70730\n[942]\tvalidation_0-rmse:24.70721\n[943]\tvalidation_0-rmse:24.70837\n[944]\tvalidation_0-rmse:24.70818\n[945]\tvalidation_0-rmse:24.70897\n[946]\tvalidation_0-rmse:24.71126\n[947]\tvalidation_0-rmse:24.71270\n[948]\tvalidation_0-rmse:24.71133\n[949]\tvalidation_0-rmse:24.71263\n[950]\tvalidation_0-rmse:24.71136\n[951]\tvalidation_0-rmse:24.71277\n[952]\tvalidation_0-rmse:24.71195\n[953]\tvalidation_0-rmse:24.71244\n[954]\tvalidation_0-rmse:24.71241\n[955]\tvalidation_0-rmse:24.71302\n[956]\tvalidation_0-rmse:24.71294\n[957]\tvalidation_0-rmse:24.71571\n[958]\tvalidation_0-rmse:24.71646\n[959]\tvalidation_0-rmse:24.71824\n[960]\tvalidation_0-rmse:24.71928\n[961]\tvalidation_0-rmse:24.71867\n[962]\tvalidation_0-rmse:24.71987\n[963]\tvalidation_0-rmse:24.71990\n[964]\tvalidation_0-rmse:24.72075\n[965]\tvalidation_0-rmse:24.72033\n[966]\tvalidation_0-rmse:24.72014\n[967]\tvalidation_0-rmse:24.71961\n[968]\tvalidation_0-rmse:24.72125\n[969]\tvalidation_0-rmse:24.72082\n[970]\tvalidation_0-rmse:24.72071\n[971]\tvalidation_0-rmse:24.72142\n[972]\tvalidation_0-rmse:24.72634\n[973]\tvalidation_0-rmse:24.72736\n[974]\tvalidation_0-rmse:24.73032\n[975]\tvalidation_0-rmse:24.73218\n[976]\tvalidation_0-rmse:24.73227\n[977]\tvalidation_0-rmse:24.73237\n[978]\tvalidation_0-rmse:24.73367\n[979]\tvalidation_0-rmse:24.73228\n[980]\tvalidation_0-rmse:24.73167\n[981]\tvalidation_0-rmse:24.73150\n[982]\tvalidation_0-rmse:24.73223\n[983]\tvalidation_0-rmse:24.73140\n[984]\tvalidation_0-rmse:24.73177\n[985]\tvalidation_0-rmse:24.73306\n[986]\tvalidation_0-rmse:24.73238\n[987]\tvalidation_0-rmse:24.73268\n[988]\tvalidation_0-rmse:24.73052\n[989]\tvalidation_0-rmse:24.73117\n[990]\tvalidation_0-rmse:24.73212\n[991]\tvalidation_0-rmse:24.73175\n[992]\tvalidation_0-rmse:24.73256\n[993]\tvalidation_0-rmse:24.73221\n[994]\tvalidation_0-rmse:24.72986\n[995]\tvalidation_0-rmse:24.73158\n[996]\tvalidation_0-rmse:24.73132\n[997]\tvalidation_0-rmse:24.73128\n[998]\tvalidation_0-rmse:24.73363\n[999]\tvalidation_0-rmse:24.73213\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007017 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3148\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 767\n[LightGBM] [Info] Start training from score 33.637152\n[0]\tvalidation_0-rmse:27.09278\n[1]\tvalidation_0-rmse:27.02053\n[2]\tvalidation_0-rmse:26.93024\n[3]\tvalidation_0-rmse:26.87874\n[4]\tvalidation_0-rmse:26.78167\n[5]\tvalidation_0-rmse:26.72010\n[6]\tvalidation_0-rmse:26.66360\n[7]\tvalidation_0-rmse:26.62255\n[8]\tvalidation_0-rmse:26.56016\n[9]\tvalidation_0-rmse:26.50228\n[10]\tvalidation_0-rmse:26.44468\n[11]\tvalidation_0-rmse:26.40764\n[12]\tvalidation_0-rmse:26.37570\n[13]\tvalidation_0-rmse:26.33787\n[14]\tvalidation_0-rmse:26.29917\n[15]\tvalidation_0-rmse:26.26898\n[16]\tvalidation_0-rmse:26.24127\n[17]\tvalidation_0-rmse:26.18149\n[18]\tvalidation_0-rmse:26.15898\n[19]\tvalidation_0-rmse:26.11193\n[20]\tvalidation_0-rmse:26.09220\n[21]\tvalidation_0-rmse:26.04822\n[22]\tvalidation_0-rmse:25.99998\n[23]\tvalidation_0-rmse:25.98337\n[24]\tvalidation_0-rmse:25.96376\n[25]\tvalidation_0-rmse:25.93496\n[26]\tvalidation_0-rmse:25.91547\n[27]\tvalidation_0-rmse:25.89425\n[28]\tvalidation_0-rmse:25.86792\n[29]\tvalidation_0-rmse:25.85754\n[30]\tvalidation_0-rmse:25.83012\n[31]\tvalidation_0-rmse:25.81041\n[32]\tvalidation_0-rmse:25.79756\n[33]\tvalidation_0-rmse:25.76922\n[34]\tvalidation_0-rmse:25.77985\n[35]\tvalidation_0-rmse:25.77931\n[36]\tvalidation_0-rmse:25.78316\n[37]\tvalidation_0-rmse:25.77462\n[38]\tvalidation_0-rmse:25.78186\n[39]\tvalidation_0-rmse:25.75490\n[40]\tvalidation_0-rmse:25.73765\n[41]\tvalidation_0-rmse:25.70564\n[42]\tvalidation_0-rmse:25.68629\n[43]\tvalidation_0-rmse:25.67375\n[44]\tvalidation_0-rmse:25.65495\n[45]\tvalidation_0-rmse:25.64425\n[46]\tvalidation_0-rmse:25.64290\n[47]\tvalidation_0-rmse:25.62166\n[48]\tvalidation_0-rmse:25.60394\n[49]\tvalidation_0-rmse:25.60342\n[50]\tvalidation_0-rmse:25.59559\n[51]\tvalidation_0-rmse:25.59807\n[52]\tvalidation_0-rmse:25.59890\n[53]\tvalidation_0-rmse:25.58944\n[54]\tvalidation_0-rmse:25.57780\n[55]\tvalidation_0-rmse:25.57396\n[56]\tvalidation_0-rmse:25.55843\n[57]\tvalidation_0-rmse:25.57178\n[58]\tvalidation_0-rmse:25.54939\n[59]\tvalidation_0-rmse:25.54405\n[60]\tvalidation_0-rmse:25.51218\n[61]\tvalidation_0-rmse:25.51015\n[62]\tvalidation_0-rmse:25.50835\n[63]\tvalidation_0-rmse:25.52025\n[64]\tvalidation_0-rmse:25.50230\n[65]\tvalidation_0-rmse:25.49547\n[66]\tvalidation_0-rmse:25.51133\n[67]\tvalidation_0-rmse:25.52499\n[68]\tvalidation_0-rmse:25.50786\n[69]\tvalidation_0-rmse:25.50706\n[70]\tvalidation_0-rmse:25.51002\n[71]\tvalidation_0-rmse:25.50694\n[72]\tvalidation_0-rmse:25.49976\n[73]\tvalidation_0-rmse:25.51070\n[74]\tvalidation_0-rmse:25.51797\n[75]\tvalidation_0-rmse:25.52843\n[76]\tvalidation_0-rmse:25.52257\n[77]\tvalidation_0-rmse:25.51373\n[78]\tvalidation_0-rmse:25.50234\n[79]\tvalidation_0-rmse:25.51228\n[80]\tvalidation_0-rmse:25.51292\n[81]\tvalidation_0-rmse:25.49873\n[82]\tvalidation_0-rmse:25.50404\n[83]\tvalidation_0-rmse:25.51047\n[84]\tvalidation_0-rmse:25.51525\n[85]\tvalidation_0-rmse:25.51292\n[86]\tvalidation_0-rmse:25.51535\n[87]\tvalidation_0-rmse:25.50569\n[88]\tvalidation_0-rmse:25.49844\n[89]\tvalidation_0-rmse:25.50372\n[90]\tvalidation_0-rmse:25.50722\n[91]\tvalidation_0-rmse:25.50816\n[92]\tvalidation_0-rmse:25.51461\n[93]\tvalidation_0-rmse:25.50256\n[94]\tvalidation_0-rmse:25.51346\n[95]\tvalidation_0-rmse:25.51109\n[96]\tvalidation_0-rmse:25.51188\n[97]\tvalidation_0-rmse:25.51347\n[98]\tvalidation_0-rmse:25.51126\n[99]\tvalidation_0-rmse:25.49658\n[100]\tvalidation_0-rmse:25.49612\n[101]\tvalidation_0-rmse:25.49991\n[102]\tvalidation_0-rmse:25.48175\n[103]\tvalidation_0-rmse:25.48205\n[104]\tvalidation_0-rmse:25.49307\n[105]\tvalidation_0-rmse:25.48864\n[106]\tvalidation_0-rmse:25.49089\n[107]\tvalidation_0-rmse:25.48804\n[108]\tvalidation_0-rmse:25.46926\n[109]\tvalidation_0-rmse:25.47187\n[110]\tvalidation_0-rmse:25.45812\n[111]\tvalidation_0-rmse:25.46082\n[112]\tvalidation_0-rmse:25.45986\n[113]\tvalidation_0-rmse:25.46184\n[114]\tvalidation_0-rmse:25.46608\n[115]\tvalidation_0-rmse:25.47045\n[116]\tvalidation_0-rmse:25.47652\n[117]\tvalidation_0-rmse:25.47835\n[118]\tvalidation_0-rmse:25.48630\n[119]\tvalidation_0-rmse:25.48632\n[120]\tvalidation_0-rmse:25.48324\n[121]\tvalidation_0-rmse:25.48813\n[122]\tvalidation_0-rmse:25.47344\n[123]\tvalidation_0-rmse:25.47773\n[124]\tvalidation_0-rmse:25.46824\n[125]\tvalidation_0-rmse:25.46596\n[126]\tvalidation_0-rmse:25.46792\n[127]\tvalidation_0-rmse:25.47405\n[128]\tvalidation_0-rmse:25.47954\n[129]\tvalidation_0-rmse:25.48310\n[130]\tvalidation_0-rmse:25.48213\n[131]\tvalidation_0-rmse:25.48762\n[132]\tvalidation_0-rmse:25.48406\n[133]\tvalidation_0-rmse:25.48819\n[134]\tvalidation_0-rmse:25.49021\n[135]\tvalidation_0-rmse:25.49239\n[136]\tvalidation_0-rmse:25.49618\n[137]\tvalidation_0-rmse:25.50441\n[138]\tvalidation_0-rmse:25.49878\n[139]\tvalidation_0-rmse:25.50956\n[140]\tvalidation_0-rmse:25.50195\n[141]\tvalidation_0-rmse:25.50916\n[142]\tvalidation_0-rmse:25.50609\n[143]\tvalidation_0-rmse:25.51586\n[144]\tvalidation_0-rmse:25.51658\n[145]\tvalidation_0-rmse:25.52892\n[146]\tvalidation_0-rmse:25.54267\n[147]\tvalidation_0-rmse:25.54248\n[148]\tvalidation_0-rmse:25.54338\n[149]\tvalidation_0-rmse:25.54647\n[150]\tvalidation_0-rmse:25.55165\n[151]\tvalidation_0-rmse:25.55067\n[152]\tvalidation_0-rmse:25.55572\n[153]\tvalidation_0-rmse:25.55185\n[154]\tvalidation_0-rmse:25.54926\n[155]\tvalidation_0-rmse:25.55663\n[156]\tvalidation_0-rmse:25.56877\n[157]\tvalidation_0-rmse:25.56079\n[158]\tvalidation_0-rmse:25.55792\n[159]\tvalidation_0-rmse:25.56390\n[160]\tvalidation_0-rmse:25.56332\n[161]\tvalidation_0-rmse:25.56329\n[162]\tvalidation_0-rmse:25.57857\n[163]\tvalidation_0-rmse:25.58338\n[164]\tvalidation_0-rmse:25.59135\n[165]\tvalidation_0-rmse:25.59959\n[166]\tvalidation_0-rmse:25.60251\n[167]\tvalidation_0-rmse:25.60421\n[168]\tvalidation_0-rmse:25.60819\n[169]\tvalidation_0-rmse:25.60815\n[170]\tvalidation_0-rmse:25.61198\n[171]\tvalidation_0-rmse:25.60440\n[172]\tvalidation_0-rmse:25.61180\n[173]\tvalidation_0-rmse:25.61244\n[174]\tvalidation_0-rmse:25.61315\n[175]\tvalidation_0-rmse:25.61333\n[176]\tvalidation_0-rmse:25.61549\n[177]\tvalidation_0-rmse:25.62121\n[178]\tvalidation_0-rmse:25.61438\n[179]\tvalidation_0-rmse:25.62072\n[180]\tvalidation_0-rmse:25.61223\n[181]\tvalidation_0-rmse:25.61260\n[182]\tvalidation_0-rmse:25.62728\n[183]\tvalidation_0-rmse:25.63144\n[184]\tvalidation_0-rmse:25.62247\n[185]\tvalidation_0-rmse:25.61753\n[186]\tvalidation_0-rmse:25.61831\n[187]\tvalidation_0-rmse:25.61989\n[188]\tvalidation_0-rmse:25.62783\n[189]\tvalidation_0-rmse:25.63522\n[190]\tvalidation_0-rmse:25.63892\n[191]\tvalidation_0-rmse:25.63782\n[192]\tvalidation_0-rmse:25.64625\n[193]\tvalidation_0-rmse:25.64839\n[194]\tvalidation_0-rmse:25.65487\n[195]\tvalidation_0-rmse:25.65771\n[196]\tvalidation_0-rmse:25.65448\n[197]\tvalidation_0-rmse:25.65270\n[198]\tvalidation_0-rmse:25.65515\n[199]\tvalidation_0-rmse:25.66489\n[200]\tvalidation_0-rmse:25.67026\n[201]\tvalidation_0-rmse:25.67472\n[202]\tvalidation_0-rmse:25.68280\n[203]\tvalidation_0-rmse:25.66980\n[204]\tvalidation_0-rmse:25.65988\n[205]\tvalidation_0-rmse:25.66041\n[206]\tvalidation_0-rmse:25.66018\n[207]\tvalidation_0-rmse:25.66374\n[208]\tvalidation_0-rmse:25.65663\n[209]\tvalidation_0-rmse:25.65740\n[210]\tvalidation_0-rmse:25.66106\n[211]\tvalidation_0-rmse:25.65584\n[212]\tvalidation_0-rmse:25.66171\n[213]\tvalidation_0-rmse:25.66499\n[214]\tvalidation_0-rmse:25.67230\n[215]\tvalidation_0-rmse:25.67541\n[216]\tvalidation_0-rmse:25.67565\n[217]\tvalidation_0-rmse:25.67505\n[218]\tvalidation_0-rmse:25.67762\n[219]\tvalidation_0-rmse:25.68349\n[220]\tvalidation_0-rmse:25.68258\n[221]\tvalidation_0-rmse:25.67631\n[222]\tvalidation_0-rmse:25.68404\n[223]\tvalidation_0-rmse:25.67963\n[224]\tvalidation_0-rmse:25.67488\n[225]\tvalidation_0-rmse:25.67723\n[226]\tvalidation_0-rmse:25.68766\n[227]\tvalidation_0-rmse:25.69067\n[228]\tvalidation_0-rmse:25.69266\n[229]\tvalidation_0-rmse:25.70311\n[230]\tvalidation_0-rmse:25.70739\n[231]\tvalidation_0-rmse:25.71406\n[232]\tvalidation_0-rmse:25.71778\n[233]\tvalidation_0-rmse:25.71492\n[234]\tvalidation_0-rmse:25.71578\n[235]\tvalidation_0-rmse:25.72077\n[236]\tvalidation_0-rmse:25.72084\n[237]\tvalidation_0-rmse:25.72309\n[238]\tvalidation_0-rmse:25.72519\n[239]\tvalidation_0-rmse:25.73182\n[240]\tvalidation_0-rmse:25.73187\n[241]\tvalidation_0-rmse:25.72979\n[242]\tvalidation_0-rmse:25.72831\n[243]\tvalidation_0-rmse:25.72427\n[244]\tvalidation_0-rmse:25.73093\n[245]\tvalidation_0-rmse:25.74380\n[246]\tvalidation_0-rmse:25.74644\n[247]\tvalidation_0-rmse:25.75340\n[248]\tvalidation_0-rmse:25.75189\n[249]\tvalidation_0-rmse:25.75634\n[250]\tvalidation_0-rmse:25.75584\n[251]\tvalidation_0-rmse:25.75744\n[252]\tvalidation_0-rmse:25.75335\n[253]\tvalidation_0-rmse:25.75314\n[254]\tvalidation_0-rmse:25.75037\n[255]\tvalidation_0-rmse:25.76331\n[256]\tvalidation_0-rmse:25.76142\n[257]\tvalidation_0-rmse:25.76373\n[258]\tvalidation_0-rmse:25.76753\n[259]\tvalidation_0-rmse:25.76808\n[260]\tvalidation_0-rmse:25.77314\n[261]\tvalidation_0-rmse:25.77443\n[262]\tvalidation_0-rmse:25.77240\n[263]\tvalidation_0-rmse:25.77441\n[264]\tvalidation_0-rmse:25.77270\n[265]\tvalidation_0-rmse:25.77083\n[266]\tvalidation_0-rmse:25.76703\n[267]\tvalidation_0-rmse:25.76195\n[268]\tvalidation_0-rmse:25.76661\n[269]\tvalidation_0-rmse:25.76472\n[270]\tvalidation_0-rmse:25.76342\n[271]\tvalidation_0-rmse:25.76530\n[272]\tvalidation_0-rmse:25.77280\n[273]\tvalidation_0-rmse:25.77294\n[274]\tvalidation_0-rmse:25.76301\n[275]\tvalidation_0-rmse:25.76066\n[276]\tvalidation_0-rmse:25.76490\n[277]\tvalidation_0-rmse:25.76566\n[278]\tvalidation_0-rmse:25.76518\n[279]\tvalidation_0-rmse:25.76212\n[280]\tvalidation_0-rmse:25.76019\n[281]\tvalidation_0-rmse:25.76782\n[282]\tvalidation_0-rmse:25.76858\n[283]\tvalidation_0-rmse:25.77296\n[284]\tvalidation_0-rmse:25.77496\n[285]\tvalidation_0-rmse:25.77578\n[286]\tvalidation_0-rmse:25.77684\n[287]\tvalidation_0-rmse:25.77664\n[288]\tvalidation_0-rmse:25.78276\n[289]\tvalidation_0-rmse:25.79430\n[290]\tvalidation_0-rmse:25.79596\n[291]\tvalidation_0-rmse:25.79699\n[292]\tvalidation_0-rmse:25.79635\n[293]\tvalidation_0-rmse:25.79953\n[294]\tvalidation_0-rmse:25.80473\n[295]\tvalidation_0-rmse:25.80150\n[296]\tvalidation_0-rmse:25.80124\n[297]\tvalidation_0-rmse:25.80090\n[298]\tvalidation_0-rmse:25.80281\n[299]\tvalidation_0-rmse:25.80121\n[300]\tvalidation_0-rmse:25.79830\n[301]\tvalidation_0-rmse:25.80098\n[302]\tvalidation_0-rmse:25.80317\n[303]\tvalidation_0-rmse:25.80327\n[304]\tvalidation_0-rmse:25.80929\n[305]\tvalidation_0-rmse:25.81286\n[306]\tvalidation_0-rmse:25.81158\n[307]\tvalidation_0-rmse:25.80548\n[308]\tvalidation_0-rmse:25.79808\n[309]\tvalidation_0-rmse:25.79902\n[310]\tvalidation_0-rmse:25.79977\n[311]\tvalidation_0-rmse:25.80327\n[312]\tvalidation_0-rmse:25.80597\n[313]\tvalidation_0-rmse:25.80404\n[314]\tvalidation_0-rmse:25.80468\n[315]\tvalidation_0-rmse:25.80611\n[316]\tvalidation_0-rmse:25.80863\n[317]\tvalidation_0-rmse:25.81086\n[318]\tvalidation_0-rmse:25.81651\n[319]\tvalidation_0-rmse:25.81952\n[320]\tvalidation_0-rmse:25.82152\n[321]\tvalidation_0-rmse:25.82044\n[322]\tvalidation_0-rmse:25.82396\n[323]\tvalidation_0-rmse:25.82539\n[324]\tvalidation_0-rmse:25.82782\n[325]\tvalidation_0-rmse:25.82507\n[326]\tvalidation_0-rmse:25.82271\n[327]\tvalidation_0-rmse:25.82171\n[328]\tvalidation_0-rmse:25.81689\n[329]\tvalidation_0-rmse:25.81906\n[330]\tvalidation_0-rmse:25.81906\n[331]\tvalidation_0-rmse:25.81966\n[332]\tvalidation_0-rmse:25.82770\n[333]\tvalidation_0-rmse:25.83328\n[334]\tvalidation_0-rmse:25.83640\n[335]\tvalidation_0-rmse:25.84046\n[336]\tvalidation_0-rmse:25.84097\n[337]\tvalidation_0-rmse:25.83856\n[338]\tvalidation_0-rmse:25.83723\n[339]\tvalidation_0-rmse:25.83948\n[340]\tvalidation_0-rmse:25.84060\n[341]\tvalidation_0-rmse:25.84394\n[342]\tvalidation_0-rmse:25.83995\n[343]\tvalidation_0-rmse:25.84744\n[344]\tvalidation_0-rmse:25.84432\n[345]\tvalidation_0-rmse:25.84223\n[346]\tvalidation_0-rmse:25.84358\n[347]\tvalidation_0-rmse:25.84191\n[348]\tvalidation_0-rmse:25.84046\n[349]\tvalidation_0-rmse:25.83364\n[350]\tvalidation_0-rmse:25.83577\n[351]\tvalidation_0-rmse:25.83290\n[352]\tvalidation_0-rmse:25.83389\n[353]\tvalidation_0-rmse:25.83307\n[354]\tvalidation_0-rmse:25.83623\n[355]\tvalidation_0-rmse:25.83162\n[356]\tvalidation_0-rmse:25.82767\n[357]\tvalidation_0-rmse:25.83106\n[358]\tvalidation_0-rmse:25.83843\n[359]\tvalidation_0-rmse:25.83505\n[360]\tvalidation_0-rmse:25.83226\n[361]\tvalidation_0-rmse:25.83294\n[362]\tvalidation_0-rmse:25.83133\n[363]\tvalidation_0-rmse:25.82955\n[364]\tvalidation_0-rmse:25.83689\n[365]\tvalidation_0-rmse:25.83905\n[366]\tvalidation_0-rmse:25.84080\n[367]\tvalidation_0-rmse:25.83815\n[368]\tvalidation_0-rmse:25.83749\n[369]\tvalidation_0-rmse:25.83883\n[370]\tvalidation_0-rmse:25.83525\n[371]\tvalidation_0-rmse:25.83561\n[372]\tvalidation_0-rmse:25.83624\n[373]\tvalidation_0-rmse:25.83447\n[374]\tvalidation_0-rmse:25.83612\n[375]\tvalidation_0-rmse:25.83647\n[376]\tvalidation_0-rmse:25.83647\n[377]\tvalidation_0-rmse:25.83556\n[378]\tvalidation_0-rmse:25.84732\n[379]\tvalidation_0-rmse:25.85405\n[380]\tvalidation_0-rmse:25.85500\n[381]\tvalidation_0-rmse:25.86021\n[382]\tvalidation_0-rmse:25.85820\n[383]\tvalidation_0-rmse:25.85616\n[384]\tvalidation_0-rmse:25.85872\n[385]\tvalidation_0-rmse:25.85990\n[386]\tvalidation_0-rmse:25.86422\n[387]\tvalidation_0-rmse:25.86431\n[388]\tvalidation_0-rmse:25.86917\n[389]\tvalidation_0-rmse:25.87210\n[390]\tvalidation_0-rmse:25.87603\n[391]\tvalidation_0-rmse:25.87310\n[392]\tvalidation_0-rmse:25.87230\n[393]\tvalidation_0-rmse:25.87603\n[394]\tvalidation_0-rmse:25.88204\n[395]\tvalidation_0-rmse:25.88168\n[396]\tvalidation_0-rmse:25.88434\n[397]\tvalidation_0-rmse:25.88654\n[398]\tvalidation_0-rmse:25.88713\n[399]\tvalidation_0-rmse:25.88887\n[400]\tvalidation_0-rmse:25.89298\n[401]\tvalidation_0-rmse:25.89880\n[402]\tvalidation_0-rmse:25.90195\n[403]\tvalidation_0-rmse:25.89677\n[404]\tvalidation_0-rmse:25.89722\n[405]\tvalidation_0-rmse:25.89784\n[406]\tvalidation_0-rmse:25.90153\n[407]\tvalidation_0-rmse:25.90658\n[408]\tvalidation_0-rmse:25.90295\n[409]\tvalidation_0-rmse:25.90404\n[410]\tvalidation_0-rmse:25.90815\n[411]\tvalidation_0-rmse:25.91284\n[412]\tvalidation_0-rmse:25.91109\n[413]\tvalidation_0-rmse:25.91306\n[414]\tvalidation_0-rmse:25.91330\n[415]\tvalidation_0-rmse:25.91517\n[416]\tvalidation_0-rmse:25.91310\n[417]\tvalidation_0-rmse:25.91280\n[418]\tvalidation_0-rmse:25.91032\n[419]\tvalidation_0-rmse:25.91100\n[420]\tvalidation_0-rmse:25.91076\n[421]\tvalidation_0-rmse:25.91098\n[422]\tvalidation_0-rmse:25.91343\n[423]\tvalidation_0-rmse:25.91518\n[424]\tvalidation_0-rmse:25.91311\n[425]\tvalidation_0-rmse:25.91088\n[426]\tvalidation_0-rmse:25.91602\n[427]\tvalidation_0-rmse:25.91735\n[428]\tvalidation_0-rmse:25.92134\n[429]\tvalidation_0-rmse:25.91899\n[430]\tvalidation_0-rmse:25.92436\n[431]\tvalidation_0-rmse:25.92215\n[432]\tvalidation_0-rmse:25.92655\n[433]\tvalidation_0-rmse:25.92860\n[434]\tvalidation_0-rmse:25.93424\n[435]\tvalidation_0-rmse:25.93749\n[436]\tvalidation_0-rmse:25.93273\n[437]\tvalidation_0-rmse:25.93511\n[438]\tvalidation_0-rmse:25.93363\n[439]\tvalidation_0-rmse:25.93902\n[440]\tvalidation_0-rmse:25.93845\n[441]\tvalidation_0-rmse:25.93855\n[442]\tvalidation_0-rmse:25.93850\n[443]\tvalidation_0-rmse:25.93800\n[444]\tvalidation_0-rmse:25.93807\n[445]\tvalidation_0-rmse:25.93844\n[446]\tvalidation_0-rmse:25.94138\n[447]\tvalidation_0-rmse:25.94307\n[448]\tvalidation_0-rmse:25.94078\n[449]\tvalidation_0-rmse:25.93184\n[450]\tvalidation_0-rmse:25.92860\n[451]\tvalidation_0-rmse:25.93128\n[452]\tvalidation_0-rmse:25.93603\n[453]\tvalidation_0-rmse:25.93713\n[454]\tvalidation_0-rmse:25.93596\n[455]\tvalidation_0-rmse:25.93848\n[456]\tvalidation_0-rmse:25.93658\n[457]\tvalidation_0-rmse:25.93610\n[458]\tvalidation_0-rmse:25.93165\n[459]\tvalidation_0-rmse:25.93441\n[460]\tvalidation_0-rmse:25.93878\n[461]\tvalidation_0-rmse:25.94185\n[462]\tvalidation_0-rmse:25.94609\n[463]\tvalidation_0-rmse:25.94520\n[464]\tvalidation_0-rmse:25.95039\n[465]\tvalidation_0-rmse:25.95095\n[466]\tvalidation_0-rmse:25.95755\n[467]\tvalidation_0-rmse:25.95647\n[468]\tvalidation_0-rmse:25.95457\n[469]\tvalidation_0-rmse:25.95920\n[470]\tvalidation_0-rmse:25.96343\n[471]\tvalidation_0-rmse:25.96232\n[472]\tvalidation_0-rmse:25.96310\n[473]\tvalidation_0-rmse:25.96242\n[474]\tvalidation_0-rmse:25.96673\n[475]\tvalidation_0-rmse:25.96350\n[476]\tvalidation_0-rmse:25.96358\n[477]\tvalidation_0-rmse:25.96575\n[478]\tvalidation_0-rmse:25.96855\n[479]\tvalidation_0-rmse:25.96677\n[480]\tvalidation_0-rmse:25.96682\n[481]\tvalidation_0-rmse:25.96295\n[482]\tvalidation_0-rmse:25.96585\n[483]\tvalidation_0-rmse:25.96228\n[484]\tvalidation_0-rmse:25.96662\n[485]\tvalidation_0-rmse:25.96570\n[486]\tvalidation_0-rmse:25.96891\n[487]\tvalidation_0-rmse:25.96278\n[488]\tvalidation_0-rmse:25.96725\n[489]\tvalidation_0-rmse:25.96885\n[490]\tvalidation_0-rmse:25.97064\n[491]\tvalidation_0-rmse:25.97497\n[492]\tvalidation_0-rmse:25.97138\n[493]\tvalidation_0-rmse:25.97105\n[494]\tvalidation_0-rmse:25.97105\n[495]\tvalidation_0-rmse:25.97167\n[496]\tvalidation_0-rmse:25.97185\n[497]\tvalidation_0-rmse:25.97154\n[498]\tvalidation_0-rmse:25.97177\n[499]\tvalidation_0-rmse:25.96672\n[500]\tvalidation_0-rmse:25.96468\n[501]\tvalidation_0-rmse:25.95959\n[502]\tvalidation_0-rmse:25.95470\n[503]\tvalidation_0-rmse:25.95236\n[504]\tvalidation_0-rmse:25.95303\n[505]\tvalidation_0-rmse:25.95340\n[506]\tvalidation_0-rmse:25.95613\n[507]\tvalidation_0-rmse:25.95174\n[508]\tvalidation_0-rmse:25.95418\n[509]\tvalidation_0-rmse:25.95775\n[510]\tvalidation_0-rmse:25.95481\n[511]\tvalidation_0-rmse:25.95314\n[512]\tvalidation_0-rmse:25.95570\n[513]\tvalidation_0-rmse:25.95661\n[514]\tvalidation_0-rmse:25.95631\n[515]\tvalidation_0-rmse:25.95678\n[516]\tvalidation_0-rmse:25.95480\n[517]\tvalidation_0-rmse:25.95086\n[518]\tvalidation_0-rmse:25.95168\n[519]\tvalidation_0-rmse:25.95169\n[520]\tvalidation_0-rmse:25.94901\n[521]\tvalidation_0-rmse:25.94714\n[522]\tvalidation_0-rmse:25.94527\n[523]\tvalidation_0-rmse:25.94942\n[524]\tvalidation_0-rmse:25.95015\n[525]\tvalidation_0-rmse:25.94955\n[526]\tvalidation_0-rmse:25.94945\n[527]\tvalidation_0-rmse:25.95194\n[528]\tvalidation_0-rmse:25.94735\n[529]\tvalidation_0-rmse:25.95046\n[530]\tvalidation_0-rmse:25.95144\n[531]\tvalidation_0-rmse:25.95071\n[532]\tvalidation_0-rmse:25.95425\n[533]\tvalidation_0-rmse:25.95387\n[534]\tvalidation_0-rmse:25.95535\n[535]\tvalidation_0-rmse:25.95244\n[536]\tvalidation_0-rmse:25.95256\n[537]\tvalidation_0-rmse:25.95209\n[538]\tvalidation_0-rmse:25.94768\n[539]\tvalidation_0-rmse:25.94540\n[540]\tvalidation_0-rmse:25.94515\n[541]\tvalidation_0-rmse:25.94396\n[542]\tvalidation_0-rmse:25.94756\n[543]\tvalidation_0-rmse:25.95006\n[544]\tvalidation_0-rmse:25.95354\n[545]\tvalidation_0-rmse:25.95229\n[546]\tvalidation_0-rmse:25.94858\n[547]\tvalidation_0-rmse:25.94559\n[548]\tvalidation_0-rmse:25.94898\n[549]\tvalidation_0-rmse:25.95045\n[550]\tvalidation_0-rmse:25.94978\n[551]\tvalidation_0-rmse:25.94980\n[552]\tvalidation_0-rmse:25.94978\n[553]\tvalidation_0-rmse:25.95370\n[554]\tvalidation_0-rmse:25.95668\n[555]\tvalidation_0-rmse:25.95378\n[556]\tvalidation_0-rmse:25.95790\n[557]\tvalidation_0-rmse:25.96065\n[558]\tvalidation_0-rmse:25.95850\n[559]\tvalidation_0-rmse:25.95543\n[560]\tvalidation_0-rmse:25.95517\n[561]\tvalidation_0-rmse:25.95022\n[562]\tvalidation_0-rmse:25.94708\n[563]\tvalidation_0-rmse:25.94745\n[564]\tvalidation_0-rmse:25.94879\n[565]\tvalidation_0-rmse:25.94509\n[566]\tvalidation_0-rmse:25.94641\n[567]\tvalidation_0-rmse:25.94531\n[568]\tvalidation_0-rmse:25.94425\n[569]\tvalidation_0-rmse:25.94346\n[570]\tvalidation_0-rmse:25.94153\n[571]\tvalidation_0-rmse:25.94137\n[572]\tvalidation_0-rmse:25.94641\n[573]\tvalidation_0-rmse:25.94884\n[574]\tvalidation_0-rmse:25.95319\n[575]\tvalidation_0-rmse:25.95563\n[576]\tvalidation_0-rmse:25.95606\n[577]\tvalidation_0-rmse:25.95491\n[578]\tvalidation_0-rmse:25.95466\n[579]\tvalidation_0-rmse:25.95281\n[580]\tvalidation_0-rmse:25.95210\n[581]\tvalidation_0-rmse:25.95035\n[582]\tvalidation_0-rmse:25.95005\n[583]\tvalidation_0-rmse:25.94977\n[584]\tvalidation_0-rmse:25.95174\n[585]\tvalidation_0-rmse:25.95073\n[586]\tvalidation_0-rmse:25.95392\n[587]\tvalidation_0-rmse:25.95204\n[588]\tvalidation_0-rmse:25.95431\n[589]\tvalidation_0-rmse:25.95038\n[590]\tvalidation_0-rmse:25.95368\n[591]\tvalidation_0-rmse:25.95064\n[592]\tvalidation_0-rmse:25.95298\n[593]\tvalidation_0-rmse:25.95411\n[594]\tvalidation_0-rmse:25.95686\n[595]\tvalidation_0-rmse:25.95442\n[596]\tvalidation_0-rmse:25.95564\n[597]\tvalidation_0-rmse:25.95719\n[598]\tvalidation_0-rmse:25.95506\n[599]\tvalidation_0-rmse:25.95721\n[600]\tvalidation_0-rmse:25.95683\n[601]\tvalidation_0-rmse:25.95577\n[602]\tvalidation_0-rmse:25.95427\n[603]\tvalidation_0-rmse:25.95489\n[604]\tvalidation_0-rmse:25.95783\n[605]\tvalidation_0-rmse:25.95743\n[606]\tvalidation_0-rmse:25.96013\n[607]\tvalidation_0-rmse:25.96158\n[608]\tvalidation_0-rmse:25.96357\n[609]\tvalidation_0-rmse:25.96260\n[610]\tvalidation_0-rmse:25.96696\n[611]\tvalidation_0-rmse:25.96666\n[612]\tvalidation_0-rmse:25.96575\n[613]\tvalidation_0-rmse:25.96630\n[614]\tvalidation_0-rmse:25.96628\n[615]\tvalidation_0-rmse:25.97249\n[616]\tvalidation_0-rmse:25.97277\n[617]\tvalidation_0-rmse:25.97404\n[618]\tvalidation_0-rmse:25.97292\n[619]\tvalidation_0-rmse:25.97319\n[620]\tvalidation_0-rmse:25.96681\n[621]\tvalidation_0-rmse:25.96337\n[622]\tvalidation_0-rmse:25.96212\n[623]\tvalidation_0-rmse:25.96511\n[624]\tvalidation_0-rmse:25.96952\n[625]\tvalidation_0-rmse:25.96695\n[626]\tvalidation_0-rmse:25.96813\n[627]\tvalidation_0-rmse:25.96814\n[628]\tvalidation_0-rmse:25.97090\n[629]\tvalidation_0-rmse:25.96760\n[630]\tvalidation_0-rmse:25.96617\n[631]\tvalidation_0-rmse:25.96507\n[632]\tvalidation_0-rmse:25.96801\n[633]\tvalidation_0-rmse:25.96575\n[634]\tvalidation_0-rmse:25.96561\n[635]\tvalidation_0-rmse:25.96703\n[636]\tvalidation_0-rmse:25.96404\n[637]\tvalidation_0-rmse:25.96747\n[638]\tvalidation_0-rmse:25.96982\n[639]\tvalidation_0-rmse:25.97135\n[640]\tvalidation_0-rmse:25.97013\n[641]\tvalidation_0-rmse:25.97093\n[642]\tvalidation_0-rmse:25.97276\n[643]\tvalidation_0-rmse:25.97084\n[644]\tvalidation_0-rmse:25.97013\n[645]\tvalidation_0-rmse:25.96992\n[646]\tvalidation_0-rmse:25.97219\n[647]\tvalidation_0-rmse:25.96977\n[648]\tvalidation_0-rmse:25.97281\n[649]\tvalidation_0-rmse:25.97293\n[650]\tvalidation_0-rmse:25.97109\n[651]\tvalidation_0-rmse:25.97548\n[652]\tvalidation_0-rmse:25.97461\n[653]\tvalidation_0-rmse:25.97481\n[654]\tvalidation_0-rmse:25.97175\n[655]\tvalidation_0-rmse:25.96924\n[656]\tvalidation_0-rmse:25.97017\n[657]\tvalidation_0-rmse:25.96838\n[658]\tvalidation_0-rmse:25.96467\n[659]\tvalidation_0-rmse:25.96536\n[660]\tvalidation_0-rmse:25.96703\n[661]\tvalidation_0-rmse:25.96786\n[662]\tvalidation_0-rmse:25.96777\n[663]\tvalidation_0-rmse:25.97067\n[664]\tvalidation_0-rmse:25.97084\n[665]\tvalidation_0-rmse:25.97057\n[666]\tvalidation_0-rmse:25.97410\n[667]\tvalidation_0-rmse:25.97314\n[668]\tvalidation_0-rmse:25.97476\n[669]\tvalidation_0-rmse:25.97159\n[670]\tvalidation_0-rmse:25.97390\n[671]\tvalidation_0-rmse:25.97457\n[672]\tvalidation_0-rmse:25.97509\n[673]\tvalidation_0-rmse:25.97747\n[674]\tvalidation_0-rmse:25.97668\n[675]\tvalidation_0-rmse:25.97685\n[676]\tvalidation_0-rmse:25.97517\n[677]\tvalidation_0-rmse:25.97369\n[678]\tvalidation_0-rmse:25.97184\n[679]\tvalidation_0-rmse:25.97570\n[680]\tvalidation_0-rmse:25.97388\n[681]\tvalidation_0-rmse:25.97810\n[682]\tvalidation_0-rmse:25.97705\n[683]\tvalidation_0-rmse:25.97735\n[684]\tvalidation_0-rmse:25.97891\n[685]\tvalidation_0-rmse:25.97687\n[686]\tvalidation_0-rmse:25.97692\n[687]\tvalidation_0-rmse:25.97789\n[688]\tvalidation_0-rmse:25.97727\n[689]\tvalidation_0-rmse:25.97552\n[690]\tvalidation_0-rmse:25.98057\n[691]\tvalidation_0-rmse:25.97901\n[692]\tvalidation_0-rmse:25.98049\n[693]\tvalidation_0-rmse:25.97866\n[694]\tvalidation_0-rmse:25.97900\n[695]\tvalidation_0-rmse:25.97714\n[696]\tvalidation_0-rmse:25.97691\n[697]\tvalidation_0-rmse:25.97772\n[698]\tvalidation_0-rmse:25.97595\n[699]\tvalidation_0-rmse:25.97612\n[700]\tvalidation_0-rmse:25.97734\n[701]\tvalidation_0-rmse:25.97754\n[702]\tvalidation_0-rmse:25.97615\n[703]\tvalidation_0-rmse:25.97789\n[704]\tvalidation_0-rmse:25.97868\n[705]\tvalidation_0-rmse:25.97792\n[706]\tvalidation_0-rmse:25.97402\n[707]\tvalidation_0-rmse:25.97500\n[708]\tvalidation_0-rmse:25.97383\n[709]\tvalidation_0-rmse:25.97564\n[710]\tvalidation_0-rmse:25.97247\n[711]\tvalidation_0-rmse:25.96956\n[712]\tvalidation_0-rmse:25.96995\n[713]\tvalidation_0-rmse:25.96758\n[714]\tvalidation_0-rmse:25.96979\n[715]\tvalidation_0-rmse:25.97085\n[716]\tvalidation_0-rmse:25.96968\n[717]\tvalidation_0-rmse:25.96951\n[718]\tvalidation_0-rmse:25.97207\n[719]\tvalidation_0-rmse:25.96913\n[720]\tvalidation_0-rmse:25.96872\n[721]\tvalidation_0-rmse:25.97261\n[722]\tvalidation_0-rmse:25.97413\n[723]\tvalidation_0-rmse:25.97462\n[724]\tvalidation_0-rmse:25.97656\n[725]\tvalidation_0-rmse:25.97900\n[726]\tvalidation_0-rmse:25.98106\n[727]\tvalidation_0-rmse:25.98125\n[728]\tvalidation_0-rmse:25.98081\n[729]\tvalidation_0-rmse:25.98442\n[730]\tvalidation_0-rmse:25.98247\n[731]\tvalidation_0-rmse:25.98671\n[732]\tvalidation_0-rmse:25.98935\n[733]\tvalidation_0-rmse:25.98968\n[734]\tvalidation_0-rmse:25.99145\n[735]\tvalidation_0-rmse:25.98743\n[736]\tvalidation_0-rmse:25.98399\n[737]\tvalidation_0-rmse:25.98352\n[738]\tvalidation_0-rmse:25.98184\n[739]\tvalidation_0-rmse:25.98419\n[740]\tvalidation_0-rmse:25.98598\n[741]\tvalidation_0-rmse:25.98796\n[742]\tvalidation_0-rmse:25.98721\n[743]\tvalidation_0-rmse:25.99082\n[744]\tvalidation_0-rmse:25.98931\n[745]\tvalidation_0-rmse:25.99074\n[746]\tvalidation_0-rmse:25.99238\n[747]\tvalidation_0-rmse:25.99637\n[748]\tvalidation_0-rmse:26.00050\n[749]\tvalidation_0-rmse:25.99885\n[750]\tvalidation_0-rmse:25.99643\n[751]\tvalidation_0-rmse:25.99761\n[752]\tvalidation_0-rmse:26.00033\n[753]\tvalidation_0-rmse:25.99992\n[754]\tvalidation_0-rmse:26.00299\n[755]\tvalidation_0-rmse:26.00165\n[756]\tvalidation_0-rmse:26.00382\n[757]\tvalidation_0-rmse:26.00351\n[758]\tvalidation_0-rmse:26.00509\n[759]\tvalidation_0-rmse:26.00772\n[760]\tvalidation_0-rmse:26.00401\n[761]\tvalidation_0-rmse:26.00303\n[762]\tvalidation_0-rmse:26.00271\n[763]\tvalidation_0-rmse:26.00195\n[764]\tvalidation_0-rmse:26.00153\n[765]\tvalidation_0-rmse:26.00178\n[766]\tvalidation_0-rmse:26.00437\n[767]\tvalidation_0-rmse:26.00480\n[768]\tvalidation_0-rmse:26.00722\n[769]\tvalidation_0-rmse:26.00609\n[770]\tvalidation_0-rmse:26.00625\n[771]\tvalidation_0-rmse:26.00440\n[772]\tvalidation_0-rmse:26.00426\n[773]\tvalidation_0-rmse:26.00166\n[774]\tvalidation_0-rmse:26.00105\n[775]\tvalidation_0-rmse:26.00445\n[776]\tvalidation_0-rmse:26.00627\n[777]\tvalidation_0-rmse:26.00696\n[778]\tvalidation_0-rmse:26.00880\n[779]\tvalidation_0-rmse:26.01050\n[780]\tvalidation_0-rmse:26.00900\n[781]\tvalidation_0-rmse:26.00882\n[782]\tvalidation_0-rmse:26.01092\n[783]\tvalidation_0-rmse:26.01088\n[784]\tvalidation_0-rmse:26.01089\n[785]\tvalidation_0-rmse:26.01042\n[786]\tvalidation_0-rmse:26.00837\n[787]\tvalidation_0-rmse:26.00950\n[788]\tvalidation_0-rmse:26.00981\n[789]\tvalidation_0-rmse:26.00985\n[790]\tvalidation_0-rmse:26.00986\n[791]\tvalidation_0-rmse:26.00656\n[792]\tvalidation_0-rmse:26.00573\n[793]\tvalidation_0-rmse:26.00629\n[794]\tvalidation_0-rmse:26.01013\n[795]\tvalidation_0-rmse:26.01063\n[796]\tvalidation_0-rmse:26.00715\n[797]\tvalidation_0-rmse:26.00744\n[798]\tvalidation_0-rmse:26.00591\n[799]\tvalidation_0-rmse:26.00538\n[800]\tvalidation_0-rmse:26.00557\n[801]\tvalidation_0-rmse:26.00400\n[802]\tvalidation_0-rmse:26.00552\n[803]\tvalidation_0-rmse:26.00580\n[804]\tvalidation_0-rmse:26.00553\n[805]\tvalidation_0-rmse:26.00601\n[806]\tvalidation_0-rmse:26.00596\n[807]\tvalidation_0-rmse:26.00534\n[808]\tvalidation_0-rmse:26.00329\n[809]\tvalidation_0-rmse:26.00267\n[810]\tvalidation_0-rmse:26.00337\n[811]\tvalidation_0-rmse:26.00234\n[812]\tvalidation_0-rmse:26.00551\n[813]\tvalidation_0-rmse:26.00484\n[814]\tvalidation_0-rmse:26.00412\n[815]\tvalidation_0-rmse:26.00528\n[816]\tvalidation_0-rmse:26.00496\n[817]\tvalidation_0-rmse:26.00611\n[818]\tvalidation_0-rmse:26.00699\n[819]\tvalidation_0-rmse:26.00866\n[820]\tvalidation_0-rmse:26.00586\n[821]\tvalidation_0-rmse:26.00784\n[822]\tvalidation_0-rmse:26.01114\n[823]\tvalidation_0-rmse:26.01217\n[824]\tvalidation_0-rmse:26.01143\n[825]\tvalidation_0-rmse:26.00827\n[826]\tvalidation_0-rmse:26.00420\n[827]\tvalidation_0-rmse:26.00533\n[828]\tvalidation_0-rmse:26.00583\n[829]\tvalidation_0-rmse:26.00526\n[830]\tvalidation_0-rmse:26.00494\n[831]\tvalidation_0-rmse:26.00434\n[832]\tvalidation_0-rmse:26.00610\n[833]\tvalidation_0-rmse:26.00552\n[834]\tvalidation_0-rmse:26.00330\n[835]\tvalidation_0-rmse:26.00305\n[836]\tvalidation_0-rmse:26.00625\n[837]\tvalidation_0-rmse:26.00789\n[838]\tvalidation_0-rmse:26.00815\n[839]\tvalidation_0-rmse:26.00918\n[840]\tvalidation_0-rmse:26.00764\n[841]\tvalidation_0-rmse:26.00909\n[842]\tvalidation_0-rmse:26.01186\n[843]\tvalidation_0-rmse:26.01014\n[844]\tvalidation_0-rmse:26.00876\n[845]\tvalidation_0-rmse:26.00783\n[846]\tvalidation_0-rmse:26.00705\n[847]\tvalidation_0-rmse:26.00991\n[848]\tvalidation_0-rmse:26.00777\n[849]\tvalidation_0-rmse:26.00693\n[850]\tvalidation_0-rmse:26.00861\n[851]\tvalidation_0-rmse:26.00886\n[852]\tvalidation_0-rmse:26.00905\n[853]\tvalidation_0-rmse:26.00787\n[854]\tvalidation_0-rmse:26.00630\n[855]\tvalidation_0-rmse:26.00424\n[856]\tvalidation_0-rmse:26.00369\n[857]\tvalidation_0-rmse:26.00575\n[858]\tvalidation_0-rmse:26.00378\n[859]\tvalidation_0-rmse:26.00399\n[860]\tvalidation_0-rmse:26.00327\n[861]\tvalidation_0-rmse:26.00291\n[862]\tvalidation_0-rmse:25.99829\n[863]\tvalidation_0-rmse:25.99953\n[864]\tvalidation_0-rmse:26.00228\n[865]\tvalidation_0-rmse:25.99977\n[866]\tvalidation_0-rmse:25.99875\n[867]\tvalidation_0-rmse:25.99586\n[868]\tvalidation_0-rmse:25.99854\n[869]\tvalidation_0-rmse:26.00107\n[870]\tvalidation_0-rmse:26.00076\n[871]\tvalidation_0-rmse:25.99905\n[872]\tvalidation_0-rmse:26.00038\n[873]\tvalidation_0-rmse:26.00310\n[874]\tvalidation_0-rmse:26.00379\n[875]\tvalidation_0-rmse:26.00244\n[876]\tvalidation_0-rmse:26.00088\n[877]\tvalidation_0-rmse:25.99905\n[878]\tvalidation_0-rmse:26.00166\n[879]\tvalidation_0-rmse:26.00330\n[880]\tvalidation_0-rmse:26.00383\n[881]\tvalidation_0-rmse:26.00238\n[882]\tvalidation_0-rmse:26.00409\n[883]\tvalidation_0-rmse:26.00180\n[884]\tvalidation_0-rmse:26.00196\n[885]\tvalidation_0-rmse:26.00051\n[886]\tvalidation_0-rmse:26.00082\n[887]\tvalidation_0-rmse:26.00227\n[888]\tvalidation_0-rmse:26.00048\n[889]\tvalidation_0-rmse:26.00099\n[890]\tvalidation_0-rmse:26.00345\n[891]\tvalidation_0-rmse:26.00311\n[892]\tvalidation_0-rmse:26.00265\n[893]\tvalidation_0-rmse:26.00268\n[894]\tvalidation_0-rmse:26.00248\n[895]\tvalidation_0-rmse:26.00315\n[896]\tvalidation_0-rmse:26.00265\n[897]\tvalidation_0-rmse:26.00189\n[898]\tvalidation_0-rmse:26.00226\n[899]\tvalidation_0-rmse:26.00184\n[900]\tvalidation_0-rmse:26.00363\n[901]\tvalidation_0-rmse:26.00304\n[902]\tvalidation_0-rmse:26.00451\n[903]\tvalidation_0-rmse:26.00414\n[904]\tvalidation_0-rmse:26.00789\n[905]\tvalidation_0-rmse:26.00744\n[906]\tvalidation_0-rmse:26.00858\n[907]\tvalidation_0-rmse:26.00866\n[908]\tvalidation_0-rmse:26.00924\n[909]\tvalidation_0-rmse:26.01034\n[910]\tvalidation_0-rmse:26.01033\n[911]\tvalidation_0-rmse:26.01006\n[912]\tvalidation_0-rmse:26.00848\n[913]\tvalidation_0-rmse:26.00768\n[914]\tvalidation_0-rmse:26.00798\n[915]\tvalidation_0-rmse:26.01073\n[916]\tvalidation_0-rmse:26.01123\n[917]\tvalidation_0-rmse:26.01401\n[918]\tvalidation_0-rmse:26.01291\n[919]\tvalidation_0-rmse:26.01290\n[920]\tvalidation_0-rmse:26.01330\n[921]\tvalidation_0-rmse:26.01172\n[922]\tvalidation_0-rmse:26.01132\n[923]\tvalidation_0-rmse:26.01063\n[924]\tvalidation_0-rmse:26.01148\n[925]\tvalidation_0-rmse:26.01112\n[926]\tvalidation_0-rmse:26.01218\n[927]\tvalidation_0-rmse:26.01192\n[928]\tvalidation_0-rmse:26.01331\n[929]\tvalidation_0-rmse:26.01448\n[930]\tvalidation_0-rmse:26.01631\n[931]\tvalidation_0-rmse:26.01443\n[932]\tvalidation_0-rmse:26.01478\n[933]\tvalidation_0-rmse:26.01828\n[934]\tvalidation_0-rmse:26.01758\n[935]\tvalidation_0-rmse:26.01732\n[936]\tvalidation_0-rmse:26.01665\n[937]\tvalidation_0-rmse:26.01537\n[938]\tvalidation_0-rmse:26.01759\n[939]\tvalidation_0-rmse:26.01799\n[940]\tvalidation_0-rmse:26.01822\n[941]\tvalidation_0-rmse:26.01549\n[942]\tvalidation_0-rmse:26.01429\n[943]\tvalidation_0-rmse:26.01642\n[944]\tvalidation_0-rmse:26.01647\n[945]\tvalidation_0-rmse:26.01736\n[946]\tvalidation_0-rmse:26.01717\n[947]\tvalidation_0-rmse:26.01693\n[948]\tvalidation_0-rmse:26.01518\n[949]\tvalidation_0-rmse:26.01596\n[950]\tvalidation_0-rmse:26.01667\n[951]\tvalidation_0-rmse:26.01545\n[952]\tvalidation_0-rmse:26.01543\n[953]\tvalidation_0-rmse:26.01640\n[954]\tvalidation_0-rmse:26.01652\n[955]\tvalidation_0-rmse:26.01774\n[956]\tvalidation_0-rmse:26.01746\n[957]\tvalidation_0-rmse:26.01895\n[958]\tvalidation_0-rmse:26.02167\n[959]\tvalidation_0-rmse:26.02093\n[960]\tvalidation_0-rmse:26.01903\n[961]\tvalidation_0-rmse:26.02200\n[962]\tvalidation_0-rmse:26.02593\n[963]\tvalidation_0-rmse:26.02690\n[964]\tvalidation_0-rmse:26.02904\n[965]\tvalidation_0-rmse:26.03483\n[966]\tvalidation_0-rmse:26.03461\n[967]\tvalidation_0-rmse:26.03760\n[968]\tvalidation_0-rmse:26.03982\n[969]\tvalidation_0-rmse:26.04020\n[970]\tvalidation_0-rmse:26.04045\n[971]\tvalidation_0-rmse:26.04051\n[972]\tvalidation_0-rmse:26.04043\n[973]\tvalidation_0-rmse:26.03975\n[974]\tvalidation_0-rmse:26.04182\n[975]\tvalidation_0-rmse:26.04285\n[976]\tvalidation_0-rmse:26.04371\n[977]\tvalidation_0-rmse:26.04528\n[978]\tvalidation_0-rmse:26.04461\n[979]\tvalidation_0-rmse:26.04287\n[980]\tvalidation_0-rmse:26.04353\n[981]\tvalidation_0-rmse:26.04525\n[982]\tvalidation_0-rmse:26.04614\n[983]\tvalidation_0-rmse:26.04425\n[984]\tvalidation_0-rmse:26.04259\n[985]\tvalidation_0-rmse:26.04337\n[986]\tvalidation_0-rmse:26.04305\n[987]\tvalidation_0-rmse:26.04181\n[988]\tvalidation_0-rmse:26.04200\n[989]\tvalidation_0-rmse:26.04248\n[990]\tvalidation_0-rmse:26.04264\n[991]\tvalidation_0-rmse:26.04176\n[992]\tvalidation_0-rmse:26.03853\n[993]\tvalidation_0-rmse:26.03877\n[994]\tvalidation_0-rmse:26.03807\n[995]\tvalidation_0-rmse:26.03841\n[996]\tvalidation_0-rmse:26.03928\n[997]\tvalidation_0-rmse:26.03812\n[998]\tvalidation_0-rmse:26.03862\n[999]\tvalidation_0-rmse:26.03696\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006979 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3175\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 783\n[LightGBM] [Info] Start training from score 33.054065\n[0]\tvalidation_0-rmse:25.44726\n[1]\tvalidation_0-rmse:25.35752\n[2]\tvalidation_0-rmse:25.25819\n[3]\tvalidation_0-rmse:25.14675\n[4]\tvalidation_0-rmse:25.07122\n[5]\tvalidation_0-rmse:24.99534\n[6]\tvalidation_0-rmse:24.92251\n[7]\tvalidation_0-rmse:24.86270\n[8]\tvalidation_0-rmse:24.79438\n[9]\tvalidation_0-rmse:24.75130\n[10]\tvalidation_0-rmse:24.70736\n[11]\tvalidation_0-rmse:24.64124\n[12]\tvalidation_0-rmse:24.59024\n[13]\tvalidation_0-rmse:24.53114\n[14]\tvalidation_0-rmse:24.49582\n[15]\tvalidation_0-rmse:24.43080\n[16]\tvalidation_0-rmse:24.38842\n[17]\tvalidation_0-rmse:24.33021\n[18]\tvalidation_0-rmse:24.28398\n[19]\tvalidation_0-rmse:24.24040\n[20]\tvalidation_0-rmse:24.19835\n[21]\tvalidation_0-rmse:24.16033\n[22]\tvalidation_0-rmse:24.13095\n[23]\tvalidation_0-rmse:24.11500\n[24]\tvalidation_0-rmse:24.10032\n[25]\tvalidation_0-rmse:24.05639\n[26]\tvalidation_0-rmse:24.03049\n[27]\tvalidation_0-rmse:23.99428\n[28]\tvalidation_0-rmse:23.97965\n[29]\tvalidation_0-rmse:23.94486\n[30]\tvalidation_0-rmse:23.93620\n[31]\tvalidation_0-rmse:23.90823\n[32]\tvalidation_0-rmse:23.86999\n[33]\tvalidation_0-rmse:23.85897\n[34]\tvalidation_0-rmse:23.84532\n[35]\tvalidation_0-rmse:23.82190\n[36]\tvalidation_0-rmse:23.78243\n[37]\tvalidation_0-rmse:23.76196\n[38]\tvalidation_0-rmse:23.75166\n[39]\tvalidation_0-rmse:23.72202\n[40]\tvalidation_0-rmse:23.71298\n[41]\tvalidation_0-rmse:23.67890\n[42]\tvalidation_0-rmse:23.66124\n[43]\tvalidation_0-rmse:23.64660\n[44]\tvalidation_0-rmse:23.64492\n[45]\tvalidation_0-rmse:23.64023\n[46]\tvalidation_0-rmse:23.63496\n[47]\tvalidation_0-rmse:23.62164\n[48]\tvalidation_0-rmse:23.61398\n[49]\tvalidation_0-rmse:23.59484\n[50]\tvalidation_0-rmse:23.56989\n[51]\tvalidation_0-rmse:23.56621\n[52]\tvalidation_0-rmse:23.55770\n[53]\tvalidation_0-rmse:23.55584\n[54]\tvalidation_0-rmse:23.57264\n[55]\tvalidation_0-rmse:23.56094\n[56]\tvalidation_0-rmse:23.55297\n[57]\tvalidation_0-rmse:23.55189\n[58]\tvalidation_0-rmse:23.53846\n[59]\tvalidation_0-rmse:23.52483\n[60]\tvalidation_0-rmse:23.51770\n[61]\tvalidation_0-rmse:23.50609\n[62]\tvalidation_0-rmse:23.49488\n[63]\tvalidation_0-rmse:23.50686\n[64]\tvalidation_0-rmse:23.49582\n[65]\tvalidation_0-rmse:23.51574\n[66]\tvalidation_0-rmse:23.52356\n[67]\tvalidation_0-rmse:23.54392\n[68]\tvalidation_0-rmse:23.53856\n[69]\tvalidation_0-rmse:23.53565\n[70]\tvalidation_0-rmse:23.54107\n[71]\tvalidation_0-rmse:23.52998\n[72]\tvalidation_0-rmse:23.52113\n[73]\tvalidation_0-rmse:23.52681\n[74]\tvalidation_0-rmse:23.53150\n[75]\tvalidation_0-rmse:23.54406\n[76]\tvalidation_0-rmse:23.54226\n[77]\tvalidation_0-rmse:23.55048\n[78]\tvalidation_0-rmse:23.55502\n[79]\tvalidation_0-rmse:23.56146\n[80]\tvalidation_0-rmse:23.57076\n[81]\tvalidation_0-rmse:23.57867\n[82]\tvalidation_0-rmse:23.57395\n[83]\tvalidation_0-rmse:23.58348\n[84]\tvalidation_0-rmse:23.56686\n[85]\tvalidation_0-rmse:23.58008\n[86]\tvalidation_0-rmse:23.56869\n[87]\tvalidation_0-rmse:23.57540\n[88]\tvalidation_0-rmse:23.56790\n[89]\tvalidation_0-rmse:23.55766\n[90]\tvalidation_0-rmse:23.56233\n[91]\tvalidation_0-rmse:23.56408\n[92]\tvalidation_0-rmse:23.57080\n[93]\tvalidation_0-rmse:23.57202\n[94]\tvalidation_0-rmse:23.56821\n[95]\tvalidation_0-rmse:23.56423\n[96]\tvalidation_0-rmse:23.57276\n[97]\tvalidation_0-rmse:23.58104\n[98]\tvalidation_0-rmse:23.58467\n[99]\tvalidation_0-rmse:23.58864\n[100]\tvalidation_0-rmse:23.57310\n[101]\tvalidation_0-rmse:23.58111\n[102]\tvalidation_0-rmse:23.58958\n[103]\tvalidation_0-rmse:23.58892\n[104]\tvalidation_0-rmse:23.59660\n[105]\tvalidation_0-rmse:23.59670\n[106]\tvalidation_0-rmse:23.60697\n[107]\tvalidation_0-rmse:23.61702\n[108]\tvalidation_0-rmse:23.61224\n[109]\tvalidation_0-rmse:23.61936\n[110]\tvalidation_0-rmse:23.60932\n[111]\tvalidation_0-rmse:23.61171\n[112]\tvalidation_0-rmse:23.61574\n[113]\tvalidation_0-rmse:23.60963\n[114]\tvalidation_0-rmse:23.61326\n[115]\tvalidation_0-rmse:23.60722\n[116]\tvalidation_0-rmse:23.61214\n[117]\tvalidation_0-rmse:23.61588\n[118]\tvalidation_0-rmse:23.62192\n[119]\tvalidation_0-rmse:23.63251\n[120]\tvalidation_0-rmse:23.63353\n[121]\tvalidation_0-rmse:23.63614\n[122]\tvalidation_0-rmse:23.63669\n[123]\tvalidation_0-rmse:23.64306\n[124]\tvalidation_0-rmse:23.63711\n[125]\tvalidation_0-rmse:23.63354\n[126]\tvalidation_0-rmse:23.62392\n[127]\tvalidation_0-rmse:23.62719\n[128]\tvalidation_0-rmse:23.62888\n[129]\tvalidation_0-rmse:23.62497\n[130]\tvalidation_0-rmse:23.63022\n[131]\tvalidation_0-rmse:23.63715\n[132]\tvalidation_0-rmse:23.64053\n[133]\tvalidation_0-rmse:23.64262\n[134]\tvalidation_0-rmse:23.63707\n[135]\tvalidation_0-rmse:23.62938\n[136]\tvalidation_0-rmse:23.63681\n[137]\tvalidation_0-rmse:23.65113\n[138]\tvalidation_0-rmse:23.65206\n[139]\tvalidation_0-rmse:23.65131\n[140]\tvalidation_0-rmse:23.66136\n[141]\tvalidation_0-rmse:23.64979\n[142]\tvalidation_0-rmse:23.65923\n[143]\tvalidation_0-rmse:23.66684\n[144]\tvalidation_0-rmse:23.66922\n[145]\tvalidation_0-rmse:23.67061\n[146]\tvalidation_0-rmse:23.67159\n[147]\tvalidation_0-rmse:23.67169\n[148]\tvalidation_0-rmse:23.67216\n[149]\tvalidation_0-rmse:23.66541\n[150]\tvalidation_0-rmse:23.66118\n[151]\tvalidation_0-rmse:23.66782\n[152]\tvalidation_0-rmse:23.67850\n[153]\tvalidation_0-rmse:23.68252\n[154]\tvalidation_0-rmse:23.68681\n[155]\tvalidation_0-rmse:23.70293\n[156]\tvalidation_0-rmse:23.70105\n[157]\tvalidation_0-rmse:23.69493\n[158]\tvalidation_0-rmse:23.70591\n[159]\tvalidation_0-rmse:23.71172\n[160]\tvalidation_0-rmse:23.71381\n[161]\tvalidation_0-rmse:23.72248\n[162]\tvalidation_0-rmse:23.72791\n[163]\tvalidation_0-rmse:23.72671\n[164]\tvalidation_0-rmse:23.72663\n[165]\tvalidation_0-rmse:23.72987\n[166]\tvalidation_0-rmse:23.73471\n[167]\tvalidation_0-rmse:23.72921\n[168]\tvalidation_0-rmse:23.73630\n[169]\tvalidation_0-rmse:23.73629\n[170]\tvalidation_0-rmse:23.74444\n[171]\tvalidation_0-rmse:23.74681\n[172]\tvalidation_0-rmse:23.74562\n[173]\tvalidation_0-rmse:23.75014\n[174]\tvalidation_0-rmse:23.75400\n[175]\tvalidation_0-rmse:23.76541\n[176]\tvalidation_0-rmse:23.77630\n[177]\tvalidation_0-rmse:23.78243\n[178]\tvalidation_0-rmse:23.78929\n[179]\tvalidation_0-rmse:23.77916\n[180]\tvalidation_0-rmse:23.78483\n[181]\tvalidation_0-rmse:23.78580\n[182]\tvalidation_0-rmse:23.79218\n[183]\tvalidation_0-rmse:23.79189\n[184]\tvalidation_0-rmse:23.79090\n[185]\tvalidation_0-rmse:23.79577\n[186]\tvalidation_0-rmse:23.79733\n[187]\tvalidation_0-rmse:23.79721\n[188]\tvalidation_0-rmse:23.79440\n[189]\tvalidation_0-rmse:23.79509\n[190]\tvalidation_0-rmse:23.80333\n[191]\tvalidation_0-rmse:23.80541\n[192]\tvalidation_0-rmse:23.81117\n[193]\tvalidation_0-rmse:23.82341\n[194]\tvalidation_0-rmse:23.84477\n[195]\tvalidation_0-rmse:23.85451\n[196]\tvalidation_0-rmse:23.85588\n[197]\tvalidation_0-rmse:23.85749\n[198]\tvalidation_0-rmse:23.85858\n[199]\tvalidation_0-rmse:23.86851\n[200]\tvalidation_0-rmse:23.87606\n[201]\tvalidation_0-rmse:23.88915\n[202]\tvalidation_0-rmse:23.88827\n[203]\tvalidation_0-rmse:23.88918\n[204]\tvalidation_0-rmse:23.89239\n[205]\tvalidation_0-rmse:23.89735\n[206]\tvalidation_0-rmse:23.89652\n[207]\tvalidation_0-rmse:23.90905\n[208]\tvalidation_0-rmse:23.91035\n[209]\tvalidation_0-rmse:23.90730\n[210]\tvalidation_0-rmse:23.91290\n[211]\tvalidation_0-rmse:23.91938\n[212]\tvalidation_0-rmse:23.92204\n[213]\tvalidation_0-rmse:23.91954\n[214]\tvalidation_0-rmse:23.92081\n[215]\tvalidation_0-rmse:23.91999\n[216]\tvalidation_0-rmse:23.92593\n[217]\tvalidation_0-rmse:23.93256\n[218]\tvalidation_0-rmse:23.93944\n[219]\tvalidation_0-rmse:23.94672\n[220]\tvalidation_0-rmse:23.93796\n[221]\tvalidation_0-rmse:23.94505\n[222]\tvalidation_0-rmse:23.94854\n[223]\tvalidation_0-rmse:23.96397\n[224]\tvalidation_0-rmse:23.96766\n[225]\tvalidation_0-rmse:23.97556\n[226]\tvalidation_0-rmse:23.97891\n[227]\tvalidation_0-rmse:23.98445\n[228]\tvalidation_0-rmse:23.98987\n[229]\tvalidation_0-rmse:23.99022\n[230]\tvalidation_0-rmse:23.99422\n[231]\tvalidation_0-rmse:23.99368\n[232]\tvalidation_0-rmse:23.99770\n[233]\tvalidation_0-rmse:23.99813\n[234]\tvalidation_0-rmse:24.00320\n[235]\tvalidation_0-rmse:24.01368\n[236]\tvalidation_0-rmse:24.02050\n[237]\tvalidation_0-rmse:24.02656\n[238]\tvalidation_0-rmse:24.02806\n[239]\tvalidation_0-rmse:24.02950\n[240]\tvalidation_0-rmse:24.03191\n[241]\tvalidation_0-rmse:24.03009\n[242]\tvalidation_0-rmse:24.03127\n[243]\tvalidation_0-rmse:24.04358\n[244]\tvalidation_0-rmse:24.04630\n[245]\tvalidation_0-rmse:24.05370\n[246]\tvalidation_0-rmse:24.06091\n[247]\tvalidation_0-rmse:24.05616\n[248]\tvalidation_0-rmse:24.05096\n[249]\tvalidation_0-rmse:24.04855\n[250]\tvalidation_0-rmse:24.05724\n[251]\tvalidation_0-rmse:24.05910\n[252]\tvalidation_0-rmse:24.05571\n[253]\tvalidation_0-rmse:24.06006\n[254]\tvalidation_0-rmse:24.06347\n[255]\tvalidation_0-rmse:24.06095\n[256]\tvalidation_0-rmse:24.05792\n[257]\tvalidation_0-rmse:24.06651\n[258]\tvalidation_0-rmse:24.07384\n[259]\tvalidation_0-rmse:24.08525\n[260]\tvalidation_0-rmse:24.07931\n[261]\tvalidation_0-rmse:24.07638\n[262]\tvalidation_0-rmse:24.07668\n[263]\tvalidation_0-rmse:24.08032\n[264]\tvalidation_0-rmse:24.09158\n[265]\tvalidation_0-rmse:24.10181\n[266]\tvalidation_0-rmse:24.10926\n[267]\tvalidation_0-rmse:24.11102\n[268]\tvalidation_0-rmse:24.10892\n[269]\tvalidation_0-rmse:24.11370\n[270]\tvalidation_0-rmse:24.12184\n[271]\tvalidation_0-rmse:24.12445\n[272]\tvalidation_0-rmse:24.12788\n[273]\tvalidation_0-rmse:24.13157\n[274]\tvalidation_0-rmse:24.13742\n[275]\tvalidation_0-rmse:24.13993\n[276]\tvalidation_0-rmse:24.14957\n[277]\tvalidation_0-rmse:24.14707\n[278]\tvalidation_0-rmse:24.14758\n[279]\tvalidation_0-rmse:24.14017\n[280]\tvalidation_0-rmse:24.14025\n[281]\tvalidation_0-rmse:24.13614\n[282]\tvalidation_0-rmse:24.14157\n[283]\tvalidation_0-rmse:24.14890\n[284]\tvalidation_0-rmse:24.14573\n[285]\tvalidation_0-rmse:24.14184\n[286]\tvalidation_0-rmse:24.14318\n[287]\tvalidation_0-rmse:24.14258\n[288]\tvalidation_0-rmse:24.15052\n[289]\tvalidation_0-rmse:24.15133\n[290]\tvalidation_0-rmse:24.15265\n[291]\tvalidation_0-rmse:24.15623\n[292]\tvalidation_0-rmse:24.15911\n[293]\tvalidation_0-rmse:24.15713\n[294]\tvalidation_0-rmse:24.16046\n[295]\tvalidation_0-rmse:24.16333\n[296]\tvalidation_0-rmse:24.16380\n[297]\tvalidation_0-rmse:24.17107\n[298]\tvalidation_0-rmse:24.17672\n[299]\tvalidation_0-rmse:24.17195\n[300]\tvalidation_0-rmse:24.17399\n[301]\tvalidation_0-rmse:24.17429\n[302]\tvalidation_0-rmse:24.17511\n[303]\tvalidation_0-rmse:24.17764\n[304]\tvalidation_0-rmse:24.17436\n[305]\tvalidation_0-rmse:24.17759\n[306]\tvalidation_0-rmse:24.17876\n[307]\tvalidation_0-rmse:24.18088\n[308]\tvalidation_0-rmse:24.17849\n[309]\tvalidation_0-rmse:24.17308\n[310]\tvalidation_0-rmse:24.17376\n[311]\tvalidation_0-rmse:24.17353\n[312]\tvalidation_0-rmse:24.18104\n[313]\tvalidation_0-rmse:24.18427\n[314]\tvalidation_0-rmse:24.18917\n[315]\tvalidation_0-rmse:24.18507\n[316]\tvalidation_0-rmse:24.18541\n[317]\tvalidation_0-rmse:24.18585\n[318]\tvalidation_0-rmse:24.18648\n[319]\tvalidation_0-rmse:24.18540\n[320]\tvalidation_0-rmse:24.18620\n[321]\tvalidation_0-rmse:24.19171\n[322]\tvalidation_0-rmse:24.20069\n[323]\tvalidation_0-rmse:24.20241\n[324]\tvalidation_0-rmse:24.20712\n[325]\tvalidation_0-rmse:24.20903\n[326]\tvalidation_0-rmse:24.20907\n[327]\tvalidation_0-rmse:24.21078\n[328]\tvalidation_0-rmse:24.20305\n[329]\tvalidation_0-rmse:24.19943\n[330]\tvalidation_0-rmse:24.20194\n[331]\tvalidation_0-rmse:24.20832\n[332]\tvalidation_0-rmse:24.21029\n[333]\tvalidation_0-rmse:24.21023\n[334]\tvalidation_0-rmse:24.21144\n[335]\tvalidation_0-rmse:24.20645\n[336]\tvalidation_0-rmse:24.20228\n[337]\tvalidation_0-rmse:24.20454\n[338]\tvalidation_0-rmse:24.20371\n[339]\tvalidation_0-rmse:24.20212\n[340]\tvalidation_0-rmse:24.20813\n[341]\tvalidation_0-rmse:24.21223\n[342]\tvalidation_0-rmse:24.20807\n[343]\tvalidation_0-rmse:24.20807\n[344]\tvalidation_0-rmse:24.21012\n[345]\tvalidation_0-rmse:24.21760\n[346]\tvalidation_0-rmse:24.21759\n[347]\tvalidation_0-rmse:24.21938\n[348]\tvalidation_0-rmse:24.21750\n[349]\tvalidation_0-rmse:24.21493\n[350]\tvalidation_0-rmse:24.21107\n[351]\tvalidation_0-rmse:24.20777\n[352]\tvalidation_0-rmse:24.20223\n[353]\tvalidation_0-rmse:24.19922\n[354]\tvalidation_0-rmse:24.20349\n[355]\tvalidation_0-rmse:24.20823\n[356]\tvalidation_0-rmse:24.20852\n[357]\tvalidation_0-rmse:24.20857\n[358]\tvalidation_0-rmse:24.20427\n[359]\tvalidation_0-rmse:24.20106\n[360]\tvalidation_0-rmse:24.20162\n[361]\tvalidation_0-rmse:24.20498\n[362]\tvalidation_0-rmse:24.20656\n[363]\tvalidation_0-rmse:24.20872\n[364]\tvalidation_0-rmse:24.20775\n[365]\tvalidation_0-rmse:24.20632\n[366]\tvalidation_0-rmse:24.20839\n[367]\tvalidation_0-rmse:24.20901\n[368]\tvalidation_0-rmse:24.20807\n[369]\tvalidation_0-rmse:24.20833\n[370]\tvalidation_0-rmse:24.21169\n[371]\tvalidation_0-rmse:24.20655\n[372]\tvalidation_0-rmse:24.20699\n[373]\tvalidation_0-rmse:24.21208\n[374]\tvalidation_0-rmse:24.21201\n[375]\tvalidation_0-rmse:24.20331\n[376]\tvalidation_0-rmse:24.20116\n[377]\tvalidation_0-rmse:24.20267\n[378]\tvalidation_0-rmse:24.20228\n[379]\tvalidation_0-rmse:24.19913\n[380]\tvalidation_0-rmse:24.20539\n[381]\tvalidation_0-rmse:24.19904\n[382]\tvalidation_0-rmse:24.20077\n[383]\tvalidation_0-rmse:24.20174\n[384]\tvalidation_0-rmse:24.19971\n[385]\tvalidation_0-rmse:24.19537\n[386]\tvalidation_0-rmse:24.19717\n[387]\tvalidation_0-rmse:24.20052\n[388]\tvalidation_0-rmse:24.20295\n[389]\tvalidation_0-rmse:24.20323\n[390]\tvalidation_0-rmse:24.20232\n[391]\tvalidation_0-rmse:24.20174\n[392]\tvalidation_0-rmse:24.20339\n[393]\tvalidation_0-rmse:24.20250\n[394]\tvalidation_0-rmse:24.20161\n[395]\tvalidation_0-rmse:24.20688\n[396]\tvalidation_0-rmse:24.20987\n[397]\tvalidation_0-rmse:24.21709\n[398]\tvalidation_0-rmse:24.21481\n[399]\tvalidation_0-rmse:24.21987\n[400]\tvalidation_0-rmse:24.22341\n[401]\tvalidation_0-rmse:24.22410\n[402]\tvalidation_0-rmse:24.22388\n[403]\tvalidation_0-rmse:24.22915\n[404]\tvalidation_0-rmse:24.23253\n[405]\tvalidation_0-rmse:24.23178\n[406]\tvalidation_0-rmse:24.23053\n[407]\tvalidation_0-rmse:24.22843\n[408]\tvalidation_0-rmse:24.23870\n[409]\tvalidation_0-rmse:24.24079\n[410]\tvalidation_0-rmse:24.24227\n[411]\tvalidation_0-rmse:24.24256\n[412]\tvalidation_0-rmse:24.24011\n[413]\tvalidation_0-rmse:24.24418\n[414]\tvalidation_0-rmse:24.24868\n[415]\tvalidation_0-rmse:24.24791\n[416]\tvalidation_0-rmse:24.24922\n[417]\tvalidation_0-rmse:24.24586\n[418]\tvalidation_0-rmse:24.24234\n[419]\tvalidation_0-rmse:24.24209\n[420]\tvalidation_0-rmse:24.24655\n[421]\tvalidation_0-rmse:24.24898\n[422]\tvalidation_0-rmse:24.25160\n[423]\tvalidation_0-rmse:24.24834\n[424]\tvalidation_0-rmse:24.25045\n[425]\tvalidation_0-rmse:24.24968\n[426]\tvalidation_0-rmse:24.24512\n[427]\tvalidation_0-rmse:24.24566\n[428]\tvalidation_0-rmse:24.24776\n[429]\tvalidation_0-rmse:24.23972\n[430]\tvalidation_0-rmse:24.24168\n[431]\tvalidation_0-rmse:24.24319\n[432]\tvalidation_0-rmse:24.24392\n[433]\tvalidation_0-rmse:24.24375\n[434]\tvalidation_0-rmse:24.24218\n[435]\tvalidation_0-rmse:24.24368\n[436]\tvalidation_0-rmse:24.23924\n[437]\tvalidation_0-rmse:24.23584\n[438]\tvalidation_0-rmse:24.23441\n[439]\tvalidation_0-rmse:24.23476\n[440]\tvalidation_0-rmse:24.23829\n[441]\tvalidation_0-rmse:24.23832\n[442]\tvalidation_0-rmse:24.23860\n[443]\tvalidation_0-rmse:24.24254\n[444]\tvalidation_0-rmse:24.24465\n[445]\tvalidation_0-rmse:24.24382\n[446]\tvalidation_0-rmse:24.24617\n[447]\tvalidation_0-rmse:24.24854\n[448]\tvalidation_0-rmse:24.24769\n[449]\tvalidation_0-rmse:24.24923\n[450]\tvalidation_0-rmse:24.25303\n[451]\tvalidation_0-rmse:24.25522\n[452]\tvalidation_0-rmse:24.25940\n[453]\tvalidation_0-rmse:24.26471\n[454]\tvalidation_0-rmse:24.26608\n[455]\tvalidation_0-rmse:24.26419\n[456]\tvalidation_0-rmse:24.25945\n[457]\tvalidation_0-rmse:24.26445\n[458]\tvalidation_0-rmse:24.26787\n[459]\tvalidation_0-rmse:24.27273\n[460]\tvalidation_0-rmse:24.27302\n[461]\tvalidation_0-rmse:24.27085\n[462]\tvalidation_0-rmse:24.26830\n[463]\tvalidation_0-rmse:24.26949\n[464]\tvalidation_0-rmse:24.26838\n[465]\tvalidation_0-rmse:24.26572\n[466]\tvalidation_0-rmse:24.26593\n[467]\tvalidation_0-rmse:24.26185\n[468]\tvalidation_0-rmse:24.26702\n[469]\tvalidation_0-rmse:24.26759\n[470]\tvalidation_0-rmse:24.27019\n[471]\tvalidation_0-rmse:24.27560\n[472]\tvalidation_0-rmse:24.27396\n[473]\tvalidation_0-rmse:24.27218\n[474]\tvalidation_0-rmse:24.27309\n[475]\tvalidation_0-rmse:24.27487\n[476]\tvalidation_0-rmse:24.27149\n[477]\tvalidation_0-rmse:24.27177\n[478]\tvalidation_0-rmse:24.27104\n[479]\tvalidation_0-rmse:24.27823\n[480]\tvalidation_0-rmse:24.28267\n[481]\tvalidation_0-rmse:24.28126\n[482]\tvalidation_0-rmse:24.27848\n[483]\tvalidation_0-rmse:24.27567\n[484]\tvalidation_0-rmse:24.27424\n[485]\tvalidation_0-rmse:24.27456\n[486]\tvalidation_0-rmse:24.27874\n[487]\tvalidation_0-rmse:24.28278\n[488]\tvalidation_0-rmse:24.27983\n[489]\tvalidation_0-rmse:24.28315\n[490]\tvalidation_0-rmse:24.28430\n[491]\tvalidation_0-rmse:24.28469\n[492]\tvalidation_0-rmse:24.28252\n[493]\tvalidation_0-rmse:24.28793\n[494]\tvalidation_0-rmse:24.29626\n[495]\tvalidation_0-rmse:24.29959\n[496]\tvalidation_0-rmse:24.29874\n[497]\tvalidation_0-rmse:24.30056\n[498]\tvalidation_0-rmse:24.29504\n[499]\tvalidation_0-rmse:24.29739\n[500]\tvalidation_0-rmse:24.29602\n[501]\tvalidation_0-rmse:24.29634\n[502]\tvalidation_0-rmse:24.29499\n[503]\tvalidation_0-rmse:24.29903\n[504]\tvalidation_0-rmse:24.29827\n[505]\tvalidation_0-rmse:24.29711\n[506]\tvalidation_0-rmse:24.29953\n[507]\tvalidation_0-rmse:24.29844\n[508]\tvalidation_0-rmse:24.29893\n[509]\tvalidation_0-rmse:24.30244\n[510]\tvalidation_0-rmse:24.30255\n[511]\tvalidation_0-rmse:24.30289\n[512]\tvalidation_0-rmse:24.30685\n[513]\tvalidation_0-rmse:24.30700\n[514]\tvalidation_0-rmse:24.30746\n[515]\tvalidation_0-rmse:24.31074\n[516]\tvalidation_0-rmse:24.31090\n[517]\tvalidation_0-rmse:24.31118\n[518]\tvalidation_0-rmse:24.31602\n[519]\tvalidation_0-rmse:24.31928\n[520]\tvalidation_0-rmse:24.32373\n[521]\tvalidation_0-rmse:24.32436\n[522]\tvalidation_0-rmse:24.32487\n[523]\tvalidation_0-rmse:24.32574\n[524]\tvalidation_0-rmse:24.32497\n[525]\tvalidation_0-rmse:24.31950\n[526]\tvalidation_0-rmse:24.31861\n[527]\tvalidation_0-rmse:24.31669\n[528]\tvalidation_0-rmse:24.31889\n[529]\tvalidation_0-rmse:24.31630\n[530]\tvalidation_0-rmse:24.31655\n[531]\tvalidation_0-rmse:24.31604\n[532]\tvalidation_0-rmse:24.31840\n[533]\tvalidation_0-rmse:24.31535\n[534]\tvalidation_0-rmse:24.31577\n[535]\tvalidation_0-rmse:24.31556\n[536]\tvalidation_0-rmse:24.31659\n[537]\tvalidation_0-rmse:24.31737\n[538]\tvalidation_0-rmse:24.32162\n[539]\tvalidation_0-rmse:24.32140\n[540]\tvalidation_0-rmse:24.32331\n[541]\tvalidation_0-rmse:24.32476\n[542]\tvalidation_0-rmse:24.32759\n[543]\tvalidation_0-rmse:24.32801\n[544]\tvalidation_0-rmse:24.32688\n[545]\tvalidation_0-rmse:24.32727\n[546]\tvalidation_0-rmse:24.32695\n[547]\tvalidation_0-rmse:24.32919\n[548]\tvalidation_0-rmse:24.33170\n[549]\tvalidation_0-rmse:24.33105\n[550]\tvalidation_0-rmse:24.33495\n[551]\tvalidation_0-rmse:24.33736\n[552]\tvalidation_0-rmse:24.33640\n[553]\tvalidation_0-rmse:24.34096\n[554]\tvalidation_0-rmse:24.33919\n[555]\tvalidation_0-rmse:24.33839\n[556]\tvalidation_0-rmse:24.33442\n[557]\tvalidation_0-rmse:24.33373\n[558]\tvalidation_0-rmse:24.33122\n[559]\tvalidation_0-rmse:24.32306\n[560]\tvalidation_0-rmse:24.32288\n[561]\tvalidation_0-rmse:24.32520\n[562]\tvalidation_0-rmse:24.32719\n[563]\tvalidation_0-rmse:24.32882\n[564]\tvalidation_0-rmse:24.33338\n[565]\tvalidation_0-rmse:24.32965\n[566]\tvalidation_0-rmse:24.32964\n[567]\tvalidation_0-rmse:24.33458\n[568]\tvalidation_0-rmse:24.33612\n[569]\tvalidation_0-rmse:24.33282\n[570]\tvalidation_0-rmse:24.33168\n[571]\tvalidation_0-rmse:24.33322\n[572]\tvalidation_0-rmse:24.33263\n[573]\tvalidation_0-rmse:24.33381\n[574]\tvalidation_0-rmse:24.33667\n[575]\tvalidation_0-rmse:24.33828\n[576]\tvalidation_0-rmse:24.33531\n[577]\tvalidation_0-rmse:24.33207\n[578]\tvalidation_0-rmse:24.33090\n[579]\tvalidation_0-rmse:24.33060\n[580]\tvalidation_0-rmse:24.32967\n[581]\tvalidation_0-rmse:24.33132\n[582]\tvalidation_0-rmse:24.33053\n[583]\tvalidation_0-rmse:24.32760\n[584]\tvalidation_0-rmse:24.32307\n[585]\tvalidation_0-rmse:24.32457\n[586]\tvalidation_0-rmse:24.32449\n[587]\tvalidation_0-rmse:24.32552\n[588]\tvalidation_0-rmse:24.32829\n[589]\tvalidation_0-rmse:24.32540\n[590]\tvalidation_0-rmse:24.32748\n[591]\tvalidation_0-rmse:24.32530\n[592]\tvalidation_0-rmse:24.32626\n[593]\tvalidation_0-rmse:24.32714\n[594]\tvalidation_0-rmse:24.32789\n[595]\tvalidation_0-rmse:24.32634\n[596]\tvalidation_0-rmse:24.32283\n[597]\tvalidation_0-rmse:24.32462\n[598]\tvalidation_0-rmse:24.32699\n[599]\tvalidation_0-rmse:24.32395\n[600]\tvalidation_0-rmse:24.32217\n[601]\tvalidation_0-rmse:24.32125\n[602]\tvalidation_0-rmse:24.31951\n[603]\tvalidation_0-rmse:24.31737\n[604]\tvalidation_0-rmse:24.31973\n[605]\tvalidation_0-rmse:24.31888\n[606]\tvalidation_0-rmse:24.31954\n[607]\tvalidation_0-rmse:24.32307\n[608]\tvalidation_0-rmse:24.32596\n[609]\tvalidation_0-rmse:24.32493\n[610]\tvalidation_0-rmse:24.32264\n[611]\tvalidation_0-rmse:24.32346\n[612]\tvalidation_0-rmse:24.32242\n[613]\tvalidation_0-rmse:24.32637\n[614]\tvalidation_0-rmse:24.32953\n[615]\tvalidation_0-rmse:24.32793\n[616]\tvalidation_0-rmse:24.32748\n[617]\tvalidation_0-rmse:24.32799\n[618]\tvalidation_0-rmse:24.32596\n[619]\tvalidation_0-rmse:24.32826\n[620]\tvalidation_0-rmse:24.32805\n[621]\tvalidation_0-rmse:24.33016\n[622]\tvalidation_0-rmse:24.32878\n[623]\tvalidation_0-rmse:24.33194\n[624]\tvalidation_0-rmse:24.33370\n[625]\tvalidation_0-rmse:24.33553\n[626]\tvalidation_0-rmse:24.33942\n[627]\tvalidation_0-rmse:24.33961\n[628]\tvalidation_0-rmse:24.33998\n[629]\tvalidation_0-rmse:24.33492\n[630]\tvalidation_0-rmse:24.33146\n[631]\tvalidation_0-rmse:24.33349\n[632]\tvalidation_0-rmse:24.33201\n[633]\tvalidation_0-rmse:24.32959\n[634]\tvalidation_0-rmse:24.33356\n[635]\tvalidation_0-rmse:24.33442\n[636]\tvalidation_0-rmse:24.33657\n[637]\tvalidation_0-rmse:24.33602\n[638]\tvalidation_0-rmse:24.33822\n[639]\tvalidation_0-rmse:24.33751\n[640]\tvalidation_0-rmse:24.33697\n[641]\tvalidation_0-rmse:24.34023\n[642]\tvalidation_0-rmse:24.33877\n[643]\tvalidation_0-rmse:24.33833\n[644]\tvalidation_0-rmse:24.33756\n[645]\tvalidation_0-rmse:24.34007\n[646]\tvalidation_0-rmse:24.33927\n[647]\tvalidation_0-rmse:24.33708\n[648]\tvalidation_0-rmse:24.33990\n[649]\tvalidation_0-rmse:24.33663\n[650]\tvalidation_0-rmse:24.33455\n[651]\tvalidation_0-rmse:24.33455\n[652]\tvalidation_0-rmse:24.33361\n[653]\tvalidation_0-rmse:24.33371\n[654]\tvalidation_0-rmse:24.33775\n[655]\tvalidation_0-rmse:24.33963\n[656]\tvalidation_0-rmse:24.33991\n[657]\tvalidation_0-rmse:24.33889\n[658]\tvalidation_0-rmse:24.34108\n[659]\tvalidation_0-rmse:24.33984\n[660]\tvalidation_0-rmse:24.33973\n[661]\tvalidation_0-rmse:24.34035\n[662]\tvalidation_0-rmse:24.34234\n[663]\tvalidation_0-rmse:24.34475\n[664]\tvalidation_0-rmse:24.34747\n[665]\tvalidation_0-rmse:24.34529\n[666]\tvalidation_0-rmse:24.34229\n[667]\tvalidation_0-rmse:24.34209\n[668]\tvalidation_0-rmse:24.34099\n[669]\tvalidation_0-rmse:24.34047\n[670]\tvalidation_0-rmse:24.34073\n[671]\tvalidation_0-rmse:24.33841\n[672]\tvalidation_0-rmse:24.33935\n[673]\tvalidation_0-rmse:24.33681\n[674]\tvalidation_0-rmse:24.33752\n[675]\tvalidation_0-rmse:24.33949\n[676]\tvalidation_0-rmse:24.33686\n[677]\tvalidation_0-rmse:24.33697\n[678]\tvalidation_0-rmse:24.33698\n[679]\tvalidation_0-rmse:24.33965\n[680]\tvalidation_0-rmse:24.33930\n[681]\tvalidation_0-rmse:24.33977\n[682]\tvalidation_0-rmse:24.34281\n[683]\tvalidation_0-rmse:24.33816\n[684]\tvalidation_0-rmse:24.33592\n[685]\tvalidation_0-rmse:24.33877\n[686]\tvalidation_0-rmse:24.33877\n[687]\tvalidation_0-rmse:24.34048\n[688]\tvalidation_0-rmse:24.34409\n[689]\tvalidation_0-rmse:24.34494\n[690]\tvalidation_0-rmse:24.34426\n[691]\tvalidation_0-rmse:24.34553\n[692]\tvalidation_0-rmse:24.34280\n[693]\tvalidation_0-rmse:24.34678\n[694]\tvalidation_0-rmse:24.34887\n[695]\tvalidation_0-rmse:24.35060\n[696]\tvalidation_0-rmse:24.35345\n[697]\tvalidation_0-rmse:24.35345\n[698]\tvalidation_0-rmse:24.35521\n[699]\tvalidation_0-rmse:24.35392\n[700]\tvalidation_0-rmse:24.35370\n[701]\tvalidation_0-rmse:24.35415\n[702]\tvalidation_0-rmse:24.35546\n[703]\tvalidation_0-rmse:24.35623\n[704]\tvalidation_0-rmse:24.35644\n[705]\tvalidation_0-rmse:24.36123\n[706]\tvalidation_0-rmse:24.36341\n[707]\tvalidation_0-rmse:24.36199\n[708]\tvalidation_0-rmse:24.35967\n[709]\tvalidation_0-rmse:24.35991\n[710]\tvalidation_0-rmse:24.35945\n[711]\tvalidation_0-rmse:24.35909\n[712]\tvalidation_0-rmse:24.36120\n[713]\tvalidation_0-rmse:24.36306\n[714]\tvalidation_0-rmse:24.36296\n[715]\tvalidation_0-rmse:24.36489\n[716]\tvalidation_0-rmse:24.36490\n[717]\tvalidation_0-rmse:24.36764\n[718]\tvalidation_0-rmse:24.37048\n[719]\tvalidation_0-rmse:24.37175\n[720]\tvalidation_0-rmse:24.37298\n[721]\tvalidation_0-rmse:24.37005\n[722]\tvalidation_0-rmse:24.37123\n[723]\tvalidation_0-rmse:24.37376\n[724]\tvalidation_0-rmse:24.37463\n[725]\tvalidation_0-rmse:24.37321\n[726]\tvalidation_0-rmse:24.37516\n[727]\tvalidation_0-rmse:24.37215\n[728]\tvalidation_0-rmse:24.37535\n[729]\tvalidation_0-rmse:24.37581\n[730]\tvalidation_0-rmse:24.37642\n[731]\tvalidation_0-rmse:24.37739\n[732]\tvalidation_0-rmse:24.37107\n[733]\tvalidation_0-rmse:24.37097\n[734]\tvalidation_0-rmse:24.37175\n[735]\tvalidation_0-rmse:24.37641\n[736]\tvalidation_0-rmse:24.37592\n[737]\tvalidation_0-rmse:24.37944\n[738]\tvalidation_0-rmse:24.38044\n[739]\tvalidation_0-rmse:24.37927\n[740]\tvalidation_0-rmse:24.37836\n[741]\tvalidation_0-rmse:24.37825\n[742]\tvalidation_0-rmse:24.37769\n[743]\tvalidation_0-rmse:24.37953\n[744]\tvalidation_0-rmse:24.37996\n[745]\tvalidation_0-rmse:24.38175\n[746]\tvalidation_0-rmse:24.38360\n[747]\tvalidation_0-rmse:24.38256\n[748]\tvalidation_0-rmse:24.38365\n[749]\tvalidation_0-rmse:24.38284\n[750]\tvalidation_0-rmse:24.37974\n[751]\tvalidation_0-rmse:24.38314\n[752]\tvalidation_0-rmse:24.38394\n[753]\tvalidation_0-rmse:24.38288\n[754]\tvalidation_0-rmse:24.38192\n[755]\tvalidation_0-rmse:24.38214\n[756]\tvalidation_0-rmse:24.38364\n[757]\tvalidation_0-rmse:24.38155\n[758]\tvalidation_0-rmse:24.37808\n[759]\tvalidation_0-rmse:24.37821\n[760]\tvalidation_0-rmse:24.37539\n[761]\tvalidation_0-rmse:24.37543\n[762]\tvalidation_0-rmse:24.38008\n[763]\tvalidation_0-rmse:24.37951\n[764]\tvalidation_0-rmse:24.37857\n[765]\tvalidation_0-rmse:24.38102\n[766]\tvalidation_0-rmse:24.38208\n[767]\tvalidation_0-rmse:24.38373\n[768]\tvalidation_0-rmse:24.38425\n[769]\tvalidation_0-rmse:24.38390\n[770]\tvalidation_0-rmse:24.38260\n[771]\tvalidation_0-rmse:24.37996\n[772]\tvalidation_0-rmse:24.38042\n[773]\tvalidation_0-rmse:24.38114\n[774]\tvalidation_0-rmse:24.38300\n[775]\tvalidation_0-rmse:24.38087\n[776]\tvalidation_0-rmse:24.38087\n[777]\tvalidation_0-rmse:24.37966\n[778]\tvalidation_0-rmse:24.38047\n[779]\tvalidation_0-rmse:24.38310\n[780]\tvalidation_0-rmse:24.38137\n[781]\tvalidation_0-rmse:24.38315\n[782]\tvalidation_0-rmse:24.38645\n[783]\tvalidation_0-rmse:24.38518\n[784]\tvalidation_0-rmse:24.38289\n[785]\tvalidation_0-rmse:24.38427\n[786]\tvalidation_0-rmse:24.38339\n[787]\tvalidation_0-rmse:24.38583\n[788]\tvalidation_0-rmse:24.37954\n[789]\tvalidation_0-rmse:24.38139\n[790]\tvalidation_0-rmse:24.38581\n[791]\tvalidation_0-rmse:24.38515\n[792]\tvalidation_0-rmse:24.38588\n[793]\tvalidation_0-rmse:24.38343\n[794]\tvalidation_0-rmse:24.38174\n[795]\tvalidation_0-rmse:24.38029\n[796]\tvalidation_0-rmse:24.38145\n[797]\tvalidation_0-rmse:24.38197\n[798]\tvalidation_0-rmse:24.38203\n[799]\tvalidation_0-rmse:24.38481\n[800]\tvalidation_0-rmse:24.38423\n[801]\tvalidation_0-rmse:24.38388\n[802]\tvalidation_0-rmse:24.38580\n[803]\tvalidation_0-rmse:24.38635\n[804]\tvalidation_0-rmse:24.38508\n[805]\tvalidation_0-rmse:24.38555\n[806]\tvalidation_0-rmse:24.38488\n[807]\tvalidation_0-rmse:24.38275\n[808]\tvalidation_0-rmse:24.38070\n[809]\tvalidation_0-rmse:24.37914\n[810]\tvalidation_0-rmse:24.37772\n[811]\tvalidation_0-rmse:24.37741\n[812]\tvalidation_0-rmse:24.37956\n[813]\tvalidation_0-rmse:24.38217\n[814]\tvalidation_0-rmse:24.38361\n[815]\tvalidation_0-rmse:24.38543\n[816]\tvalidation_0-rmse:24.38122\n[817]\tvalidation_0-rmse:24.38142\n[818]\tvalidation_0-rmse:24.38015\n[819]\tvalidation_0-rmse:24.37843\n[820]\tvalidation_0-rmse:24.37694\n[821]\tvalidation_0-rmse:24.37805\n[822]\tvalidation_0-rmse:24.37750\n[823]\tvalidation_0-rmse:24.37793\n[824]\tvalidation_0-rmse:24.37785\n[825]\tvalidation_0-rmse:24.37822\n[826]\tvalidation_0-rmse:24.37890\n[827]\tvalidation_0-rmse:24.37696\n[828]\tvalidation_0-rmse:24.37497\n[829]\tvalidation_0-rmse:24.37439\n[830]\tvalidation_0-rmse:24.37670\n[831]\tvalidation_0-rmse:24.37501\n[832]\tvalidation_0-rmse:24.37652\n[833]\tvalidation_0-rmse:24.37649\n[834]\tvalidation_0-rmse:24.37616\n[835]\tvalidation_0-rmse:24.37753\n[836]\tvalidation_0-rmse:24.37737\n[837]\tvalidation_0-rmse:24.37972\n[838]\tvalidation_0-rmse:24.38089\n[839]\tvalidation_0-rmse:24.38193\n[840]\tvalidation_0-rmse:24.38485\n[841]\tvalidation_0-rmse:24.38279\n[842]\tvalidation_0-rmse:24.37997\n[843]\tvalidation_0-rmse:24.37302\n[844]\tvalidation_0-rmse:24.37588\n[845]\tvalidation_0-rmse:24.37556\n[846]\tvalidation_0-rmse:24.37417\n[847]\tvalidation_0-rmse:24.37510\n[848]\tvalidation_0-rmse:24.37713\n[849]\tvalidation_0-rmse:24.37413\n[850]\tvalidation_0-rmse:24.37478\n[851]\tvalidation_0-rmse:24.37906\n[852]\tvalidation_0-rmse:24.38140\n[853]\tvalidation_0-rmse:24.38188\n[854]\tvalidation_0-rmse:24.38261\n[855]\tvalidation_0-rmse:24.38185\n[856]\tvalidation_0-rmse:24.38389\n[857]\tvalidation_0-rmse:24.38139\n[858]\tvalidation_0-rmse:24.38028\n[859]\tvalidation_0-rmse:24.38133\n[860]\tvalidation_0-rmse:24.37905\n[861]\tvalidation_0-rmse:24.37872\n[862]\tvalidation_0-rmse:24.37881\n[863]\tvalidation_0-rmse:24.37725\n[864]\tvalidation_0-rmse:24.37567\n[865]\tvalidation_0-rmse:24.37570\n[866]\tvalidation_0-rmse:24.37480\n[867]\tvalidation_0-rmse:24.37706\n[868]\tvalidation_0-rmse:24.37617\n[869]\tvalidation_0-rmse:24.37646\n[870]\tvalidation_0-rmse:24.37876\n[871]\tvalidation_0-rmse:24.37842\n[872]\tvalidation_0-rmse:24.37638\n[873]\tvalidation_0-rmse:24.37682\n[874]\tvalidation_0-rmse:24.37865\n[875]\tvalidation_0-rmse:24.37547\n[876]\tvalidation_0-rmse:24.37534\n[877]\tvalidation_0-rmse:24.37590\n[878]\tvalidation_0-rmse:24.37633\n[879]\tvalidation_0-rmse:24.37514\n[880]\tvalidation_0-rmse:24.37462\n[881]\tvalidation_0-rmse:24.37671\n[882]\tvalidation_0-rmse:24.37739\n[883]\tvalidation_0-rmse:24.37754\n[884]\tvalidation_0-rmse:24.38038\n[885]\tvalidation_0-rmse:24.37924\n[886]\tvalidation_0-rmse:24.37907\n[887]\tvalidation_0-rmse:24.37728\n[888]\tvalidation_0-rmse:24.37852\n[889]\tvalidation_0-rmse:24.38037\n[890]\tvalidation_0-rmse:24.38011\n[891]\tvalidation_0-rmse:24.38226\n[892]\tvalidation_0-rmse:24.38354\n[893]\tvalidation_0-rmse:24.38489\n[894]\tvalidation_0-rmse:24.38482\n[895]\tvalidation_0-rmse:24.38432\n[896]\tvalidation_0-rmse:24.38401\n[897]\tvalidation_0-rmse:24.38329\n[898]\tvalidation_0-rmse:24.38376\n[899]\tvalidation_0-rmse:24.38618\n[900]\tvalidation_0-rmse:24.38829\n[901]\tvalidation_0-rmse:24.39095\n[902]\tvalidation_0-rmse:24.38973\n[903]\tvalidation_0-rmse:24.38840\n[904]\tvalidation_0-rmse:24.38713\n[905]\tvalidation_0-rmse:24.38474\n[906]\tvalidation_0-rmse:24.38372\n[907]\tvalidation_0-rmse:24.37991\n[908]\tvalidation_0-rmse:24.38049\n[909]\tvalidation_0-rmse:24.37824\n[910]\tvalidation_0-rmse:24.37944\n[911]\tvalidation_0-rmse:24.37792\n[912]\tvalidation_0-rmse:24.37800\n[913]\tvalidation_0-rmse:24.37935\n[914]\tvalidation_0-rmse:24.37924\n[915]\tvalidation_0-rmse:24.37865\n[916]\tvalidation_0-rmse:24.37855\n[917]\tvalidation_0-rmse:24.37935\n[918]\tvalidation_0-rmse:24.37820\n[919]\tvalidation_0-rmse:24.37555\n[920]\tvalidation_0-rmse:24.37233\n[921]\tvalidation_0-rmse:24.37169\n[922]\tvalidation_0-rmse:24.37218\n[923]\tvalidation_0-rmse:24.37451\n[924]\tvalidation_0-rmse:24.37242\n[925]\tvalidation_0-rmse:24.37139\n[926]\tvalidation_0-rmse:24.37168\n[927]\tvalidation_0-rmse:24.37096\n[928]\tvalidation_0-rmse:24.37200\n[929]\tvalidation_0-rmse:24.37023\n[930]\tvalidation_0-rmse:24.36721\n[931]\tvalidation_0-rmse:24.36769\n[932]\tvalidation_0-rmse:24.36918\n[933]\tvalidation_0-rmse:24.37015\n[934]\tvalidation_0-rmse:24.37031\n[935]\tvalidation_0-rmse:24.36984\n[936]\tvalidation_0-rmse:24.37018\n[937]\tvalidation_0-rmse:24.36808\n[938]\tvalidation_0-rmse:24.36852\n[939]\tvalidation_0-rmse:24.36786\n[940]\tvalidation_0-rmse:24.36887\n[941]\tvalidation_0-rmse:24.36798\n[942]\tvalidation_0-rmse:24.36709\n[943]\tvalidation_0-rmse:24.36384\n[944]\tvalidation_0-rmse:24.36363\n[945]\tvalidation_0-rmse:24.36375\n[946]\tvalidation_0-rmse:24.36352\n[947]\tvalidation_0-rmse:24.36319\n[948]\tvalidation_0-rmse:24.35945\n[949]\tvalidation_0-rmse:24.36084\n[950]\tvalidation_0-rmse:24.36018\n[951]\tvalidation_0-rmse:24.36067\n[952]\tvalidation_0-rmse:24.36230\n[953]\tvalidation_0-rmse:24.36296\n[954]\tvalidation_0-rmse:24.36411\n[955]\tvalidation_0-rmse:24.36300\n[956]\tvalidation_0-rmse:24.36306\n[957]\tvalidation_0-rmse:24.36529\n[958]\tvalidation_0-rmse:24.36569\n[959]\tvalidation_0-rmse:24.36736\n[960]\tvalidation_0-rmse:24.36753\n[961]\tvalidation_0-rmse:24.36735\n[962]\tvalidation_0-rmse:24.36543\n[963]\tvalidation_0-rmse:24.36653\n[964]\tvalidation_0-rmse:24.36804\n[965]\tvalidation_0-rmse:24.36809\n[966]\tvalidation_0-rmse:24.36973\n[967]\tvalidation_0-rmse:24.37180\n[968]\tvalidation_0-rmse:24.37326\n[969]\tvalidation_0-rmse:24.37293\n[970]\tvalidation_0-rmse:24.37046\n[971]\tvalidation_0-rmse:24.37140\n[972]\tvalidation_0-rmse:24.37303\n[973]\tvalidation_0-rmse:24.37487\n[974]\tvalidation_0-rmse:24.37578\n[975]\tvalidation_0-rmse:24.37546\n[976]\tvalidation_0-rmse:24.37680\n[977]\tvalidation_0-rmse:24.37455\n[978]\tvalidation_0-rmse:24.37635\n[979]\tvalidation_0-rmse:24.37699\n[980]\tvalidation_0-rmse:24.37537\n[981]\tvalidation_0-rmse:24.37513\n[982]\tvalidation_0-rmse:24.37419\n[983]\tvalidation_0-rmse:24.37317\n[984]\tvalidation_0-rmse:24.37365\n[985]\tvalidation_0-rmse:24.37559\n[986]\tvalidation_0-rmse:24.37566\n[987]\tvalidation_0-rmse:24.37725\n[988]\tvalidation_0-rmse:24.37838\n[989]\tvalidation_0-rmse:24.37830\n[990]\tvalidation_0-rmse:24.37708\n[991]\tvalidation_0-rmse:24.37757\n[992]\tvalidation_0-rmse:24.37645\n[993]\tvalidation_0-rmse:24.37487\n[994]\tvalidation_0-rmse:24.37574\n[995]\tvalidation_0-rmse:24.37491\n[996]\tvalidation_0-rmse:24.37548\n[997]\tvalidation_0-rmse:24.37680\n[998]\tvalidation_0-rmse:24.37533\n[999]\tvalidation_0-rmse:24.37274\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3131\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 767\n[LightGBM] [Info] Start training from score 33.229167\n[0]\tvalidation_0-rmse:27.30751\n[1]\tvalidation_0-rmse:27.18810\n[2]\tvalidation_0-rmse:27.07337\n[3]\tvalidation_0-rmse:26.96694\n[4]\tvalidation_0-rmse:26.86228\n[5]\tvalidation_0-rmse:26.74931\n[6]\tvalidation_0-rmse:26.66231\n[7]\tvalidation_0-rmse:26.57414\n[8]\tvalidation_0-rmse:26.49559\n[9]\tvalidation_0-rmse:26.42295\n[10]\tvalidation_0-rmse:26.36446\n[11]\tvalidation_0-rmse:26.29444\n[12]\tvalidation_0-rmse:26.24655\n[13]\tvalidation_0-rmse:26.15081\n[14]\tvalidation_0-rmse:26.10455\n[15]\tvalidation_0-rmse:26.05633\n[16]\tvalidation_0-rmse:26.01401\n[17]\tvalidation_0-rmse:25.95537\n[18]\tvalidation_0-rmse:25.88827\n[19]\tvalidation_0-rmse:25.84558\n[20]\tvalidation_0-rmse:25.78894\n[21]\tvalidation_0-rmse:25.74772\n[22]\tvalidation_0-rmse:25.69845\n[23]\tvalidation_0-rmse:25.64136\n[24]\tvalidation_0-rmse:25.60826\n[25]\tvalidation_0-rmse:25.55559\n[26]\tvalidation_0-rmse:25.52090\n[27]\tvalidation_0-rmse:25.48719\n[28]\tvalidation_0-rmse:25.45695\n[29]\tvalidation_0-rmse:25.45188\n[30]\tvalidation_0-rmse:25.40589\n[31]\tvalidation_0-rmse:25.39018\n[32]\tvalidation_0-rmse:25.34174\n[33]\tvalidation_0-rmse:25.30458\n[34]\tvalidation_0-rmse:25.27231\n[35]\tvalidation_0-rmse:25.25834\n[36]\tvalidation_0-rmse:25.22980\n[37]\tvalidation_0-rmse:25.20623\n[38]\tvalidation_0-rmse:25.17295\n[39]\tvalidation_0-rmse:25.14653\n[40]\tvalidation_0-rmse:25.13438\n[41]\tvalidation_0-rmse:25.10442\n[42]\tvalidation_0-rmse:25.10497\n[43]\tvalidation_0-rmse:25.09254\n[44]\tvalidation_0-rmse:25.06465\n[45]\tvalidation_0-rmse:25.05349\n[46]\tvalidation_0-rmse:25.05792\n[47]\tvalidation_0-rmse:25.06044\n[48]\tvalidation_0-rmse:25.04018\n[49]\tvalidation_0-rmse:25.04023\n[50]\tvalidation_0-rmse:25.03840\n[51]\tvalidation_0-rmse:25.02810\n[52]\tvalidation_0-rmse:25.00381\n[53]\tvalidation_0-rmse:24.98702\n[54]\tvalidation_0-rmse:24.97025\n[55]\tvalidation_0-rmse:24.96549\n[56]\tvalidation_0-rmse:24.95141\n[57]\tvalidation_0-rmse:24.94728\n[58]\tvalidation_0-rmse:24.93994\n[59]\tvalidation_0-rmse:24.94546\n[60]\tvalidation_0-rmse:24.92823\n[61]\tvalidation_0-rmse:24.92810\n[62]\tvalidation_0-rmse:24.91851\n[63]\tvalidation_0-rmse:24.89571\n[64]\tvalidation_0-rmse:24.87501\n[65]\tvalidation_0-rmse:24.87191\n[66]\tvalidation_0-rmse:24.85034\n[67]\tvalidation_0-rmse:24.86345\n[68]\tvalidation_0-rmse:24.84048\n[69]\tvalidation_0-rmse:24.81236\n[70]\tvalidation_0-rmse:24.80214\n[71]\tvalidation_0-rmse:24.79306\n[72]\tvalidation_0-rmse:24.77741\n[73]\tvalidation_0-rmse:24.76505\n[74]\tvalidation_0-rmse:24.75732\n[75]\tvalidation_0-rmse:24.75305\n[76]\tvalidation_0-rmse:24.74857\n[77]\tvalidation_0-rmse:24.74340\n[78]\tvalidation_0-rmse:24.73721\n[79]\tvalidation_0-rmse:24.71941\n[80]\tvalidation_0-rmse:24.72210\n[81]\tvalidation_0-rmse:24.71652\n[82]\tvalidation_0-rmse:24.70596\n[83]\tvalidation_0-rmse:24.68277\n[84]\tvalidation_0-rmse:24.67943\n[85]\tvalidation_0-rmse:24.67404\n[86]\tvalidation_0-rmse:24.66921\n[87]\tvalidation_0-rmse:24.68776\n[88]\tvalidation_0-rmse:24.67495\n[89]\tvalidation_0-rmse:24.67037\n[90]\tvalidation_0-rmse:24.66468\n[91]\tvalidation_0-rmse:24.65872\n[92]\tvalidation_0-rmse:24.66568\n[93]\tvalidation_0-rmse:24.68151\n[94]\tvalidation_0-rmse:24.68981\n[95]\tvalidation_0-rmse:24.68385\n[96]\tvalidation_0-rmse:24.68456\n[97]\tvalidation_0-rmse:24.67965\n[98]\tvalidation_0-rmse:24.68214\n[99]\tvalidation_0-rmse:24.67807\n[100]\tvalidation_0-rmse:24.67837\n[101]\tvalidation_0-rmse:24.69784\n[102]\tvalidation_0-rmse:24.69087\n[103]\tvalidation_0-rmse:24.69347\n[104]\tvalidation_0-rmse:24.69532\n[105]\tvalidation_0-rmse:24.68987\n[106]\tvalidation_0-rmse:24.70097\n[107]\tvalidation_0-rmse:24.69352\n[108]\tvalidation_0-rmse:24.70309\n[109]\tvalidation_0-rmse:24.70655\n[110]\tvalidation_0-rmse:24.69476\n[111]\tvalidation_0-rmse:24.68343\n[112]\tvalidation_0-rmse:24.68711\n[113]\tvalidation_0-rmse:24.68483\n[114]\tvalidation_0-rmse:24.69228\n[115]\tvalidation_0-rmse:24.69074\n[116]\tvalidation_0-rmse:24.69159\n[117]\tvalidation_0-rmse:24.68152\n[118]\tvalidation_0-rmse:24.68392\n[119]\tvalidation_0-rmse:24.67600\n[120]\tvalidation_0-rmse:24.68831\n[121]\tvalidation_0-rmse:24.68494\n[122]\tvalidation_0-rmse:24.69282\n[123]\tvalidation_0-rmse:24.70061\n[124]\tvalidation_0-rmse:24.70273\n[125]\tvalidation_0-rmse:24.70096\n[126]\tvalidation_0-rmse:24.70301\n[127]\tvalidation_0-rmse:24.69299\n[128]\tvalidation_0-rmse:24.70095\n[129]\tvalidation_0-rmse:24.69977\n[130]\tvalidation_0-rmse:24.70508\n[131]\tvalidation_0-rmse:24.70188\n[132]\tvalidation_0-rmse:24.69296\n[133]\tvalidation_0-rmse:24.70012\n[134]\tvalidation_0-rmse:24.69770\n[135]\tvalidation_0-rmse:24.69165\n[136]\tvalidation_0-rmse:24.70074\n[137]\tvalidation_0-rmse:24.69992\n[138]\tvalidation_0-rmse:24.70033\n[139]\tvalidation_0-rmse:24.70612\n[140]\tvalidation_0-rmse:24.69906\n[141]\tvalidation_0-rmse:24.71605\n[142]\tvalidation_0-rmse:24.71144\n[143]\tvalidation_0-rmse:24.70742\n[144]\tvalidation_0-rmse:24.71398\n[145]\tvalidation_0-rmse:24.70476\n[146]\tvalidation_0-rmse:24.69773\n[147]\tvalidation_0-rmse:24.70506\n[148]\tvalidation_0-rmse:24.69801\n[149]\tvalidation_0-rmse:24.70131\n[150]\tvalidation_0-rmse:24.70342\n[151]\tvalidation_0-rmse:24.69818\n[152]\tvalidation_0-rmse:24.70010\n[153]\tvalidation_0-rmse:24.70363\n[154]\tvalidation_0-rmse:24.70098\n[155]\tvalidation_0-rmse:24.70190\n[156]\tvalidation_0-rmse:24.70214\n[157]\tvalidation_0-rmse:24.71666\n[158]\tvalidation_0-rmse:24.72062\n[159]\tvalidation_0-rmse:24.71924\n[160]\tvalidation_0-rmse:24.72218\n[161]\tvalidation_0-rmse:24.72142\n[162]\tvalidation_0-rmse:24.72498\n[163]\tvalidation_0-rmse:24.72319\n[164]\tvalidation_0-rmse:24.72874\n[165]\tvalidation_0-rmse:24.73467\n[166]\tvalidation_0-rmse:24.72592\n[167]\tvalidation_0-rmse:24.72680\n[168]\tvalidation_0-rmse:24.72254\n[169]\tvalidation_0-rmse:24.72618\n[170]\tvalidation_0-rmse:24.72667\n[171]\tvalidation_0-rmse:24.72712\n[172]\tvalidation_0-rmse:24.73247\n[173]\tvalidation_0-rmse:24.73151\n[174]\tvalidation_0-rmse:24.73378\n[175]\tvalidation_0-rmse:24.73632\n[176]\tvalidation_0-rmse:24.74233\n[177]\tvalidation_0-rmse:24.74587\n[178]\tvalidation_0-rmse:24.74229\n[179]\tvalidation_0-rmse:24.73988\n[180]\tvalidation_0-rmse:24.74281\n[181]\tvalidation_0-rmse:24.74630\n[182]\tvalidation_0-rmse:24.74973\n[183]\tvalidation_0-rmse:24.75096\n[184]\tvalidation_0-rmse:24.75447\n[185]\tvalidation_0-rmse:24.75483\n[186]\tvalidation_0-rmse:24.75309\n[187]\tvalidation_0-rmse:24.76123\n[188]\tvalidation_0-rmse:24.76378\n[189]\tvalidation_0-rmse:24.76388\n[190]\tvalidation_0-rmse:24.76600\n[191]\tvalidation_0-rmse:24.77324\n[192]\tvalidation_0-rmse:24.77349\n[193]\tvalidation_0-rmse:24.77341\n[194]\tvalidation_0-rmse:24.77089\n[195]\tvalidation_0-rmse:24.77388\n[196]\tvalidation_0-rmse:24.76899\n[197]\tvalidation_0-rmse:24.76284\n[198]\tvalidation_0-rmse:24.76223\n[199]\tvalidation_0-rmse:24.76207\n[200]\tvalidation_0-rmse:24.76436\n[201]\tvalidation_0-rmse:24.76736\n[202]\tvalidation_0-rmse:24.77285\n[203]\tvalidation_0-rmse:24.76804\n[204]\tvalidation_0-rmse:24.77597\n[205]\tvalidation_0-rmse:24.78106\n[206]\tvalidation_0-rmse:24.78235\n[207]\tvalidation_0-rmse:24.79127\n[208]\tvalidation_0-rmse:24.78779\n[209]\tvalidation_0-rmse:24.79055\n[210]\tvalidation_0-rmse:24.79009\n[211]\tvalidation_0-rmse:24.79568\n[212]\tvalidation_0-rmse:24.79357\n[213]\tvalidation_0-rmse:24.80078\n[214]\tvalidation_0-rmse:24.81317\n[215]\tvalidation_0-rmse:24.81836\n[216]\tvalidation_0-rmse:24.81914\n[217]\tvalidation_0-rmse:24.81457\n[218]\tvalidation_0-rmse:24.81811\n[219]\tvalidation_0-rmse:24.81525\n[220]\tvalidation_0-rmse:24.81476\n[221]\tvalidation_0-rmse:24.81762\n[222]\tvalidation_0-rmse:24.81421\n[223]\tvalidation_0-rmse:24.81777\n[224]\tvalidation_0-rmse:24.82102\n[225]\tvalidation_0-rmse:24.83440\n[226]\tvalidation_0-rmse:24.84147\n[227]\tvalidation_0-rmse:24.84291\n[228]\tvalidation_0-rmse:24.84669\n[229]\tvalidation_0-rmse:24.84168\n[230]\tvalidation_0-rmse:24.83872\n[231]\tvalidation_0-rmse:24.83962\n[232]\tvalidation_0-rmse:24.83916\n[233]\tvalidation_0-rmse:24.84597\n[234]\tvalidation_0-rmse:24.84674\n[235]\tvalidation_0-rmse:24.85363\n[236]\tvalidation_0-rmse:24.85793\n[237]\tvalidation_0-rmse:24.86092\n[238]\tvalidation_0-rmse:24.86315\n[239]\tvalidation_0-rmse:24.86647\n[240]\tvalidation_0-rmse:24.86736\n[241]\tvalidation_0-rmse:24.86675\n[242]\tvalidation_0-rmse:24.87182\n[243]\tvalidation_0-rmse:24.87773\n[244]\tvalidation_0-rmse:24.88690\n[245]\tvalidation_0-rmse:24.88821\n[246]\tvalidation_0-rmse:24.89171\n[247]\tvalidation_0-rmse:24.89401\n[248]\tvalidation_0-rmse:24.89154\n[249]\tvalidation_0-rmse:24.90329\n[250]\tvalidation_0-rmse:24.90518\n[251]\tvalidation_0-rmse:24.90121\n[252]\tvalidation_0-rmse:24.90431\n[253]\tvalidation_0-rmse:24.90227\n[254]\tvalidation_0-rmse:24.90542\n[255]\tvalidation_0-rmse:24.90972\n[256]\tvalidation_0-rmse:24.91070\n[257]\tvalidation_0-rmse:24.91067\n[258]\tvalidation_0-rmse:24.90919\n[259]\tvalidation_0-rmse:24.91322\n[260]\tvalidation_0-rmse:24.91765\n[261]\tvalidation_0-rmse:24.92569\n[262]\tvalidation_0-rmse:24.93348\n[263]\tvalidation_0-rmse:24.92985\n[264]\tvalidation_0-rmse:24.93950\n[265]\tvalidation_0-rmse:24.93852\n[266]\tvalidation_0-rmse:24.93231\n[267]\tvalidation_0-rmse:24.93927\n[268]\tvalidation_0-rmse:24.94818\n[269]\tvalidation_0-rmse:24.95113\n[270]\tvalidation_0-rmse:24.95841\n[271]\tvalidation_0-rmse:24.96253\n[272]\tvalidation_0-rmse:24.97032\n[273]\tvalidation_0-rmse:24.97424\n[274]\tvalidation_0-rmse:24.97816\n[275]\tvalidation_0-rmse:24.97963\n[276]\tvalidation_0-rmse:24.97982\n[277]\tvalidation_0-rmse:24.98163\n[278]\tvalidation_0-rmse:24.98410\n[279]\tvalidation_0-rmse:24.98514\n[280]\tvalidation_0-rmse:24.99233\n[281]\tvalidation_0-rmse:24.98888\n[282]\tvalidation_0-rmse:24.99541\n[283]\tvalidation_0-rmse:24.99095\n[284]\tvalidation_0-rmse:24.99083\n[285]\tvalidation_0-rmse:24.98912\n[286]\tvalidation_0-rmse:24.98741\n[287]\tvalidation_0-rmse:24.99628\n[288]\tvalidation_0-rmse:25.00485\n[289]\tvalidation_0-rmse:25.00336\n[290]\tvalidation_0-rmse:25.00689\n[291]\tvalidation_0-rmse:24.99941\n[292]\tvalidation_0-rmse:25.00425\n[293]\tvalidation_0-rmse:25.00258\n[294]\tvalidation_0-rmse:25.00182\n[295]\tvalidation_0-rmse:25.00685\n[296]\tvalidation_0-rmse:25.00782\n[297]\tvalidation_0-rmse:25.01136\n[298]\tvalidation_0-rmse:25.01638\n[299]\tvalidation_0-rmse:25.01385\n[300]\tvalidation_0-rmse:25.01272\n[301]\tvalidation_0-rmse:25.01249\n[302]\tvalidation_0-rmse:25.01118\n[303]\tvalidation_0-rmse:25.01332\n[304]\tvalidation_0-rmse:25.00742\n[305]\tvalidation_0-rmse:25.00830\n[306]\tvalidation_0-rmse:25.00953\n[307]\tvalidation_0-rmse:25.00989\n[308]\tvalidation_0-rmse:25.00844\n[309]\tvalidation_0-rmse:25.00606\n[310]\tvalidation_0-rmse:24.99799\n[311]\tvalidation_0-rmse:25.00655\n[312]\tvalidation_0-rmse:25.01605\n[313]\tvalidation_0-rmse:25.01913\n[314]\tvalidation_0-rmse:25.02236\n[315]\tvalidation_0-rmse:25.02454\n[316]\tvalidation_0-rmse:25.03175\n[317]\tvalidation_0-rmse:25.03282\n[318]\tvalidation_0-rmse:25.03831\n[319]\tvalidation_0-rmse:25.03773\n[320]\tvalidation_0-rmse:25.03896\n[321]\tvalidation_0-rmse:25.03825\n[322]\tvalidation_0-rmse:25.04134\n[323]\tvalidation_0-rmse:25.03586\n[324]\tvalidation_0-rmse:25.04066\n[325]\tvalidation_0-rmse:25.04006\n[326]\tvalidation_0-rmse:25.03894\n[327]\tvalidation_0-rmse:25.04143\n[328]\tvalidation_0-rmse:25.04387\n[329]\tvalidation_0-rmse:25.04307\n[330]\tvalidation_0-rmse:25.04774\n[331]\tvalidation_0-rmse:25.04565\n[332]\tvalidation_0-rmse:25.05327\n[333]\tvalidation_0-rmse:25.05709\n[334]\tvalidation_0-rmse:25.05574\n[335]\tvalidation_0-rmse:25.05659\n[336]\tvalidation_0-rmse:25.05867\n[337]\tvalidation_0-rmse:25.05735\n[338]\tvalidation_0-rmse:25.06086\n[339]\tvalidation_0-rmse:25.06154\n[340]\tvalidation_0-rmse:25.05789\n[341]\tvalidation_0-rmse:25.05568\n[342]\tvalidation_0-rmse:25.05924\n[343]\tvalidation_0-rmse:25.05834\n[344]\tvalidation_0-rmse:25.06393\n[345]\tvalidation_0-rmse:25.06951\n[346]\tvalidation_0-rmse:25.07217\n[347]\tvalidation_0-rmse:25.07423\n[348]\tvalidation_0-rmse:25.08156\n[349]\tvalidation_0-rmse:25.08365\n[350]\tvalidation_0-rmse:25.08386\n[351]\tvalidation_0-rmse:25.09024\n[352]\tvalidation_0-rmse:25.09272\n[353]\tvalidation_0-rmse:25.09220\n[354]\tvalidation_0-rmse:25.08975\n[355]\tvalidation_0-rmse:25.08845\n[356]\tvalidation_0-rmse:25.08962\n[357]\tvalidation_0-rmse:25.09414\n[358]\tvalidation_0-rmse:25.09458\n[359]\tvalidation_0-rmse:25.09243\n[360]\tvalidation_0-rmse:25.09596\n[361]\tvalidation_0-rmse:25.10045\n[362]\tvalidation_0-rmse:25.09657\n[363]\tvalidation_0-rmse:25.10134\n[364]\tvalidation_0-rmse:25.10358\n[365]\tvalidation_0-rmse:25.09362\n[366]\tvalidation_0-rmse:25.10213\n[367]\tvalidation_0-rmse:25.10170\n[368]\tvalidation_0-rmse:25.09747\n[369]\tvalidation_0-rmse:25.10030\n[370]\tvalidation_0-rmse:25.10931\n[371]\tvalidation_0-rmse:25.10972\n[372]\tvalidation_0-rmse:25.11088\n[373]\tvalidation_0-rmse:25.11242\n[374]\tvalidation_0-rmse:25.11269\n[375]\tvalidation_0-rmse:25.11629\n[376]\tvalidation_0-rmse:25.11354\n[377]\tvalidation_0-rmse:25.11517\n[378]\tvalidation_0-rmse:25.11337\n[379]\tvalidation_0-rmse:25.11806\n[380]\tvalidation_0-rmse:25.11761\n[381]\tvalidation_0-rmse:25.12029\n[382]\tvalidation_0-rmse:25.11628\n[383]\tvalidation_0-rmse:25.11824\n[384]\tvalidation_0-rmse:25.11749\n[385]\tvalidation_0-rmse:25.11931\n[386]\tvalidation_0-rmse:25.12432\n[387]\tvalidation_0-rmse:25.12308\n[388]\tvalidation_0-rmse:25.12304\n[389]\tvalidation_0-rmse:25.12284\n[390]\tvalidation_0-rmse:25.12406\n[391]\tvalidation_0-rmse:25.12647\n[392]\tvalidation_0-rmse:25.12288\n[393]\tvalidation_0-rmse:25.12177\n[394]\tvalidation_0-rmse:25.12472\n[395]\tvalidation_0-rmse:25.12308\n[396]\tvalidation_0-rmse:25.12670\n[397]\tvalidation_0-rmse:25.12617\n[398]\tvalidation_0-rmse:25.12478\n[399]\tvalidation_0-rmse:25.12066\n[400]\tvalidation_0-rmse:25.11952\n[401]\tvalidation_0-rmse:25.11579\n[402]\tvalidation_0-rmse:25.11661\n[403]\tvalidation_0-rmse:25.11742\n[404]\tvalidation_0-rmse:25.11622\n[405]\tvalidation_0-rmse:25.11898\n[406]\tvalidation_0-rmse:25.11798\n[407]\tvalidation_0-rmse:25.11569\n[408]\tvalidation_0-rmse:25.11722\n[409]\tvalidation_0-rmse:25.12572\n[410]\tvalidation_0-rmse:25.12151\n[411]\tvalidation_0-rmse:25.11994\n[412]\tvalidation_0-rmse:25.12034\n[413]\tvalidation_0-rmse:25.12219\n[414]\tvalidation_0-rmse:25.12303\n[415]\tvalidation_0-rmse:25.12240\n[416]\tvalidation_0-rmse:25.12353\n[417]\tvalidation_0-rmse:25.12880\n[418]\tvalidation_0-rmse:25.13105\n[419]\tvalidation_0-rmse:25.13076\n[420]\tvalidation_0-rmse:25.12940\n[421]\tvalidation_0-rmse:25.12902\n[422]\tvalidation_0-rmse:25.12899\n[423]\tvalidation_0-rmse:25.12999\n[424]\tvalidation_0-rmse:25.13685\n[425]\tvalidation_0-rmse:25.13825\n[426]\tvalidation_0-rmse:25.13794\n[427]\tvalidation_0-rmse:25.14294\n[428]\tvalidation_0-rmse:25.14887\n[429]\tvalidation_0-rmse:25.15057\n[430]\tvalidation_0-rmse:25.14789\n[431]\tvalidation_0-rmse:25.15115\n[432]\tvalidation_0-rmse:25.15457\n[433]\tvalidation_0-rmse:25.15940\n[434]\tvalidation_0-rmse:25.16267\n[435]\tvalidation_0-rmse:25.16349\n[436]\tvalidation_0-rmse:25.16751\n[437]\tvalidation_0-rmse:25.17304\n[438]\tvalidation_0-rmse:25.17056\n[439]\tvalidation_0-rmse:25.17197\n[440]\tvalidation_0-rmse:25.17302\n[441]\tvalidation_0-rmse:25.17982\n[442]\tvalidation_0-rmse:25.17924\n[443]\tvalidation_0-rmse:25.18156\n[444]\tvalidation_0-rmse:25.18201\n[445]\tvalidation_0-rmse:25.18699\n[446]\tvalidation_0-rmse:25.18139\n[447]\tvalidation_0-rmse:25.17931\n[448]\tvalidation_0-rmse:25.17941\n[449]\tvalidation_0-rmse:25.17884\n[450]\tvalidation_0-rmse:25.18147\n[451]\tvalidation_0-rmse:25.17961\n[452]\tvalidation_0-rmse:25.18019\n[453]\tvalidation_0-rmse:25.18125\n[454]\tvalidation_0-rmse:25.18280\n[455]\tvalidation_0-rmse:25.18532\n[456]\tvalidation_0-rmse:25.18883\n[457]\tvalidation_0-rmse:25.18942\n[458]\tvalidation_0-rmse:25.18964\n[459]\tvalidation_0-rmse:25.18768\n[460]\tvalidation_0-rmse:25.18870\n[461]\tvalidation_0-rmse:25.18916\n[462]\tvalidation_0-rmse:25.18875\n[463]\tvalidation_0-rmse:25.19150\n[464]\tvalidation_0-rmse:25.19478\n[465]\tvalidation_0-rmse:25.19363\n[466]\tvalidation_0-rmse:25.19678\n[467]\tvalidation_0-rmse:25.20081\n[468]\tvalidation_0-rmse:25.20433\n[469]\tvalidation_0-rmse:25.20562\n[470]\tvalidation_0-rmse:25.20653\n[471]\tvalidation_0-rmse:25.20609\n[472]\tvalidation_0-rmse:25.20666\n[473]\tvalidation_0-rmse:25.20496\n[474]\tvalidation_0-rmse:25.21143\n[475]\tvalidation_0-rmse:25.21052\n[476]\tvalidation_0-rmse:25.21246\n[477]\tvalidation_0-rmse:25.21871\n[478]\tvalidation_0-rmse:25.21984\n[479]\tvalidation_0-rmse:25.21290\n[480]\tvalidation_0-rmse:25.20815\n[481]\tvalidation_0-rmse:25.19975\n[482]\tvalidation_0-rmse:25.20120\n[483]\tvalidation_0-rmse:25.20519\n[484]\tvalidation_0-rmse:25.20464\n[485]\tvalidation_0-rmse:25.20728\n[486]\tvalidation_0-rmse:25.20949\n[487]\tvalidation_0-rmse:25.20781\n[488]\tvalidation_0-rmse:25.21145\n[489]\tvalidation_0-rmse:25.21296\n[490]\tvalidation_0-rmse:25.21354\n[491]\tvalidation_0-rmse:25.21345\n[492]\tvalidation_0-rmse:25.21480\n[493]\tvalidation_0-rmse:25.21426\n[494]\tvalidation_0-rmse:25.21669\n[495]\tvalidation_0-rmse:25.22123\n[496]\tvalidation_0-rmse:25.21822\n[497]\tvalidation_0-rmse:25.22033\n[498]\tvalidation_0-rmse:25.21903\n[499]\tvalidation_0-rmse:25.21603\n[500]\tvalidation_0-rmse:25.21858\n[501]\tvalidation_0-rmse:25.22037\n[502]\tvalidation_0-rmse:25.22145\n[503]\tvalidation_0-rmse:25.22470\n[504]\tvalidation_0-rmse:25.22543\n[505]\tvalidation_0-rmse:25.22616\n[506]\tvalidation_0-rmse:25.22615\n[507]\tvalidation_0-rmse:25.22724\n[508]\tvalidation_0-rmse:25.22741\n[509]\tvalidation_0-rmse:25.23352\n[510]\tvalidation_0-rmse:25.23132\n[511]\tvalidation_0-rmse:25.23028\n[512]\tvalidation_0-rmse:25.22972\n[513]\tvalidation_0-rmse:25.23056\n[514]\tvalidation_0-rmse:25.23558\n[515]\tvalidation_0-rmse:25.23755\n[516]\tvalidation_0-rmse:25.23788\n[517]\tvalidation_0-rmse:25.23867\n[518]\tvalidation_0-rmse:25.24038\n[519]\tvalidation_0-rmse:25.24172\n[520]\tvalidation_0-rmse:25.23964\n[521]\tvalidation_0-rmse:25.24329\n[522]\tvalidation_0-rmse:25.24136\n[523]\tvalidation_0-rmse:25.24021\n[524]\tvalidation_0-rmse:25.24166\n[525]\tvalidation_0-rmse:25.24036\n[526]\tvalidation_0-rmse:25.24126\n[527]\tvalidation_0-rmse:25.24306\n[528]\tvalidation_0-rmse:25.24153\n[529]\tvalidation_0-rmse:25.24280\n[530]\tvalidation_0-rmse:25.24345\n[531]\tvalidation_0-rmse:25.24590\n[532]\tvalidation_0-rmse:25.24519\n[533]\tvalidation_0-rmse:25.25142\n[534]\tvalidation_0-rmse:25.25383\n[535]\tvalidation_0-rmse:25.25711\n[536]\tvalidation_0-rmse:25.25312\n[537]\tvalidation_0-rmse:25.25269\n[538]\tvalidation_0-rmse:25.25178\n[539]\tvalidation_0-rmse:25.25176\n[540]\tvalidation_0-rmse:25.24745\n[541]\tvalidation_0-rmse:25.25051\n[542]\tvalidation_0-rmse:25.25235\n[543]\tvalidation_0-rmse:25.25644\n[544]\tvalidation_0-rmse:25.25942\n[545]\tvalidation_0-rmse:25.26213\n[546]\tvalidation_0-rmse:25.26537\n[547]\tvalidation_0-rmse:25.26524\n[548]\tvalidation_0-rmse:25.27313\n[549]\tvalidation_0-rmse:25.27212\n[550]\tvalidation_0-rmse:25.27282\n[551]\tvalidation_0-rmse:25.27263\n[552]\tvalidation_0-rmse:25.27222\n[553]\tvalidation_0-rmse:25.27390\n[554]\tvalidation_0-rmse:25.27761\n[555]\tvalidation_0-rmse:25.26800\n[556]\tvalidation_0-rmse:25.26785\n[557]\tvalidation_0-rmse:25.26613\n[558]\tvalidation_0-rmse:25.26972\n[559]\tvalidation_0-rmse:25.26926\n[560]\tvalidation_0-rmse:25.27211\n[561]\tvalidation_0-rmse:25.27164\n[562]\tvalidation_0-rmse:25.26899\n[563]\tvalidation_0-rmse:25.26866\n[564]\tvalidation_0-rmse:25.26498\n[565]\tvalidation_0-rmse:25.26311\n[566]\tvalidation_0-rmse:25.26319\n[567]\tvalidation_0-rmse:25.26338\n[568]\tvalidation_0-rmse:25.26320\n[569]\tvalidation_0-rmse:25.26440\n[570]\tvalidation_0-rmse:25.26657\n[571]\tvalidation_0-rmse:25.27068\n[572]\tvalidation_0-rmse:25.27016\n[573]\tvalidation_0-rmse:25.27258\n[574]\tvalidation_0-rmse:25.27150\n[575]\tvalidation_0-rmse:25.26959\n[576]\tvalidation_0-rmse:25.27036\n[577]\tvalidation_0-rmse:25.26858\n[578]\tvalidation_0-rmse:25.26934\n[579]\tvalidation_0-rmse:25.26905\n[580]\tvalidation_0-rmse:25.26946\n[581]\tvalidation_0-rmse:25.26602\n[582]\tvalidation_0-rmse:25.27005\n[583]\tvalidation_0-rmse:25.27437\n[584]\tvalidation_0-rmse:25.27710\n[585]\tvalidation_0-rmse:25.27846\n[586]\tvalidation_0-rmse:25.27915\n[587]\tvalidation_0-rmse:25.27578\n[588]\tvalidation_0-rmse:25.27401\n[589]\tvalidation_0-rmse:25.27489\n[590]\tvalidation_0-rmse:25.27791\n[591]\tvalidation_0-rmse:25.27595\n[592]\tvalidation_0-rmse:25.27558\n[593]\tvalidation_0-rmse:25.27374\n[594]\tvalidation_0-rmse:25.27422\n[595]\tvalidation_0-rmse:25.27576\n[596]\tvalidation_0-rmse:25.27320\n[597]\tvalidation_0-rmse:25.27510\n[598]\tvalidation_0-rmse:25.26851\n[599]\tvalidation_0-rmse:25.26743\n[600]\tvalidation_0-rmse:25.26822\n[601]\tvalidation_0-rmse:25.27151\n[602]\tvalidation_0-rmse:25.27154\n[603]\tvalidation_0-rmse:25.27154\n[604]\tvalidation_0-rmse:25.27233\n[605]\tvalidation_0-rmse:25.27340\n[606]\tvalidation_0-rmse:25.27574\n[607]\tvalidation_0-rmse:25.27869\n[608]\tvalidation_0-rmse:25.27848\n[609]\tvalidation_0-rmse:25.27973\n[610]\tvalidation_0-rmse:25.27673\n[611]\tvalidation_0-rmse:25.28151\n[612]\tvalidation_0-rmse:25.28409\n[613]\tvalidation_0-rmse:25.28486\n[614]\tvalidation_0-rmse:25.28594\n[615]\tvalidation_0-rmse:25.28740\n[616]\tvalidation_0-rmse:25.28414\n[617]\tvalidation_0-rmse:25.28910\n[618]\tvalidation_0-rmse:25.29015\n[619]\tvalidation_0-rmse:25.29035\n[620]\tvalidation_0-rmse:25.29061\n[621]\tvalidation_0-rmse:25.29150\n[622]\tvalidation_0-rmse:25.29459\n[623]\tvalidation_0-rmse:25.29362\n[624]\tvalidation_0-rmse:25.29084\n[625]\tvalidation_0-rmse:25.29044\n[626]\tvalidation_0-rmse:25.29025\n[627]\tvalidation_0-rmse:25.28843\n[628]\tvalidation_0-rmse:25.28439\n[629]\tvalidation_0-rmse:25.28391\n[630]\tvalidation_0-rmse:25.28439\n[631]\tvalidation_0-rmse:25.28430\n[632]\tvalidation_0-rmse:25.28654\n[633]\tvalidation_0-rmse:25.28520\n[634]\tvalidation_0-rmse:25.28653\n[635]\tvalidation_0-rmse:25.28914\n[636]\tvalidation_0-rmse:25.28942\n[637]\tvalidation_0-rmse:25.29047\n[638]\tvalidation_0-rmse:25.29106\n[639]\tvalidation_0-rmse:25.28825\n[640]\tvalidation_0-rmse:25.28706\n[641]\tvalidation_0-rmse:25.28832\n[642]\tvalidation_0-rmse:25.28921\n[643]\tvalidation_0-rmse:25.29031\n[644]\tvalidation_0-rmse:25.29421\n[645]\tvalidation_0-rmse:25.29687\n[646]\tvalidation_0-rmse:25.29979\n[647]\tvalidation_0-rmse:25.29938\n[648]\tvalidation_0-rmse:25.30243\n[649]\tvalidation_0-rmse:25.30288\n[650]\tvalidation_0-rmse:25.30466\n[651]\tvalidation_0-rmse:25.30811\n[652]\tvalidation_0-rmse:25.30901\n[653]\tvalidation_0-rmse:25.30970\n[654]\tvalidation_0-rmse:25.30890\n[655]\tvalidation_0-rmse:25.30916\n[656]\tvalidation_0-rmse:25.31183\n[657]\tvalidation_0-rmse:25.31281\n[658]\tvalidation_0-rmse:25.31552\n[659]\tvalidation_0-rmse:25.31563\n[660]\tvalidation_0-rmse:25.31763\n[661]\tvalidation_0-rmse:25.31711\n[662]\tvalidation_0-rmse:25.31562\n[663]\tvalidation_0-rmse:25.31622\n[664]\tvalidation_0-rmse:25.31494\n[665]\tvalidation_0-rmse:25.31781\n[666]\tvalidation_0-rmse:25.32022\n[667]\tvalidation_0-rmse:25.32124\n[668]\tvalidation_0-rmse:25.32207\n[669]\tvalidation_0-rmse:25.32596\n[670]\tvalidation_0-rmse:25.32675\n[671]\tvalidation_0-rmse:25.32839\n[672]\tvalidation_0-rmse:25.32976\n[673]\tvalidation_0-rmse:25.33041\n[674]\tvalidation_0-rmse:25.33409\n[675]\tvalidation_0-rmse:25.33730\n[676]\tvalidation_0-rmse:25.33957\n[677]\tvalidation_0-rmse:25.34044\n[678]\tvalidation_0-rmse:25.34005\n[679]\tvalidation_0-rmse:25.34091\n[680]\tvalidation_0-rmse:25.33990\n[681]\tvalidation_0-rmse:25.33730\n[682]\tvalidation_0-rmse:25.33694\n[683]\tvalidation_0-rmse:25.33818\n[684]\tvalidation_0-rmse:25.34031\n[685]\tvalidation_0-rmse:25.34020\n[686]\tvalidation_0-rmse:25.33800\n[687]\tvalidation_0-rmse:25.34044\n[688]\tvalidation_0-rmse:25.34062\n[689]\tvalidation_0-rmse:25.34282\n[690]\tvalidation_0-rmse:25.34382\n[691]\tvalidation_0-rmse:25.34269\n[692]\tvalidation_0-rmse:25.34479\n[693]\tvalidation_0-rmse:25.34845\n[694]\tvalidation_0-rmse:25.34741\n[695]\tvalidation_0-rmse:25.34873\n[696]\tvalidation_0-rmse:25.34975\n[697]\tvalidation_0-rmse:25.35015\n[698]\tvalidation_0-rmse:25.35046\n[699]\tvalidation_0-rmse:25.35134\n[700]\tvalidation_0-rmse:25.35494\n[701]\tvalidation_0-rmse:25.35557\n[702]\tvalidation_0-rmse:25.35318\n[703]\tvalidation_0-rmse:25.35598\n[704]\tvalidation_0-rmse:25.35841\n[705]\tvalidation_0-rmse:25.35898\n[706]\tvalidation_0-rmse:25.35847\n[707]\tvalidation_0-rmse:25.35924\n[708]\tvalidation_0-rmse:25.36451\n[709]\tvalidation_0-rmse:25.36566\n[710]\tvalidation_0-rmse:25.36725\n[711]\tvalidation_0-rmse:25.36587\n[712]\tvalidation_0-rmse:25.36699\n[713]\tvalidation_0-rmse:25.36707\n[714]\tvalidation_0-rmse:25.36627\n[715]\tvalidation_0-rmse:25.36597\n[716]\tvalidation_0-rmse:25.36385\n[717]\tvalidation_0-rmse:25.36354\n[718]\tvalidation_0-rmse:25.36426\n[719]\tvalidation_0-rmse:25.36344\n[720]\tvalidation_0-rmse:25.36287\n[721]\tvalidation_0-rmse:25.36333\n[722]\tvalidation_0-rmse:25.36330\n[723]\tvalidation_0-rmse:25.36438\n[724]\tvalidation_0-rmse:25.36461\n[725]\tvalidation_0-rmse:25.36411\n[726]\tvalidation_0-rmse:25.36463\n[727]\tvalidation_0-rmse:25.36497\n[728]\tvalidation_0-rmse:25.36539\n[729]\tvalidation_0-rmse:25.36718\n[730]\tvalidation_0-rmse:25.36940\n[731]\tvalidation_0-rmse:25.37213\n[732]\tvalidation_0-rmse:25.37528\n[733]\tvalidation_0-rmse:25.37402\n[734]\tvalidation_0-rmse:25.37083\n[735]\tvalidation_0-rmse:25.37083\n[736]\tvalidation_0-rmse:25.36919\n[737]\tvalidation_0-rmse:25.37258\n[738]\tvalidation_0-rmse:25.37206\n[739]\tvalidation_0-rmse:25.37281\n[740]\tvalidation_0-rmse:25.37116\n[741]\tvalidation_0-rmse:25.36978\n[742]\tvalidation_0-rmse:25.36758\n[743]\tvalidation_0-rmse:25.36896\n[744]\tvalidation_0-rmse:25.36673\n[745]\tvalidation_0-rmse:25.36860\n[746]\tvalidation_0-rmse:25.36903\n[747]\tvalidation_0-rmse:25.37097\n[748]\tvalidation_0-rmse:25.37253\n[749]\tvalidation_0-rmse:25.37324\n[750]\tvalidation_0-rmse:25.37142\n[751]\tvalidation_0-rmse:25.37355\n[752]\tvalidation_0-rmse:25.37477\n[753]\tvalidation_0-rmse:25.37726\n[754]\tvalidation_0-rmse:25.37793\n[755]\tvalidation_0-rmse:25.37753\n[756]\tvalidation_0-rmse:25.37852\n[757]\tvalidation_0-rmse:25.37909\n[758]\tvalidation_0-rmse:25.37595\n[759]\tvalidation_0-rmse:25.37683\n[760]\tvalidation_0-rmse:25.37933\n[761]\tvalidation_0-rmse:25.37599\n[762]\tvalidation_0-rmse:25.37781\n[763]\tvalidation_0-rmse:25.37663\n[764]\tvalidation_0-rmse:25.37649\n[765]\tvalidation_0-rmse:25.37268\n[766]\tvalidation_0-rmse:25.37300\n[767]\tvalidation_0-rmse:25.37595\n[768]\tvalidation_0-rmse:25.37743\n[769]\tvalidation_0-rmse:25.37390\n[770]\tvalidation_0-rmse:25.37330\n[771]\tvalidation_0-rmse:25.37229\n[772]\tvalidation_0-rmse:25.37183\n[773]\tvalidation_0-rmse:25.37239\n[774]\tvalidation_0-rmse:25.37243\n[775]\tvalidation_0-rmse:25.37200\n[776]\tvalidation_0-rmse:25.37037\n[777]\tvalidation_0-rmse:25.37422\n[778]\tvalidation_0-rmse:25.37620\n[779]\tvalidation_0-rmse:25.37684\n[780]\tvalidation_0-rmse:25.37701\n[781]\tvalidation_0-rmse:25.37886\n[782]\tvalidation_0-rmse:25.37816\n[783]\tvalidation_0-rmse:25.37894\n[784]\tvalidation_0-rmse:25.37868\n[785]\tvalidation_0-rmse:25.37702\n[786]\tvalidation_0-rmse:25.37763\n[787]\tvalidation_0-rmse:25.37760\n[788]\tvalidation_0-rmse:25.37862\n[789]\tvalidation_0-rmse:25.37718\n[790]\tvalidation_0-rmse:25.37610\n[791]\tvalidation_0-rmse:25.37865\n[792]\tvalidation_0-rmse:25.37657\n[793]\tvalidation_0-rmse:25.37663\n[794]\tvalidation_0-rmse:25.37642\n[795]\tvalidation_0-rmse:25.37660\n[796]\tvalidation_0-rmse:25.37915\n[797]\tvalidation_0-rmse:25.38174\n[798]\tvalidation_0-rmse:25.38105\n[799]\tvalidation_0-rmse:25.37870\n[800]\tvalidation_0-rmse:25.37678\n[801]\tvalidation_0-rmse:25.37653\n[802]\tvalidation_0-rmse:25.37755\n[803]\tvalidation_0-rmse:25.37899\n[804]\tvalidation_0-rmse:25.37882\n[805]\tvalidation_0-rmse:25.37668\n[806]\tvalidation_0-rmse:25.37606\n[807]\tvalidation_0-rmse:25.37733\n[808]\tvalidation_0-rmse:25.37896\n[809]\tvalidation_0-rmse:25.38004\n[810]\tvalidation_0-rmse:25.37830\n[811]\tvalidation_0-rmse:25.37957\n[812]\tvalidation_0-rmse:25.38186\n[813]\tvalidation_0-rmse:25.38328\n[814]\tvalidation_0-rmse:25.38559\n[815]\tvalidation_0-rmse:25.38563\n[816]\tvalidation_0-rmse:25.38343\n[817]\tvalidation_0-rmse:25.38455\n[818]\tvalidation_0-rmse:25.38520\n[819]\tvalidation_0-rmse:25.38783\n[820]\tvalidation_0-rmse:25.38651\n[821]\tvalidation_0-rmse:25.38214\n[822]\tvalidation_0-rmse:25.38209\n[823]\tvalidation_0-rmse:25.38249\n[824]\tvalidation_0-rmse:25.38285\n[825]\tvalidation_0-rmse:25.38463\n[826]\tvalidation_0-rmse:25.38743\n[827]\tvalidation_0-rmse:25.38663\n[828]\tvalidation_0-rmse:25.38745\n[829]\tvalidation_0-rmse:25.38700\n[830]\tvalidation_0-rmse:25.38949\n[831]\tvalidation_0-rmse:25.38928\n[832]\tvalidation_0-rmse:25.38881\n[833]\tvalidation_0-rmse:25.39022\n[834]\tvalidation_0-rmse:25.38948\n[835]\tvalidation_0-rmse:25.38847\n[836]\tvalidation_0-rmse:25.38676\n[837]\tvalidation_0-rmse:25.38818\n[838]\tvalidation_0-rmse:25.38689\n[839]\tvalidation_0-rmse:25.38678\n[840]\tvalidation_0-rmse:25.38485\n[841]\tvalidation_0-rmse:25.38609\n[842]\tvalidation_0-rmse:25.38675\n[843]\tvalidation_0-rmse:25.38561\n[844]\tvalidation_0-rmse:25.38503\n[845]\tvalidation_0-rmse:25.38543\n[846]\tvalidation_0-rmse:25.38496\n[847]\tvalidation_0-rmse:25.38383\n[848]\tvalidation_0-rmse:25.38287\n[849]\tvalidation_0-rmse:25.38337\n[850]\tvalidation_0-rmse:25.38583\n[851]\tvalidation_0-rmse:25.38505\n[852]\tvalidation_0-rmse:25.38768\n[853]\tvalidation_0-rmse:25.38768\n[854]\tvalidation_0-rmse:25.38411\n[855]\tvalidation_0-rmse:25.38515\n[856]\tvalidation_0-rmse:25.38286\n[857]\tvalidation_0-rmse:25.38318\n[858]\tvalidation_0-rmse:25.38260\n[859]\tvalidation_0-rmse:25.38562\n[860]\tvalidation_0-rmse:25.38636\n[861]\tvalidation_0-rmse:25.38492\n[862]\tvalidation_0-rmse:25.38241\n[863]\tvalidation_0-rmse:25.38178\n[864]\tvalidation_0-rmse:25.38131\n[865]\tvalidation_0-rmse:25.37512\n[866]\tvalidation_0-rmse:25.37816\n[867]\tvalidation_0-rmse:25.37881\n[868]\tvalidation_0-rmse:25.37954\n[869]\tvalidation_0-rmse:25.38081\n[870]\tvalidation_0-rmse:25.38094\n[871]\tvalidation_0-rmse:25.37995\n[872]\tvalidation_0-rmse:25.37993\n[873]\tvalidation_0-rmse:25.37646\n[874]\tvalidation_0-rmse:25.37620\n[875]\tvalidation_0-rmse:25.37603\n[876]\tvalidation_0-rmse:25.37635\n[877]\tvalidation_0-rmse:25.37495\n[878]\tvalidation_0-rmse:25.37446\n[879]\tvalidation_0-rmse:25.37527\n[880]\tvalidation_0-rmse:25.37572\n[881]\tvalidation_0-rmse:25.37189\n[882]\tvalidation_0-rmse:25.37374\n[883]\tvalidation_0-rmse:25.37400\n[884]\tvalidation_0-rmse:25.37317\n[885]\tvalidation_0-rmse:25.37313\n[886]\tvalidation_0-rmse:25.37499\n[887]\tvalidation_0-rmse:25.37328\n[888]\tvalidation_0-rmse:25.37261\n[889]\tvalidation_0-rmse:25.37228\n[890]\tvalidation_0-rmse:25.37212\n[891]\tvalidation_0-rmse:25.36941\n[892]\tvalidation_0-rmse:25.37130\n[893]\tvalidation_0-rmse:25.37042\n[894]\tvalidation_0-rmse:25.36947\n[895]\tvalidation_0-rmse:25.36744\n[896]\tvalidation_0-rmse:25.36915\n[897]\tvalidation_0-rmse:25.36913\n[898]\tvalidation_0-rmse:25.36736\n[899]\tvalidation_0-rmse:25.36844\n[900]\tvalidation_0-rmse:25.36821\n[901]\tvalidation_0-rmse:25.36895\n[902]\tvalidation_0-rmse:25.36811\n[903]\tvalidation_0-rmse:25.36780\n[904]\tvalidation_0-rmse:25.36672\n[905]\tvalidation_0-rmse:25.36790\n[906]\tvalidation_0-rmse:25.36956\n[907]\tvalidation_0-rmse:25.36882\n[908]\tvalidation_0-rmse:25.36670\n[909]\tvalidation_0-rmse:25.36486\n[910]\tvalidation_0-rmse:25.36470\n[911]\tvalidation_0-rmse:25.36470\n[912]\tvalidation_0-rmse:25.36320\n[913]\tvalidation_0-rmse:25.36172\n[914]\tvalidation_0-rmse:25.36327\n[915]\tvalidation_0-rmse:25.36360\n[916]\tvalidation_0-rmse:25.36573\n[917]\tvalidation_0-rmse:25.36472\n[918]\tvalidation_0-rmse:25.36397\n[919]\tvalidation_0-rmse:25.36568\n[920]\tvalidation_0-rmse:25.36531\n[921]\tvalidation_0-rmse:25.36562\n[922]\tvalidation_0-rmse:25.36692\n[923]\tvalidation_0-rmse:25.36519\n[924]\tvalidation_0-rmse:25.36585\n[925]\tvalidation_0-rmse:25.36829\n[926]\tvalidation_0-rmse:25.36908\n[927]\tvalidation_0-rmse:25.37109\n[928]\tvalidation_0-rmse:25.37175\n[929]\tvalidation_0-rmse:25.37097\n[930]\tvalidation_0-rmse:25.36871\n[931]\tvalidation_0-rmse:25.37070\n[932]\tvalidation_0-rmse:25.37207\n[933]\tvalidation_0-rmse:25.37419\n[934]\tvalidation_0-rmse:25.37519\n[935]\tvalidation_0-rmse:25.37366\n[936]\tvalidation_0-rmse:25.37253\n[937]\tvalidation_0-rmse:25.37084\n[938]\tvalidation_0-rmse:25.37170\n[939]\tvalidation_0-rmse:25.37286\n[940]\tvalidation_0-rmse:25.37513\n[941]\tvalidation_0-rmse:25.37646\n[942]\tvalidation_0-rmse:25.37830\n[943]\tvalidation_0-rmse:25.37719\n[944]\tvalidation_0-rmse:25.37721\n[945]\tvalidation_0-rmse:25.37824\n[946]\tvalidation_0-rmse:25.37734\n[947]\tvalidation_0-rmse:25.37616\n[948]\tvalidation_0-rmse:25.37491\n[949]\tvalidation_0-rmse:25.37496\n[950]\tvalidation_0-rmse:25.37586\n[951]\tvalidation_0-rmse:25.37628\n[952]\tvalidation_0-rmse:25.37140\n[953]\tvalidation_0-rmse:25.37243\n[954]\tvalidation_0-rmse:25.37275\n[955]\tvalidation_0-rmse:25.37404\n[956]\tvalidation_0-rmse:25.37486\n[957]\tvalidation_0-rmse:25.37354\n[958]\tvalidation_0-rmse:25.37424\n[959]\tvalidation_0-rmse:25.37443\n[960]\tvalidation_0-rmse:25.37425\n[961]\tvalidation_0-rmse:25.37193\n[962]\tvalidation_0-rmse:25.37197\n[963]\tvalidation_0-rmse:25.37275\n[964]\tvalidation_0-rmse:25.37068\n[965]\tvalidation_0-rmse:25.37111\n[966]\tvalidation_0-rmse:25.37269\n[967]\tvalidation_0-rmse:25.37317\n[968]\tvalidation_0-rmse:25.37126\n[969]\tvalidation_0-rmse:25.37261\n[970]\tvalidation_0-rmse:25.37369\n[971]\tvalidation_0-rmse:25.37383\n[972]\tvalidation_0-rmse:25.37317\n[973]\tvalidation_0-rmse:25.37322\n[974]\tvalidation_0-rmse:25.37344\n[975]\tvalidation_0-rmse:25.37404\n[976]\tvalidation_0-rmse:25.37577\n[977]\tvalidation_0-rmse:25.37541\n[978]\tvalidation_0-rmse:25.37629\n[979]\tvalidation_0-rmse:25.37684\n[980]\tvalidation_0-rmse:25.37636\n[981]\tvalidation_0-rmse:25.37881\n[982]\tvalidation_0-rmse:25.37937\n[983]\tvalidation_0-rmse:25.37857\n[984]\tvalidation_0-rmse:25.37881\n[985]\tvalidation_0-rmse:25.37711\n[986]\tvalidation_0-rmse:25.37380\n[987]\tvalidation_0-rmse:25.37342\n[988]\tvalidation_0-rmse:25.37306\n[989]\tvalidation_0-rmse:25.37251\n[990]\tvalidation_0-rmse:25.37424\n[991]\tvalidation_0-rmse:25.37181\n[992]\tvalidation_0-rmse:25.37263\n[993]\tvalidation_0-rmse:25.37347\n[994]\tvalidation_0-rmse:25.37295\n[995]\tvalidation_0-rmse:25.37282\n[996]\tvalidation_0-rmse:25.37626\n[997]\tvalidation_0-rmse:25.37811\n[998]\tvalidation_0-rmse:25.37805\n[999]\tvalidation_0-rmse:25.37820\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007184 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3143\n[LightGBM] [Info] Number of data points in the train set: 1345, number of used features: 772\n[LightGBM] [Info] Start training from score 32.797655\n\n✅ 최종 CV RMSE: 24.0792\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ 라이브러리 및 데이터 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors, MACCSkeys\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import optuna\n",
        "import os\n",
        "\n",
        "path = '/kaggle/input/drug-data'\n",
        "train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "y = train['Inhibition']\n",
        "\n",
        "# 2️⃣ Feature 생성 함수들\n",
        "def smiles_to_morgan(smiles, radius=2, nBits=1024):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return np.zeros(nBits) if mol is None else np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits))\n",
        "\n",
        "def smiles_to_maccs(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return np.zeros(167) if mol is None else np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "\n",
        "def physchem_feats(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None: return np.zeros(15)\n",
        "    return np.array([\n",
        "        Descriptors.MolWt(mol),\n",
        "        Descriptors.TPSA(mol),\n",
        "        Descriptors.MolLogP(mol),\n",
        "        Descriptors.NumHDonors(mol),\n",
        "        Descriptors.NumHAcceptors(mol),\n",
        "        Descriptors.NumRotatableBonds(mol),\n",
        "        Descriptors.RingCount(mol),\n",
        "        Descriptors.HeavyAtomCount(mol),\n",
        "        rdMolDescriptors.CalcFractionCSP3(mol),\n",
        "        rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
        "        rdMolDescriptors.CalcNumAromaticRings(mol),\n",
        "        rdMolDescriptors.CalcNumSaturatedRings(mol),\n",
        "        Descriptors.MaxEStateIndex(mol),\n",
        "        Descriptors.MinEStateIndex(mol),\n",
        "        Descriptors.FractionCSP3(mol)\n",
        "    ])\n",
        "\n",
        "def extract_features(df):\n",
        "    feats = []\n",
        "    for smi in tqdm(df['Canonical_Smiles']):\n",
        "        f = np.concatenate([smiles_to_morgan(smi),\n",
        "                            smiles_to_maccs(smi),\n",
        "                            physchem_feats(smi)])\n",
        "        feats.append(f)\n",
        "    return np.vstack(feats)\n",
        "\n",
        "# 3️⃣ Feature 생성\n",
        "X = extract_features(train)\n",
        "X_test = extract_features(test)\n",
        "\n",
        "# 4️⃣ Optuna 튜닝 함수\n",
        "def tune_model(name):\n",
        "    def obj(trial):\n",
        "        if name == 'cat':\n",
        "            params = {\n",
        "                'iterations':1000,\n",
        "                'learning_rate':trial.suggest_float('lr',0.01,0.2),\n",
        "                'depth':trial.suggest_int('d',4,10),\n",
        "                'l2_leaf_reg':trial.suggest_int('l2',1,10),\n",
        "                'random_seed':42, 'loss_function':'RMSE', 'verbose':0\n",
        "            }\n",
        "            mdl = CatBoostRegressor(**params)\n",
        "        elif name=='xgb':\n",
        "            params = {'n_estimators':1000,\n",
        "                      'learning_rate':trial.suggest_float('lr',0.01,0.2),\n",
        "                      'max_depth':trial.suggest_int('d',3,10),\n",
        "                      'subsample':trial.suggest_float('sub',0.5,1),\n",
        "                      'colsample_bytree':trial.suggest_float('col',0.5,1),\n",
        "                      'random_state':42}\n",
        "            mdl = XGBRegressor(**params)\n",
        "        else:\n",
        "            params = {'n_estimators':1000,\n",
        "                      'learning_rate':trial.suggest_float('lr',0.01,0.2),\n",
        "                      'max_depth':trial.suggest_int('d',3,10),\n",
        "                      'subsample':trial.suggest_float('sub',0.5,1),\n",
        "                      'colsample_bytree':trial.suggest_float('col',0.5,1),\n",
        "                      'random_state':42}\n",
        "            mdl = LGBMRegressor(**params)\n",
        "\n",
        "        kf = KFold(3, shuffle=True, random_state=42)\n",
        "        oof = np.zeros(len(X))\n",
        "        for ti, vi in kf.split(X):\n",
        "            mdl.fit(X[ti], y.iloc[ti],\n",
        "                    eval_set=[(X[vi], y.iloc[vi])],\n",
        "                    early_stopping_rounds=50, verbose=False)\n",
        "            oof[vi] = mdl.predict(X[vi])\n",
        "        return np.sqrt(mean_squared_error(y, oof))\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(obj, n_trials=30)\n",
        "    return study.best_params\n",
        "\n",
        "cat_p = tune_model('cat')\n",
        "xgb_p = tune_model('xgb')\n",
        "lgb_p = tune_model('lgb')\n",
        "\n",
        "# 5️⃣ Weighted Ensemble\n",
        "kf = KFold(5, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(X)); preds = np.zeros(len(X_test))\n",
        "w = {'cat':0.5, 'xgb':0.3, 'lgb':0.2}\n",
        "\n",
        "for ti, vi in kf.split(X):\n",
        "    Xt, Xv = X[ti], X[vi]; yt, yv = y.iloc[ti], y.iloc[vi]\n",
        "    c = CatBoostRegressor(**cat_p, iterations=1000, verbose=0)\n",
        "    x = XGBRegressor(**xgb_p)\n",
        "    l = LGBMRegressor(**lgb_p)\n",
        "    c.fit(Xt, yt, eval_set=(Xv,yv), early_stopping_rounds=50, verbose=False)\n",
        "    x.fit(Xt, yt, eval_set=[(Xv,yv)], early_stopping_rounds=50, verbose=False)\n",
        "    l.fit(Xt, yt, eval_set=[(Xv,yv)], early_stopping_rounds=50, verbose=False)\n",
        "\n",
        "    oof[vi] = w['cat']*c.predict(Xv)+w['xgb']*x.predict(Xv)+w['lgb']*l.predict(Xv)\n",
        "    preds += (w['cat']*c.predict(X_test)+\n",
        "              w['xgb']*x.predict(X_test)+\n",
        "              w['lgb']*l.predict(X_test)) / 5\n",
        "\n",
        "# 6️⃣ 저장\n",
        "print(\"CV RMSE:\", np.sqrt(mean_squared_error(y, oof)))\n",
        "submission['Inhibition'] = preds\n",
        "submission.to_csv('/kaggle/working/submission_weighted_optuna.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yGo--POaBPkj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}